// Auto-generated from BC-Prompt-Library.md
window.promptsData = [
  {
    "name": "Catch Up on Slack Threads",
    "category": "Productivity",
    "prompt": "<slack_thread_synthesizer>\n\n<thread_inputs>\nPASTE THE SLACK THREAD:\n[Full thread content - copy/paste from Slack]\n\nCONTEXT QUESTIONS:\n1. Why are you reading this? (catching up after vacation, deciding if you need to weigh in, preparing for a meeting)\n2. What's your role in this discussion? (decision-maker, contributor, FYI only)\n3. Are there specific people whose opinions matter most to you?\n4. What decisions or action items are you hoping to find?\n5. How much detail do you need? (quick skim vs. deep understanding)\n</thread_inputs>\n\n<synthesis_process>\n\nYou are an expert at synthesizing long, meandering Slack threads into clear, actionable summaries. You identify the signal through the noise, tracking how discussions evolve and surfacing what actually matters.\n\nPHASE 1: THREAD ANALYSIS\n\nFirst, map the conversation structure:\n\n1. CONVERSATION FLOW\n- How many participants?\n- How long is the thread? (timestamp of first vs. last message)\n- Did the topic shift during the conversation?\n- Are there multiple sub-conversations happening in parallel?\n- Were there any long gaps? (discussion paused and resumed)\n\n2. IDENTIFY PARTICIPANTS & ROLES\nFor key participants, note:\n- Who started the thread and why?\n- Who are the decision-makers?\n- Who are subject matter experts?\n- Who's just reacting/observing?\n- Any conflicts or disagreements between people?\n\n3. TRACK TOPIC EVOLUTION\n- What was the original question/topic?\n- Did it evolve? (often threads drift from initial topic)\n- Were new issues raised mid-thread?\n- Was the original question answered?\n\nPHASE 2: EXTRACT KEY ELEMENTS\n\nPull out the important information:\n\n1. DECISIONS MADE\nFor each decision:\n- What was decided: [specific outcome]\n- Who decided: [person or consensus]\n- Rationale: [why this decision, what alternatives were considered]\n- When: [timestamp]\n- Finality: Is this locked in or still being debated?\n\nExample:\n❗DECISION: Will launch feature X with limited beta first, not full rollout\n- Decided by: @sarah (PM lead)\n- Why: Engineering raised concerns about scale; beta lets us test with 100 users first\n- When: Today at 2:47 PM\n- Status: Final, team aligned\n\n2. ACTION ITEMS\nFor each action item:\n- What needs to be done: [specific task]\n- Who owns it: [person assigned, or \"unassigned\" if unclear]\n- Deadline: [if mentioned, or \"unclear\"]\n- Blocker status: Does this block other work?\n\nFormat as checklist:\n- [ ] @jamie: Write PRD for beta launch (due Friday)\n- [ ] @engineering: Estimate effort for dashboard work (due this week) - BLOCKS design work\n- [ ] UNASSIGNED: Schedule follow-up meeting (no deadline mentioned)\n\n3. OPEN QUESTIONS\nQuestions that were raised but not answered:\n- \"What's our budget for this?\" - Asked by @mike, no response yet\n- \"Does this need legal review?\" - Discussed but no clear answer\n- \"Who's going to handle customer comms?\" - Multiple people asked, still unclear\n\n4. KEY CONTEXT & BACKGROUND\nImportant facts or context shared:\n- Customer X specifically requested this feature\n- We tried something similar 2 years ago and it failed because [reason]\n- Legal says we must comply with [regulation] for this\n- Our main competitor launched this last week\n\n5. CONCERNS & OBJECTIONS\nWho raised concerns and about what:\n- @engineering: Worried about performance at scale (message link)\n- @design: Thinks this UX will confuse users (message link)\n- @sales: Concerned this won't work for enterprise customers (message link)\n\nNote: Did concerns get addressed or are they still hanging?\n\n6. CONSENSUS vs. DISAGREEMENT\n- What does everyone agree on?\n- Where is there active disagreement?\n- Are disagreements resolved or still debating?\n\nPHASE 3: SYNTHESIZE SUMMARY\n\nCreate a structured summary:\n\n## Thread TL;DR (2-3 sentences)\n[What was this thread about and what's the current status? Bottom line up front.]\n\n## Key Decisions\n[List all decisions made, formatted as shown above]\n\n## Action Items\n[Checklist format with owners and deadlines]\n\n## Open Questions\n[List with who asked and whether discussed]\n\n## Important Context\n[Key facts, background, or constraints mentioned]\n\n## Concerns Raised\n[Who's worried about what, and whether addressed]\n\n## Current Status\n[Where does this stand? Is discussion complete or ongoing? What happens next?]\n\n## Your Role (if specified in context)\n[Based on your context, do you need to: weigh in on something, complete an action item, make a decision, or just stay informed?]\n\n## Related Threads or Documents\n[If mentioned: links to PRDs, previous threads, related discussions]\n\n</synthesis_process>\n\n<quality_checks>\n\nValidate your summary:\n\n1. COMPLETENESS CHECK\n- Did you capture all decisions, even small ones?\n- Are all action items accounted for, including implied ones?\n- Did you note ALL open questions, not just the most recent?\n\n2. ACCURACY CHECK\n- Are you quoting decisions correctly or adding your interpretation?\n- Did you attribute statements to the right people?\n- Are timestamps correct?\n\n3. CLARITY CHECK\n- Can someone who wasn't in the thread understand what happened?\n- Is it clear what's decided vs. still debating?\n- Are action item owners unambiguous?\n\n4. ACTIONABILITY CHECK\n- Is it clear what YOU need to do (if anything)?\n- Are next steps obvious?\n- Would someone know what to do Monday morning?\n\nSELF-CRITIQUE: If any check fails, strengthen that section.\n\n</quality_checks>\n\n<output_format>\n\nFor quick catch-ups (< 50 messages):\n- TL;DR\n- Decisions\n- Action Items\n- Your next step\n\nFor complex threads (50+ messages):\n- Full structure with all sections\n- Timeline of how discussion evolved\n- Link to key messages for deep-dive if needed\n\nFor urgent decision threads:\n- Lead with: \"NEEDS YOUR INPUT ON: [specific question]\"\n- Then standard structure\n\n</output_format>\n\n<meta_guidance>\n\nGood Slack summaries:\n- Front-load the most important information (decisions and action items)\n- Make it scannable (headers, bullets, formatting)\n- Distinguish between facts and opinions\n- Flag what still needs resolution\n- Are honest about what's unclear or messy\n\nAvoid:\n- Summarizing every message chronologically (that's not synthesis)\n- Losing context about WHY decisions were made\n- Missing implied action items (\"someone should...\")\n- Treating all opinions equally (flag the decision-maker's view)\n- Hiding important concerns in the middle of the summary\n\nRemember: Slack threads often meander. Your job is to extract the structure and outcomes from the chaos. When people disagree or the thread is messy, say so - don't pretend it's cleaner than it is.\n\n</meta_guidance>\n\n</slack_thread_synthesizer>",
    "technique": "Asks the AI to explain dependencies between steps (\"why each phase must come before the next\") rather than just listing a sequence",
    "tools": "LLM or Workflow: Lindy, Zapier",
    "useCase": "Long slack thread to digest"
  },
  {
    "name": "Convert Meeting Transcripts to Action Items",
    "category": "Productivity",
    "prompt": "<transcript_to_summary>\n\n<inputs>\nPASTE YOUR TRANSCRIPT:\n[Raw Teams/Zoom transcript - timestamps, speaker labels, the whole mess]\n\nMEETING CONTEXT:\n- Meeting type: [Sync, decision, brainstorm, review]\n- Attendees: [If not clear from transcript]\n- Your role: [What you need to track]\n\nOUTPUT FORMAT:\n- [ ] Executive summary (2-3 sentences)\n- [ ] Key topics discussed\n- [ ] Decisions made\n- [ ] Action items with owners\n- [ ] Open questions\n- [ ] All of the above\n</inputs>\n\n<conversion_framework>\n\nYou're distilling signal from noisy conversation. Strip filler, capture substance.\n\n**Create a Word document (.docx) with the following structure:**\n\n## EXECUTIVE SUMMARY\n[2-3 sentences: What was this meeting about? What was accomplished?]\n\n## KEY TOPICS\n- **[Topic 1]**: [1-2 sentence summary of discussion]\n- **[Topic 2]**: [1-2 sentence summary of discussion]\n\n## DECISIONS MADE\n- [Decision 1]\n- [Decision 2]\n- [None explicitly made] ← flag if true\n\n## ACTION ITEMS\n| Owner | Action | Due Date |\n|-------|--------|----------|\n| [Name] | [Specific action] | [Date/TBD] |\n\n## OPEN QUESTIONS / PARKING LOT\n- [Unresolved topic]\n- [Who owns resolution / when revisit]\n\n</conversion_framework>\n\n<output_instructions>\n\nSave as: `[Meeting Name]_Summary_[YYYY-MM-DD].docx`\n\nUse professional formatting:\n- Calibri or Arial 11pt\n- Bold section headers\n- Table for action items\n- Page header with meeting date and attendees\n\n</output_instructions>\n\n<transcript_specific>\n\nIgnore:\n- Filler (\"um\", \"you know\", \"let me think\")\n- Side conversations\n- Technical difficulties discussion\n- Pleasantries unless context-relevant\n\nFlag when:\n- Speaker unclear on ownership\n- Decision implied but not confirmed\n- Timeline mentioned without commitment\n\n</transcript_specific>\n\n</transcript_to_summary>",
    "technique": "Distills signal from noisy conversation, strips filler, captures substance with clear ownership",
    "tools": "Claude, ChatGPT, or any LLM",
    "useCase": "Turn messy Teams/Zoom transcripts into formatted Word documents with clear action items and decisions"
  },
  {
    "name": "Explain Technical Concept Simply",
    "category": "Productivity",
    "prompt": "<explain_technical_concept>\n\n<concept_inputs>\nPASTE WHAT YOU NEED TO EXPLAIN:\n[The technical thing - screenshot, quote, message, doc]\n\nWHO'S YOUR AUDIENCE:\n- [ ] Customer (non-technical)\n- [ ] Executive (business-focused)\n- [ ] Sales team (needs to sell it)\n- [ ] Designer (visual thinker)\n- [ ] Other: [specify]\n\nWHAT DO THEY NEED TO KNOW:\n- [ ] Why this matters (business impact)\n- [ ] What changed (what's different)\n- [ ] What it enables (new possibilities)\n- [ ] Why it takes time (if relevant)\n- [ ] Other: [specify]\n</concept_inputs>\n\n<explanation_framework>\n\nYou're a translator between tech and business. Your job: make complex things simple without dumbing them down.\n\nTHE APPROACH:1. Strip the jargon\nIdentify every technical term and replace with plain English.\n\n2. Use analogies\nConnect to something they already understand.\n\n3. Focus on \"so what\"\nLead with impact, not mechanism.\n\n4. Layer the detail\nStart simple, add complexity only if needed.\n\n---\n\n## YOUR EXPLANATION\n\nOne-sentence version:\n[The absolute simplest explanation]\n\nParagraph version:\n[2-3 sentences with a bit more context]\n\nWith analogy:\n[Use comparison to something familiar]\n\nWhy this matters:\n[Business impact or benefit]\n\nIf they ask for more detail:\n[Slightly more technical version, still accessible]\n\n---\n\n## EXAMPLE TRANSFORMATIONS\n\nTechnical: \"We need to migrate from monolith to microservices architecture\"\n\nSimple: \"We're splitting our codebase into smaller, independent pieces. Think of it like moving from one giant warehouse to many specialized shops - each can operate and upgrade independently without breaking the others.\"\n\nWhy it matters: \"This lets us ship features faster and makes the product more reliable.\"\n\n---\n\nTechnical: \"The API rate limit is causing 429 errors\"\n\nSimple: \"Too many requests are hitting our system at once, so it's rejecting some to protect itself. Like a restaurant that stops seating people when it's at capacity.\"\n\nWhy it matters: \"Some users are seeing errors. We're either raising the limit or optimizing how often we check for updates.\"\n\n</explanation_framework>\n\n<meta_guidance>\n\nGood explanations:\n- Start with what changed or why it matters\n- Use concrete analogies\n- Avoid saying \"basically\" or \"essentially\"\n- Don't patronize\n\nCommon analogies that work:\n- Houses/buildings (architecture)\n- Roads/traffic (networking)\n- Restaurants/kitchens (systems)\n- Libraries/filing systems (databases)\n- Mail/packages (data transfer)\n\nRed flags:\n- If explanation includes \"just\" - it's not simple\n- If you use acronyms - spell them out\n- If you say \"it's complicated\" - you don't understand it yet\n\n</meta_guidance>\n\n</explain_technical_concept>",
    "technique": "Analogy generation, jargon removal, audience-specific translation",
    "tools": "Claude, ChatGPT, Gemini",
    "useCase": "Engineer/tech person said something technical, need to explain it to non-technical stakeholder"
  },
  {
    "name": "Make Writing Shorter/Clearer",
    "category": "Productivity",
    "prompt": "<shorten_writing>\n\n<writing_inputs>\nPASTE WHAT YOU WROTE:\n[Your draft]\n\nTARGET LENGTH:\n- [ ] Half the length\n- [ ] One paragraph\n- [ ] 3 sentences\n- [ ] One sentence\n- [ ] Specific: [X words]\n\nTONE TO MAINTAIN:\n- [ ] Professional\n- [ ] Casual/friendly\n- [ ] Direct/firm\n- [ ] Empathetic\n- [ ] Match original\n\nWHAT TO PRESERVE:\n- [ ] Key facts/numbers\n- [ ] Specific asks\n- [ ] Action items\n- [ ] Everything important (just shorter)\n</writing_inputs>\n\n<editing_framework>\n\nYou're an editor who makes writing crisp. Your rules: cut ruthlessly, keep meaning, maintain tone.\n\nTHE PROCESS:1. Identify the core message\nWhat's the one thing this must communicate?\n\n2. Cut mercilessly\n- Filler words: just, really, very, actually, basically\n- Hedging: I think, maybe, perhaps, possibly\n- Redundancy: \"end result\", \"past history\", \"future plans\"\n- Throat-clearing: \"I wanted to reach out to...\"\n\n3. Make it active\n- Passive: \"The decision was made by the team\"\n- Active: \"The team decided\"\n\n4. Front-load the point\nPut the most important thing first.\n\n---\n\n## YOUR EDIT\n\nOriginal length: [X words]\nNew length: [Y words]\n\nSHORTENED VERSION:\n\n[Your tightened text]\n\n---\n\nWHAT CHANGED:\n- Removed: [What you cut]\n- Simplified: [What you made clearer]\n- Reordered: [If you moved things around]\n\n---\n\nALTERNATIVE (even shorter):\n[If they need it even more concise]\n\n</editing_framework>\n\n<meta_guidance>\n\nCommon cuts:\n\n❌ \"I wanted to follow up to see if...\"\n✅ \"Following up on...\"\n\n❌ \"I think we should probably consider maybe...\"\n✅ \"We should...\"\n\n❌ \"In order to improve the situation...\"\n✅ \"To improve...\"\n\n❌ \"Due to the fact that...\"\n✅ \"Because...\"\n\nThe 50% rule:\nFirst draft can usually lose 50% of words without losing meaning.\n\n</meta_guidance>\n\n</shorten_writing>",
    "technique": "Conciseness editing, clarity enhancement, active voice conversion",
    "tools": "Claude",
    "useCase": "You wrote something but it's too long, unclear, or rambly"
  },
  {
    "name": "Turn Meeting Notes into Action Items",
    "category": "Productivity",
    "prompt": "<meeting_to_actions>\n\n<meeting_inputs>\nPASTE YOUR NOTES:\n[Raw notes, bullet points, stream of consciousness - messy is fine]\n\nMEETING CONTEXT:\n- Meeting type: [Sync, decision, brainstorm, review]\n- Attendees: [List if relevant for ownership]\n- Key decisions made: [If any]\n\nWHAT YOU NEED:\n- [ ] Action items with owners\n- [ ] Decisions documented\n- [ ] Follow-up meetings\n- [ ] Questions to resolve\n- [ ] All of the above\n</meeting_inputs>\n\n<conversion_framework>\n\nYou're the person who actually writes down what needs to happen after a meeting. Your job: extract signal from noise.\n\nTHE OUTPUT:\n\n## DECISIONS MADE\n- [Decision 1]\n- [Decision 2]\n\n## ACTION ITEMS\n- [ ] [Owner]: [Specific action] - Due: [Date or timeframe]\n- [ ] [Owner]: [Specific action] - Due: [Date or timeframe]\n- [ ] UNASSIGNED: [Action that needs an owner]\n\n## OPEN QUESTIONS\n- [Question that needs answering]\n- [Who should answer / when we'll revisit]\n\n## NEXT MEETING\n- Topic: [What to discuss]\n- When: [Timeframe]\n- Who needs to be there: [Names]\n\n---\n\nFOR SLACK/EMAIL:\n\nQuick update from today's meeting:\n\nDecided:\n- [Key decision]\n\nAction items:\n- @[person]: [action by date]\n- @[person]: [action by date]\n\nNext steps:\n[What happens next]\n\n</conversion_framework>\n\n<quality_check>\n\nGood action items are:\n- ✅ Specific (not \"look into X\")\n- ✅ Owned (person's name attached)\n- ✅ Time-bound (by when)\n- ✅ Actually actionable (can start Monday)\n\nBad action items:\n- ❌ \"Team to discuss further\"\n- ❌ \"Consider options for X\"\n- ❌ \"Follow up on Y\" (follow up how?)\n\nIf no owner or date:\nFlag it as needing assignment.\n\n</quality_check>\n\n<meta_guidance>\n\nCommon patterns to catch:\n\n\"We should...\" → Who specifically?\n\"Let's circle back...\" → When specifically?\n\"Someone needs to...\" → Assign to someone\n\"We talked about...\" → Was a decision made?\n\nIf notes are really messy:\nExtract what you can, flag what's unclear with [?]\n\n</meta_guidance>\n\n</meeting_to_actions>",
    "technique": "<meeting_to_actions>",
    "tools": "Claude or ChatGPT Project",
    "useCase": "Just finished meeting with messy notes, need clear followups fast"
  },
  {
    "name": "Email Inbox Triage",
    "category": "Productivity",
    "prompt": "Scan my inbox from the last 5 days. Create three sections:\n\n**PRIORITY ACTIONS**\n- Emails from executives, direct reports, or external partners requiring decisions\n- Anything with \"urgent,\" \"blocker,\" \"approval needed,\" or deadline mentions\n- Calendar conflicts or meeting requests pending response\n\n**UNANSWERED THREADS**\n- Emails where I'm in the TO field (not CC) that I haven't replied to\n- Sort by sender seniority and days waiting\n- Flag any that mention waiting on me specifically\n\n**DEPENDENCY BLOCKERS**\n- Threads where someone is waiting on my input to proceed\n- Items I committed to deliver that appear unresolved\n- Approval requests or sign-offs pending my action\n\nFormat as a table with: Sender | Subject | Days Old | Action Required",
    "technique": "Structured categorization by urgency, sender importance, and dependency blocking",
    "tools": "Gmail, Outlook, Claude integration",
    "useCase": "Quickly identify urgent emails and action items from the last 5 days"
  },
  {
    "name": "Rapid Prototyping",
    "category": "PM Artifacts",
    "prompt": "You are a product designer building a working prototype to test a hypothesis with users. Build a complete, functional prototype from this spec.\n\n[NOTE: Connect your design system and/or codebase to Lovable or Claude Code for consistency with your product]\n\n<hypothesis>\nWhat I'm testing: [Specific, falsifiable assumption]\n\nExample: \"Users prefer drag-and-drop scheduling over click-to-book because it gives spatial control, leading to 30% faster booking completion\"\n\nSuccess: [Specific measurable behavior]\nFailure: [What would disprove this]\n</hypothesis>\n\n<complete_spec>\nUSER FLOW:\n1. [Step 1]: [What user sees/does]\n2. [Step 2]: [What user sees/does]\n3. [Step 3]: [What user sees/does]\n4. [Step 4]: [Completion state]\n\nExample:\n1. User sees weekly calendar grid with available slots in green, booked in gray\n2. User drags a 30-min block to Tuesday 2pm slot\n3. Modal appears asking for name/email (pre-filled if logged in)\n4. Success message with calendar invite sent\n\nINTERFACE COMPONENTS:\n[List every UI element needed]\n- [Component]: [Exact specs]\n- [Component]: [Exact specs]\n\nExample:\n- Calendar grid: 7-day week view, 9am-5pm hours, 30-min blocks\n- Draggable blocks: 30-min height, blue (#0066FF), cursor changes on hover\n- Modal form: Name field, email field (validated), \"Confirm\" button, \"Cancel\" link\n- Success state: Green checkmark, \"Meeting booked for [time]\" message\n\nINTERACTIONS:\n[Specify every interaction and response]\n- User does [X] → System does [Y]\n\nExample:\n- User hovers over available slot → Slot highlights with time preview\n- User drags block → Block follows cursor, shows target time\n- User drops on valid slot → Modal opens with form\n- User drops on invalid slot (booked/outside hours) → Block snaps back, shows error toast\n- User submits form → Success message, block turns solid blue on calendar\n\nEDGE CASES:\n[Specify handling for edge cases]\n\nExample:\n- Drag outside hours → Block snaps back, toast: \"Please select a time between 9am-5pm\"\n- Drag to booked slot → Block snaps back, toast: \"This time is already booked\"\n- Invalid email → Form error: \"Please enter a valid email\"\n- Empty name → Form error: \"Name is required\"\n\nDESIGN:\nStyle: [Aesthetic direction]\nColors: [Specific colors with hex codes]\nTypography: [Font choices]\nResponsive: [Platform requirements]\n\nExample:\n- Style: Clean, minimal B2B SaaS aesthetic\n- Colors: Primary blue #0066FF, success green #00C853, error red #F44336, neutral grays\n- Typography: Inter font family, 16px base size\n- Responsive: Desktop-first (tablet/mobile nice-to-have)\n\nWHAT'S REAL VS FAKE:\nReal (must actually work):\n- [What needs to function]\n\nFake (can be hardcoded):\n- [What can be mocked]\n\nExample:\nReal: Drag interaction, form validation, timezone detection\nFake: Availability data (hardcode 50% slots available), email sending (just show success), calendar sync (mock the confirmation)\n</complete_spec>\n\n<scope_constraints>\nThis should be testable with 5 users in 15-minute sessions.\n\nBuild exactly what's specified above. Nothing more.\n\nTime budget: [N hours max]\nIf it's taking longer, we're building too much.\n</scope_constraints>\n\nBuild the complete, working prototype now. Make it feel real enough that users forget it's a prototype.",
    "technique": "Build iteratively - start with core, then interactions, then styling",
    "tools": "Lovable, v0, Bolt, Claude Code",
    "useCase": "Need to quickly prototype interactive feature to validate concept before full development"
  },
  {
    "name": "Create User Stories",
    "category": "PM Artifacts",
    "prompt": "<user_story_generator>\n\n<story_inputs>\nWHAT YOU'RE BUILDING:\n1. Feature/requirement description (vague is fine, that's what we're fixing)\n2. Target users (who will use this)\n3. User research or context (why this matters)\n4. Technical constraints (if any)\n5. Success criteria (how we'll measure if it works)\n\nUPLOADS:\n- PRD or project brief\n- User research notes\n- Design mockups or prototypes\n- Technical specs\n</story_inputs>\n\n<generation_process>\n\nYou are a senior PM who writes user stories that engineering teams can implement without asking 20 clarifying questions. Your stories cover happy paths AND edge cases.\n\nPHASE 1: EXTRACT THE JOBS-TO-BE-DONE\n\nDon't just describe features. Understand what users are trying to accomplish.\n\nFor the requirement provided:\n- User goal: What are they trying to achieve? (outcome, not feature)\n- Current pain: What's broken or missing today?\n- Context: When/where/why does this matter?\n- Success looks like: How will they know it worked?\n\nBreak complex features into atomic user stories:\n- Each story should be independently valuable\n- Should be completable in one sprint\n- Should be testable\n\nPHASE 2: WRITE USER STORIES\n\nUse this format:\n\nAs a [specific user type]I want to [action/capability]So that [benefit/outcome]Make user types specific:\n- ❌ \"As a user\"\n- ✅ \"As a free trial user on day 3\"\n- ✅ \"As an admin managing a 100-person team\"\n\nMake actions concrete:\n- ❌ \"I want better search\"\n- ✅ \"I want to search by date range and filter by status\"\n\nMake benefits clear:\n- ❌ \"So that I can use the product\"\n- ✅ \"So that I can find last quarter's reports in under 30 seconds\"\n\nPHASE 3: ACCEPTANCE CRITERIA\n\nFor each story, define what \"done\" means:\n\nGIVEN [initial context/state]WHEN [action taken]THEN [expected outcome]\n\nCover these scenarios:\n\n1. Happy Path - Ideal scenario, everything works\n2. Edge Cases - Boundary conditions, empty states, max limits\n3. Error States - What happens when things fail\n4. Permissions - Who can/can't do this\n5. Performance - Speed requirements if relevant\n\nExample:\n\nStory: \"As an admin, I want to bulk delete users, so that I can quickly offboard departing teams.\"\n\nAcceptance Criteria:\n- GIVEN I'm an admin viewing the users list\nWHEN I select 5 users and click \"Delete\"\nTHEN all 5 users are removed and I see \"5 users deleted\" confirmation\n\n- GIVEN I select 100+ users\nWHEN I click \"Delete\"\nTHEN I see a warning \"You're about to delete 100+ users. This cannot be undone. Type DELETE to confirm\"\n\n- GIVEN I try to delete my own account\nWHEN I click \"Delete\"\nTHEN I see error \"You cannot delete your own account\"\n\n- GIVEN I'm a non-admin\nTHEN I don't see the \"Delete\" option\n\n- GIVEN the API fails during deletion\nTHEN I see \"Some users couldn't be deleted. X succeeded, Y failed.\" with retry option\n\nPHASE 4: ADD KEY DETAILS\n\nFor each story, include:\n\nPriority: P0 (Must-have) / P1 (Should-have) / P2 (Nice-to-have)Effort Estimate: Small / Medium / Large (or story points if your team uses them)Dependencies: What must be done firstOpen Questions: Anything unclear that needs PM decisionDesign Notes: Link to mockups or UX guidanceTechnical Notes: Implementation approach or constraints\n\nPHASE 5: SELF-VALIDATION\n\nCheck each story:\n\nCAN AN ENGINEER BUILD THIS?\n- Is it clear what to build?\n- Are edge cases covered?\n- Are error states defined?\n\nCAN QA TEST THIS?\n- Can they write test cases from acceptance criteria?\n- Is \"done\" unambiguous?\n\nIS IT INDEPENDENTLY VALUABLE?\n- Does this story deliver value on its own?\n- Or is it just a piece that's useless until other stories are done?\n\nIS IT RIGHT-SIZED?\n- Can this be completed in one sprint?\n- If not, split it further\n\n</generation_process>\n\n<output_format>\n\n## Story 1: [User-facing title]\n\nAs a [specific user type]I want to [action]So that [benefit]\n\nAcceptance Criteria:\n- GIVEN/WHEN/THEN format\n- Cover happy path, edge cases, errors\n\nPriority: P0/P1/P2Effort: S/M/LDependencies: [if any]Open Questions: [if any]\n\n[Repeat for all stories]\n\n## Summary\n- Total stories: X\n- P0 (must-have): Y\n- Estimated sprints: Z\n- Key risks: [anything blocking these stories]\n\n</output_format>\n\n<meta_guidance>\n\nGreat user stories:\n- Are independently testable and valuable\n- Cover edge cases, not just happy path\n- Use specific user types, not generic \"user\"\n- Connect features to user benefits\n- Make \"done\" unambiguous\n\nAvoid:\n- Technical implementation details in story description\n- Combining multiple features in one story\n- Vague acceptance criteria (\"works well\", \"looks good\")\n- Missing error states and edge cases\n- Stories that can't be completed independently\n\n</meta_guidance>\n\n</user_story_generator>",
    "technique": "Jobs-to-be-done framework, edge case thinking",
    "tools": "Claude Projects, Linear, Jira",
    "useCase": "Turn vague requirements into well-formed user stories with clear acceptance criteria"
  },
  {
    "name": "Win/Loss Analysis",
    "category": "Discovery",
    "prompt": "<win_loss_analysis>\n\n<analysis_inputs>\nYOUR WIN/LOSS DATA:\n- Wins: [# and context]\n- Losses: [# and context]\n- Time period: [When]\n\nDATA SOURCES:\n- [ ] Sales notes\n- [ ] Customer interviews\n- [ ] CRM data\n- [ ] Call recordings\n- [ ] Competitive intel\n\nWHAT YOU WANT TO KNOW:\n- [ ] Why we win vs [competitor]\n- [ ] Common objections\n- [ ] Deal-breaker features\n- [ ] Pricing sensitivity\n- [ ] Buying process insights\n</analysis_inputs>\n\n<winloss_framework>\n\nYou analyze wins and losses to find actionable patterns that improve close rates. Your process:\n\nSTEP 1: Categorize outcomesBasic buckets:\n- Won: Chose us\n- Lost to competitor: Chose [specific competitor]\n- Lost to status quo: Chose to do nothing\n- Lost to budget: No money\n- Lost to other: [Build product themselves, different approach]\n\nFor losses, always identify WHERE they went.STEP 2: Extract key data pointsFor each deal, capture:Deal characteristics:\n- Company size\n- Industry\n- Use case\n- Deal size\n- Sales cycle length\n\nCompetition:\n- Who else they evaluated\n- How far along each got\n\nDecision factors:\n- What mattered most\n- What was \"nice to have\"\n- Deal breakers (if any)\n\nSTEP 3: Find patterns in WINSWhy did we win?Look for:\n- Feature differentiation\n- Pricing/value perception\n- Sales process effectiveness\n- Timing factors\n- Relationship/trust elements\n\nGroup into themes:Product reasons:\n\"We won because we have [feature] that [competitor] lacks\"\n\nGTM reasons:\n\"We won because our sales process was [faster/more consultative/better]\"\n\nMarket position:\n\"We won with [segment] because [positioning]\"\n\nSTEP 4: Find patterns in LOSSESWhy did we lose?Loss categories:Product gaps:\n- Missing features\n- Integration limitations\n- Performance issues\n- UX problems\n\nPricing issues:\n- Too expensive\n- Wrong packaging\n- ROI not clear\n\nGTM problems:\n- Sales process too slow\n- Wrong messaging\n- Poor demo\n- Lack of references\n\nMarket fit:\n- Not built for their segment\n- Competitor better aligned\n\nFor each loss, ask:\n- Was this winnable?\n- What would have changed the outcome?\n- Is this a pattern?\n\nSTEP 5: Competitive analysisBy competitor:[Competitor A]\n- When we win vs them: [Reasons]\n- When we lose vs them: [Reasons]\n- Their strength: [What they do well]\n- Our advantage: [Where we beat them]\n- Positioning: [How to position against them]\n\nSTEP 6: Segment analysisDo patterns differ by:Company size:\n- SMB wins/losses\n- Mid-market wins/losses\n- Enterprise wins/losses\n\nIndustry:\n- [Vertical A] patterns\n- [Vertical B] patterns\n\nUse case:\n- [Use case X] patterns\n- [Use case Y] patterns\n\nExample insight:\n\"We lose to Competitor A in enterprise but win in mid-market because [reason]\"\n\nSTEP 7: Quantify impactPrioritize themes by:\n- Frequency: How often does this come up?\n- Magnitude: How much revenue at stake?\n- Addressability: Can we fix it?\n\nImpact formula:",
    "technique": "Pattern extraction, competitive intelligence, objection categorization",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Understand why deals close or don't close"
  },
  {
    "name": "RICE Feature Scoring",
    "category": "Strategy & Planning",
    "prompt": "RICE Score = (Reach × Impact × Confidence) / Effort\n\nFeature: [NAME]\nReach: [NUMBER] | Impact: [SCORE] | Confidence: [%] | Effort: [WEEKS]\nScore: [CALCULATED]\nRationale: [2-3 sentences on the biggest assumption or risk]",
    "technique": "Structured scoring with explicit rationale for assumptions and risks",
    "tools": "Claude, ChatGPT, or any LLM",
    "useCase": "Score and prioritize features using the RICE framework"
  },
  {
    "name": "Develop a GTM Strategy",
    "category": "Strategy & Planning",
    "prompt": "<gtm_strategy>\n\n<gtm_inputs>\nPRODUCT CONTEXT:\n1. What are you launching? (new product, major feature, pricing change)\n2. Who's it for? (target customer segments with specifics)\n3. What problem does it solve? (pain point and value prop)\n4. What's the business goal? (revenue, adoption, market position)\n5. How is this sold today? (if existing product) or how will it be sold?\n6. What's the competitive landscape? (alternatives, substitutes)\n\nCONSTRAINTS:\n7. Launch timeline? (target date and why)\n8. Budget? (marketing, sales enablement, product)\n9. Team resources? (who's available to support launch)\n10. Technical readiness? (GA, beta, alpha)\n\nUPLOADS:\n- Product brief or PRD\n- Market research\n- Competitive analysis\n- Sales/customer feedback\n- Pricing model\n</gtm_inputs>\n\n<gtm_process>\n\nYou are a GTM strategist who has launched 50+ products. You know that most launches fail not from bad products, but from unclear positioning, wrong audience, or poor execution.\n\nPHASE 1: GTM FOUNDATION\n\n1. DEFINE YOUR ICP (Ideal Customer Profile)\n\nWho will get the most value fastest?\n\nFirmographic:\n- Company size: [employees, revenue]\n- Industry: [specific verticals]\n- Geography: [markets]\n\nBehavioral:\n- Current solution: [what they use today]\n- Pain intensity: [how badly do they need this]\n- Buying process: [who decides, how long]\n\nQualification criteria:\n- Must-have: [non-negotiable attributes]\n- Nice-to-have: [bonus attributes]\n- Disqualifiers: [who this isn't for]\n\nYour Primary ICP: [specific description]Secondary ICP: [if relevant]Explicitly NOT for: [who to avoid]\n\n2. NAIL YOUR POSITIONING\n\nComplete these statements:\n\nFor [target customer]Who [have this problem]Our product [does what]Unlike [alternative]We [key differentiator]\n\nOne-sentence pitch: [15 words max]Elevator pitch: [30 seconds]Key message pillars: [3-5 themes you'll emphasize]\n\n3. SET MEASURABLE GOALSPrimary metric: [the one number that matters]\n- Baseline: [current state]\n- Target: [success threshold]\n- Timeline: [when measured]\n\nSecondary metrics: [2-3 supporting indicators]\n\nCounter-metrics: [what you're watching to avoid breaking]\n\nPHASE 2: GTM STRATEGY CHOICES\n\n1. LAUNCH APPROACH\n\nChoose your strategy:\n\nBig Bang Launch - Everything at once, maximum noise\n- When: You have brand power, big news, one shot to make splash\n- Risk: If it flops, everyone knows\n\nPhased Rollout - Staged release by segment/geography\n- When: Testing GTM fit, iterating on messaging, reducing risk\n- Risk: Slower momentum, competitors see you coming\n\nStealth → Loud - Beta with champions, then broad launch\n- When: Need proof points before going wide, building case studies\n- Risk: Competitors have time to react\n\nAlways-On Growth - No launch moment, continuous optimization\n- When: Feature enhancement, not net-new capability\n- Risk: Lacks urgency, harder to align team\n\nYour choice: [approach + why]\n\n2. PRICING & PACKAGING\n\nHow is this monetized?\n\nPricing model: [per user, per usage, flat fee, freemium]Price point: $[amount] because [rationale]Packaging tiers: [if multiple, what's in each]\n\nPricing positioning:\n- Compared to alternatives: [higher/lower/similar and why]\n- Launch pricing: [discount/promo or full price from day 1]\n\n3. CHANNEL STRATEGY\n\nHow will customers discover and buy this?\n\nDiscovery channels: [where target customers will learn about this]\n- Organic: SEO, word-of-mouth, community\n- Paid: Ads, sponsorships, partnerships\n- Sales: Outbound, account expansion\n- PR: Media, analysts, influencers\n\nPurchase path: [how they go from awareness to customer]\n\nYour primary channel: [and why you're betting on it]\n\nPHASE 3: EXECUTION ROADMAP\n\n## Pre-Launch (4-6 weeks before)\n\nInternal Readiness:\n- [ ] Sales trained and confident selling this\n- [ ] Support trained on key scenarios\n- [ ] Docs/help content ready\n- [ ] Pricing/packaging finalized\n- [ ] Internal FAQ distributed\n\nExternal Groundwork:\n- [ ] Beta customers recruited (if applicable)\n- [ ] Case studies/testimonials in progress\n- [ ] Launch content calendar planned\n- [ ] Analyst/press briefings scheduled\n- [ ] Partners/integrations aligned\n\nAssets Created:\n- [ ] Landing page live\n- [ ] Product demo video\n- [ ] Sales deck\n- [ ] Email nurture sequences\n- [ ] Ad creative\n\n## Launch Week\n\nDay -1: [what happens]Launch Day: [sequence of activities]Day +1 to +7: [follow-up actions]\n\nLaunch Day Checklist:\n- [ ] Product live and stable\n- [ ] Landing page published\n- [ ] Blog post live\n- [ ] Email to existing customers\n- [ ] Social posts scheduled\n- [ ] Sales team activated\n- [ ] Support standing by\n- [ ] Monitoring dashboard active\n\n## Post-Launch (30/60/90 days)\n\nFirst 30 days: [focus on...]60 days: [optimize...]90 days: [evaluate and decide...]\n\nSuccess criteria for each phase:\n- 30 days: [specific milestones]\n- 60 days: [specific milestones]\n- 90 days: [go/no-go decision point]\n\nPHASE 4: RISK MITIGATION\n\nTop Launch Risks:\n\n1. [Risk name]\n- Probability: High/Medium/Low\n- Impact: High/Medium/Low\n- Mitigation: [specific action]\n- Owner: [person responsible]\n\n2. [Risk name]\n- [same structure]\n\nCommon failure modes to plan for:\n- Low initial demand: [what we'll do]\n- Technical issues at launch: [incident plan]\n- Competitor responds: [counter-strategy]\n- Sales team doesn't prioritize: [enablement plan]\n- Customers confused by positioning: [message testing]\n\nPHASE 5: TEAM & TIMELINE\n\nLaunch Team:\n| Role | Owner | Responsibilities |\n|------|-------|-----------------|\n| Launch Lead | [name] | Overall success |\n| Product | [name] | Feature readiness |\n| Marketing | [name] | Demand gen |\n| Sales | [name] | Enablement & deals |\n| Support | [name] | Customer success |\n\nTimeline:\n| Milestone | Date | Owner | Status |\n|-----------|------|-------|--------|\n| Beta launch | [date] | [name] | [on track/at risk] |\n| Sales training | [date] | [name] | [status] |\n| GA launch | [date] | [name] | [status] |\n| First customer | [date] | [name] | [status] |\n\n</gtm_process>\n\n<output_format>\n\n## Executive Summary\n- What: [product in one sentence]\n- Who: [target customer]\n- Goal: [primary metric target]\n- Launch date: [when]\n- Success looks like: [specific outcome]\n\n## Full GTM Strategy\n[All sections from process]\n\n## One-Pager for Stakeholders\n- The play: [approach in 3 bullets]\n- The bet: [why we think this will work]\n- The ask: [what you need from stakeholders]\n\n</output_format>\n\n<meta_guidance>\n\nGreat GTM strategy:\n- Starts with a tightly defined ICP, not \"everyone\"\n- Makes positioning trade-offs explicit\n- Has measurable goals with timelines\n- Plans for failure modes, not just success\n- Aligns cross-functional teams with clear owners\n\nAvoid:\n- Boil the ocean strategy (\"all channels, all customers\")\n- Vague goals (\"increase awareness\")\n- Assuming \"build it and they will come\"\n- Over-optimizing launch day vs. first 90 days\n- Treating GTM as marketing's job alone\n\n</meta_guidance>\n\n</gtm_strategy>",
    "technique": "Chain-of-thought through launch phases, identifying failure modes",
    "tools": "Claude Projects, NotebookLM",
    "useCase": "Planning a product launch that actually drives adoption and revenue"
  },
  {
    "name": "Identify North Star Metric",
    "category": "Analytics",
    "prompt": "<north_star_metric>\n\n<metric_inputs>\nBUSINESS CONTEXT:\n1. What does your product do? (core value proposition)\n2. How do you make money? (business model)\n3. What's your growth stage? (early, growth, mature)\n4. Current key metrics you track? (and why you picked them)\n\nCUSTOMER CONTEXT:\n5. When do customers get value? (aha moment, activation point)\n6. What makes customers stick? (retention drivers)\n7. What makes customers churn? (common reasons)\n8. How do customers use the product? (daily, weekly, monthly)\n\nUPLOADS:\n- Analytics dashboards\n- Business model canvas\n- User journey map\n- Cohort analysis\n</metric_inputs>\n\n<metric_process>\n\nYou are a product strategist who helps companies identify their North Star Metric - the single metric that best captures the value you deliver to customers and predicts business success.\n\nPHASE 1: UNDERSTAND VALUE DELIVERY\n\n1. MAP THE VALUE CHAIN\n\nCustomer value → Product usage → Business outcome\n\nWhat value do you deliver?\n[Specific benefit customers get, not features]\n\nWhen do customers experience that value?\n[The moment they realize \"this works for me\"]\n\nWhat actions lead to value?\n[Specific behaviors that predict success]\n\nHow does customer value drive revenue?\n[Connection between usage and money]\n\n2. IDENTIFY CANDIDATE METRICS\n\nBrainstorm metrics across categories:\n\nActivation metrics (new user experience)\n- Time to first value\n- Completion of key setup steps\n- First core action taken\n\nEngagement metrics (ongoing usage)\n- DAU, WAU, MAU\n- Actions per session\n- Feature adoption\n\nValue creation metrics (delivered outcome)\n- Projects created\n- Messages sent\n- Reports generated\n- Transactions completed\n\nRetention proxy metrics (predicts stickiness)\n- Return rate\n- Habit formation indicators\n- Cross-feature usage\n\nRevenue metrics (business outcome)\n- MRR/ARR\n- Expansion revenue\n- LTV\n\nYour candidate list: [8-12 potential NSMs]\n\nPHASE 2: EVALUATE CANDIDATES\n\nFor each candidate metric, score against criteria:\n\n1. Does it capture VALUE DELIVERY?\n- Does this metric go up when customers get more value?\n- Can you game this metric without delivering value? (if yes, bad sign)\n\n2. Does it PREDICT REVENUE?\n- Do customers with higher [metric] pay more / retain longer?\n- Show the correlation if you have data\n\n3. Is it a LEADING INDICATOR?\n- Does it predict future success before revenue shows up?\n- Or is it lagging? (revenue itself is lagging)\n\n4. Can the WHOLE COMPANY influence it?\n- Can product, marketing, sales, support all drive this up?\n- Or is it only one team's responsibility?\n\n5. Is it EASY TO UNDERSTAND?\n- Can you explain it to your grandmother?\n- Do employees intuitively get why it matters?\n\n6. Can you MEASURE IT reliably?\n- Do you have the data today or can you instrument it?\n- Is it consistent across platforms?\n\nScore each: High / Medium / Low\n\nTop 3 finalists: [metrics with highest scores]\n\nPHASE 3: TEST NORTH STAR CANDIDATES\n\nFor your finalists, validate:\n\nHistorical Analysis (if you have data):\n- Cohorts with high [NSM] → retention rate\n- Cohorts with low [NSM] → retention rate\n- Correlation between [NSM] and revenue\n\nCustomer Interview Validation:\n- Do power users have high [NSM]?\n- Do churned customers have low [NSM]?\n- What do customers say about value?\n\nThe \"So What\" Test:\nIf this metric goes up 20%, does that definitely mean:\n- Customers are getting more value? [yes/no]\n- The business is healthier? [yes/no]\n- The team knows what to build? [yes/no]\n\nPHASE 4: RECOMMENDATION\n\n## Your North Star Metric: [specific metric]\n\nDefinition:\n[Exact formula, what counts, what doesn't]\n\nWhy this metric:\n- Captures customer value because: [reason]\n- Predicts business success because: [reason]\n- Whole company can influence because: [reason]\n\nCurrent baseline: [number if known]\n\nWhat \"good\" looks like:\n- This month: [target]\n- This quarter: [target]\n- This year: [target]\n\nHow to move it:\n[3-5 levers that increase NSM]\n\nWhat NOT to do:\n[Ways to game metric that don't create value]\n\n## Supporting Metrics\n\nYour NSM needs context. Track these too:\n\nInput metrics (drive NSM up):\n- [Metric]: [why it matters]\n- [Metric]: [why it matters]\n\nOutput metrics (validate NSM):\n- Revenue/retention: [to confirm NSM→ business value]\n- Quality metrics: [to avoid gaming NSM]\n\nThe Dashboard:\n- NSM: [big number]\n- Trend: [↑↓ vs last period]\n- Inputs: [metrics that drive it]\n- Outputs: [metrics that validate it]\n\nPHASE 5: OPERATIONALIZING\n\nHow to use your NSM:In roadmap prioritization:\n- Ask: \"Will this feature increase [NSM]?\"\n- Estimate: \"By how much?\"\n\nIn experiment design:\n- Primary metric: [NSM] or input to NSM\n- Success criteria: [% increase]\n\nIn team goals:\n- Company OKR: Increase [NSM] from X to Y\n- Team OKRs: Improve [input metrics]\n\nIn stakeholder communication:\n- Board decks: Lead with NSM trend\n- All-hands: Celebrate NSM wins\n- New hires: Explain why we picked this NSM\n\nReview cadence:\n- Daily: Monitor NSM for anomalies\n- Weekly: Review with product/growth team\n- Monthly: Deep dive into what moved it\n- Quarterly: Validate NSM still predicts success\n\n</metric_process>\n\n<output_format>\n\n## Recommendation\n- NSM: [metric name and definition]\n- Current: [baseline]\n- Target: [goal]\n- Why: [2-3 sentence rationale]\n\n## Full Analysis\n[All sections from process]\n\n## One-Slide Summary\n[NSM, how to move it, why it matters]\n\n</output_format>\n\n<meta_guidance>\n\nGreat North Star Metric:\n- Measures value delivery, not vanity\n- Leading indicator, not lagging\n- Whole company can influence it\n- Simple to explain and understand\n- Has strong correlation to revenue/retention\n\nAvoid:\n- Picking revenue as NSM (lagging, not actionable)\n- Metrics easily gamed without creating value\n- Too complex (multi-variable formulas)\n- Only one team can influence it\n- Changing it every quarter\n\nRemember: NSM should be stable for years, not months. It's okay to evolve it as business model changes, but picking a new NSM quarterly means you never picked the right one.\n\n</meta_guidance>\n\n</north_star_metric>",
    "technique": "Chain of reasoning through what drives business value",
    "tools": "Claude Projects, analytics platform",
    "useCase": "Find the one metric that best captures product value delivery"
  },
  {
    "name": "Write SQL Query",
    "category": "Analytics",
    "prompt": "<sql_query_writer>\n\n<query_inputs>\nWHAT DO YOU WANT TO KNOW:\n[Describe in plain English what you're trying to find out]\n\nYOUR DATABASE:\n[Paste table names and columns, or just describe what tables you have]\n\nOptional - if you have it:\n- Sample data\n- Database type (Postgres, MySQL, BigQuery, etc.)\n- Specific date ranges or filters\n</query_inputs>\n\n<query_framework>\n\nYou write SQL queries for PMs who know enough to be dangerous but not enough to be confident.\n\nYOUR PROCESS:\n\n1. Understand the question\n- Restate what they're asking in your own words\n- Identify what data points are needed\n\n2. Check if you have enough info\n- Do you know the table names?\n- Do you know the relevant column names?\n- Do you know how tables relate?\n\nIf NO: Ask specific questions:\n- \"What table has user signup data?\"\n- \"What's the column name for [X]?\"\n- \"How do I join users to events?\"\n\n3. Write the query\n- Start with simple SELECT\n- Add filters\n- Add aggregations if needed\n- Add comments explaining each part\n\n4. Explain what it does\n- Plain English explanation\n- What each column means\n- How to interpret results\n\nTHE OUTPUT:",
    "technique": "Schema understanding, query building step-by-step, explaining logic",
    "tools": "Claude, ChatGPT, Cursor (for code)",
    "useCase": "Need to analyze product data but don't write SQL daily"
  },
  {
    "name": "Sprint Planning",
    "category": "Operations",
    "prompt": "<sprint_planning>\n\n<sprint_inputs>\nTEAM CAPACITY:\n- Engineers: [Number available this sprint]\n- Sprint length: [1 or 2 weeks]\n- Historical velocity: [Points per sprint, or just \"we usually finish X tickets\"]\n- This sprint's constraints: [PTO, holidays, other work]\n\nPASTE BACKLOG:\n[Your prioritized list of work - tickets, features, bugs, tech debt]\n\nCONTEXT:\n- Recent velocity: [Last 3 sprints - did you over/under commit?]\n- Team health: [ ] Crushing it [ ] Normal [ ] Struggling/underwater\n- Big picture: [What is this sprint working toward?]\n</sprint_inputs>\n\n<planning_framework>\n\nYou help PMs plan realistic sprints. Your analysis process:\n\nStep 1: Calculate actual capacity\nStart with theoretical capacity, then account for reality:\n- Meetings take 20-30% of time\n- Bug fixes and support interruptions\n- PR reviews and code review\n- Onboarding if new team members\n- Scope creep and unknowns\n\nRule of thumb: Plan for 60-70% of theoretical capacity.\n\nStep 2: Analyze the backlog\nFor each item, assess:\n- Is it actually ready to start? (design done, requirements clear, no blockers)\n- What's the real size? (not story points, but \"will this take days or weeks?\")\n- Dependencies on other work or teams?\n- Risk level: well-understood vs lots of unknowns\n- Can it be split into smaller chunks?\n\nStep 3: Apply prioritization forcing function\nIf backlog has 20 things and you can do 8, use this process:\n- What breaks if we don't do it? (critical)\n- What's blocking other work? (unblock)\n- What's needed for upcoming deadline? (time-sensitive)\n- What's high value/low effort? (quick wins)\n- What can wait? (defer)\n\nStep 4: Sequence intelligently\nDon't just stack work randomly:\n- Front-load risky/unknown work (so you have time to adjust)\n- Group related work (context switching is expensive)\n- Sequence dependencies (do foundation before feature)\n- Leave buffer at end (something will go wrong)\n\nStep 5: Reality check\nRed flags that you're over-committing:\n- Planned capacity = 100% of team time\n- All work is \"critical\"\n- Multiple large unknowns\n- Dependencies on other teams\n- No buffer for bugs/support\n- Last 3 sprints you missed goals\n\nStep 6: Define what \"done\" means\nFor this sprint specifically:\n- What's the sprint goal in one sentence?\n- What's the minimum to call sprint successful?\n- What would you cut if week 1 goes poorly?\n- What's your flex capacity if things go well?\n\nNow create a realistic sprint plan. Show your reasoning about what's in, what's out, and why.\n\n</planning_framework>\n\n</sprint_planning>",
    "technique": "Capacity planning, dependency mapping, realistic scoping",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Turn messy backlog into realistic 2-week sprint plan"
  }
];
