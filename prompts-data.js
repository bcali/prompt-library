// Auto-generated from BC Prompt Library.xlsx
window.promptsData = [
  {
    "name": "AI Product Strategy",
    "category": "AI Features",
    "prompt": "<ai_product_strategy>\n\n<strategy_inputs>\nYOUR PRODUCT:\n[What you build today]\n\nTARGET MARKET:\n[Who you serve]\n\nBUSINESS GOAL:\n[What AI needs to accomplish]\n\nCOMPETITIVE CONTEXT:\n[Who else is doing AI in your space]\n\nRESOURCES:\n[Team, budget, timeline]\n</strategy_inputs>\n\n<strategy_framework>\n\nYou develop AI strategies that create defensible moats. Your process:\n\nSTEP 1: Capability analysisWhat AI fits your use case:\n- LLMs: Text generation, understanding\n- ML: Prediction, classification\n- Computer vision: Image analysis\n- Agents: Multi-step workflows\n\nWhat's feasible with current AI:\n[What's proven vs experimental]\n\nKey limitations to design around:\n[Non-determinism, hallucinations, cost]\n\n---\n\nSTEP 2: Value propositionWhat becomes possible with AI:\n[Specific capabilities that didn't exist before]\n\nWhy it's defensible:\n- Proprietary data\n- Network effects\n- Accumulated learning\n- Integration depth\n\nWhy competitors can't easily copy:\n[Your specific moat]\n\n---\n\nSTEP 3: Market positioningHow you stand out:\n[Your unique AI angle]\n\nResponsible AI narrative:\n[How you address concerns]\n\nUser trust strategy:\n[How users gain confidence]\n\n---\n\nSTEP 4: Build sequenceMinimum viable AI:\n[First thing to ship]\n\nWhat to learn vs leverage:\n- Learn: [Where you need discovery]\n- Leverage: [What you can use off-shelf]\n\nFeature sequencing:\n[Order and why]\n\n---\n\nSTEP 5: Success metricsAI product-market fit indicators:\n[What proves it's working]\n\nRealistic goals:\n- 6 months: [Target]\n- 12 months: [Target]\n- 24 months: [Target]\n\nKill criteria:\n[What proves you should stop]\n\nNow create an AI product strategy for the context provided.\n\n</strategy_framework>\n\n---\n\n## Example AI Strategy\n\n### AI Product Strategy: [Product Name]\n\n---\n\n### 1. Capability Analysis\n\nAI technologies we're using:\n[LLMs for X, ML for Y, etc.]\n\nWhat's feasible:\n[What works reliably today]\n\nKey limitations:\n[What we must design around]\n\n---\n\n### 2. Value Proposition\n\nWhat AI enables:\n[Specific new capabilities]\n\nOur defensibility:\n[Why this creates moat]\n\nCompetitive advantage:\n[Why we'll win]\n\n---\n\n### 3. Positioning\n\nMarket angle:\n[How we're different]\n\nResponsible AI:\n[How we address concerns]\n\nTrust building:\n[User confidence strategy]\n\n---\n\n### 4. Build Plan\n\nMVP: [First AI feature]\n\nLearning needs: [What to discover]\n\nSequence:\n1. [Feature] - [Why first]\n2. [Feature] - [Why next]\n3. [Feature] - [Why last]\n\n---\n\n### 5. Success Metrics\n\nProduct-market fit signals:\n[What indicates it's working]\n\n6-month goal: [Target]12-month goal: [Target]24-month goal: [Target]\n\nKill if: [Criteria to stop]\n\n</ai_product_strategy>",
    "technique": "Why chain-of-thought: Strategy requires step-by-step reasoning to connect capabilities → value → business model",
    "tools": "Claude or ChatGPT Project",
    "useCase": "Building your AI product strategy from scratch"
  },
  {
    "name": "Catch Up on Slack Threads",
    "category": "Productivity",
    "prompt": "<slack_thread_synthesizer>\n\n<thread_inputs>\nPASTE THE SLACK THREAD:\n[Full thread content - copy/paste from Slack]\n\nCONTEXT QUESTIONS:\n1. Why are you reading this? (catching up after vacation, deciding if you need to weigh in, preparing for a meeting)\n2. What's your role in this discussion? (decision-maker, contributor, FYI only)\n3. Are there specific people whose opinions matter most to you?\n4. What decisions or action items are you hoping to find?\n5. How much detail do you need? (quick skim vs. deep understanding)\n</thread_inputs>\n\n<synthesis_process>\n\nYou are an expert at synthesizing long, meandering Slack threads into clear, actionable summaries. You identify the signal through the noise, tracking how discussions evolve and surfacing what actually matters.\n\nPHASE 1: THREAD ANALYSIS\n\nFirst, map the conversation structure:\n\n1. CONVERSATION FLOW\n- How many participants?\n- How long is the thread? (timestamp of first vs. last message)\n- Did the topic shift during the conversation?\n- Are there multiple sub-conversations happening in parallel?\n- Were there any long gaps? (discussion paused and resumed)\n\n2. IDENTIFY PARTICIPANTS & ROLES\nFor key participants, note:\n- Who started the thread and why?\n- Who are the decision-makers?\n- Who are subject matter experts?\n- Who's just reacting/observing?\n- Any conflicts or disagreements between people?\n\n3. TRACK TOPIC EVOLUTION\n- What was the original question/topic?\n- Did it evolve? (often threads drift from initial topic)\n- Were new issues raised mid-thread?\n- Was the original question answered?\n\nPHASE 2: EXTRACT KEY ELEMENTS\n\nPull out the important information:\n\n1. DECISIONS MADE\nFor each decision:\n- What was decided: [specific outcome]\n- Who decided: [person or consensus]\n- Rationale: [why this decision, what alternatives were considered]\n- When: [timestamp]\n- Finality: Is this locked in or still being debated?\n\nExample:\n❗DECISION: Will launch feature X with limited beta first, not full rollout\n- Decided by: @sarah (PM lead)\n- Why: Engineering raised concerns about scale; beta lets us test with 100 users first\n- When: Today at 2:47 PM\n- Status: Final, team aligned\n\n2. ACTION ITEMS\nFor each action item:\n- What needs to be done: [specific task]\n- Who owns it: [person assigned, or \"unassigned\" if unclear]\n- Deadline: [if mentioned, or \"unclear\"]\n- Blocker status: Does this block other work?\n\nFormat as checklist:\n- [ ] @jamie: Write PRD for beta launch (due Friday)\n- [ ] @engineering: Estimate effort for dashboard work (due this week) - BLOCKS design work\n- [ ] UNASSIGNED: Schedule follow-up meeting (no deadline mentioned)\n\n3. OPEN QUESTIONS\nQuestions that were raised but not answered:\n- \"What's our budget for this?\" - Asked by @mike, no response yet\n- \"Does this need legal review?\" - Discussed but no clear answer\n- \"Who's going to handle customer comms?\" - Multiple people asked, still unclear\n\n4. KEY CONTEXT & BACKGROUND\nImportant facts or context shared:\n- Customer X specifically requested this feature\n- We tried something similar 2 years ago and it failed because [reason]\n- Legal says we must comply with [regulation] for this\n- Our main competitor launched this last week\n\n5. CONCERNS & OBJECTIONS\nWho raised concerns and about what:\n- @engineering: Worried about performance at scale (message link)\n- @design: Thinks this UX will confuse users (message link)\n- @sales: Concerned this won't work for enterprise customers (message link)\n\nNote: Did concerns get addressed or are they still hanging?\n\n6. CONSENSUS vs. DISAGREEMENT\n- What does everyone agree on?\n- Where is there active disagreement?\n- Are disagreements resolved or still debating?\n\nPHASE 3: SYNTHESIZE SUMMARY\n\nCreate a structured summary:\n\n## Thread TL;DR (2-3 sentences)\n[What was this thread about and what's the current status? Bottom line up front.]\n\n## Key Decisions\n[List all decisions made, formatted as shown above]\n\n## Action Items\n[Checklist format with owners and deadlines]\n\n## Open Questions\n[List with who asked and whether discussed]\n\n## Important Context\n[Key facts, background, or constraints mentioned]\n\n## Concerns Raised\n[Who's worried about what, and whether addressed]\n\n## Current Status\n[Where does this stand? Is discussion complete or ongoing? What happens next?]\n\n## Your Role (if specified in context)\n[Based on your context, do you need to: weigh in on something, complete an action item, make a decision, or just stay informed?]\n\n## Related Threads or Documents\n[If mentioned: links to PRDs, previous threads, related discussions]\n\n</synthesis_process>\n\n<quality_checks>\n\nValidate your summary:\n\n1. COMPLETENESS CHECK\n- Did you capture all decisions, even small ones?\n- Are all action items accounted for, including implied ones?\n- Did you note ALL open questions, not just the most recent?\n\n2. ACCURACY CHECK\n- Are you quoting decisions correctly or adding your interpretation?\n- Did you attribute statements to the right people?\n- Are timestamps correct?\n\n3. CLARITY CHECK\n- Can someone who wasn't in the thread understand what happened?\n- Is it clear what's decided vs. still debating?\n- Are action item owners unambiguous?\n\n4. ACTIONABILITY CHECK\n- Is it clear what YOU need to do (if anything)?\n- Are next steps obvious?\n- Would someone know what to do Monday morning?\n\nSELF-CRITIQUE: If any check fails, strengthen that section.\n\n</quality_checks>\n\n<output_format>\n\nFor quick catch-ups (< 50 messages):\n- TL;DR\n- Decisions\n- Action Items\n- Your next step\n\nFor complex threads (50+ messages):\n- Full structure with all sections\n- Timeline of how discussion evolved\n- Link to key messages for deep-dive if needed\n\nFor urgent decision threads:\n- Lead with: \"NEEDS YOUR INPUT ON: [specific question]\"\n- Then standard structure\n\n</output_format>\n\n<meta_guidance>\n\nGood Slack summaries:\n- Front-load the most important information (decisions and action items)\n- Make it scannable (headers, bullets, formatting)\n- Distinguish between facts and opinions\n- Flag what still needs resolution\n- Are honest about what's unclear or messy\n\nAvoid:\n- Summarizing every message chronologically (that's not synthesis)\n- Losing context about WHY decisions were made\n- Missing implied action items (\"someone should...\")\n- Treating all opinions equally (flag the decision-maker's view)\n- Hiding important concerns in the middle of the summary\n\nRemember: Slack threads often meander. Your job is to extract the structure and outcomes from the chaos. When people disagree or the thread is messy, say so - don't pretend it's cleaner than it is.\n\n</meta_guidance>\n\n</slack_thread_synthesizer>",
    "technique": "Asks the AI to explain dependencies between steps (\"why each phase must come before the next\") rather than just listing a sequence",
    "tools": "LLM or Workflow: Lindy, Zapier",
    "useCase": "Long slack thread to digest"
  },
  {
    "name": " AI Roadmap Prioritization",
    "category": "AI Features",
    "prompt": "<ai_roadmap_prioritization>\n\n<roadmap_inputs>\nYOUR AI FEATURES:\n[List features with what AI capability they need]\n\nCURRENT STATE:\n- What AI infrastructure exists: [Vector DB? Evals? Monitoring?]\n- Team AI expertise: [Level]\n- Model access: [What you use]\n\nBUSINESS CONTEXT:\n- North Star Metric: [What matters]\n- Timeline pressure: [Any hard deadlines]\n- Strategic goal: [What you're proving]\n</roadmap_inputs>\n\n<roadmap_framework>\n\nYou sequence AI features based on dependencies and learning - not arbitrary timelines. Your process:\n\nSTEP 1: Score features\n\nFor each feature:\n- Impact (1-10): Effect on north star\n- AI Feasibility (1-10): Is this proven AI capability or research?\n- Technical Readiness (1-10): Can we build with current infrastructure?\n- Confidence (1-10): Do we have evidence this will work?\n- Strategic Value: Foundation feature (enables others) or standalone?\n\nSTEP 2: Map dependencies\n\nWhat must be built first:\n- Infrastructure: Vector DB before RAG features, Evals before production AI\n- Capabilities: Simple prompting before agents\n- Learning: Assisted features before fully automated\n\nSTEP 3: Identify learning sequences\n\nWhat must we learn:\n- Suggest before automate (learn what users accept)\n- Narrow before broad (one use case well, then expand)\n- Assisted before autonomous (build trust first)\n\nSTEP 4: CategorizeQuick Wins (4-6 weeks):\nHigh impact, uses existing infrastructure, low risk\n\nFoundation Features:\nEnables multiple future features, infrastructure investment\n\nHigh-Impact Bets:\nBiggest business impact, needs foundation first\n\nNot Now:\nLow ROI, too risky, or doesn't fit strategy\n\nSTEP 5: Build phases\n\nCreate 3 phases showing:\n- What ships when\n- Why this sequence (dependencies)\n- What we learn that enables next phase\n- Success criteria\n- Risk factors\n\nNow sequence the provided AI features.\n\n</roadmap_framework>\n\n---\n\n## Example Roadmap\n\n(Output adapts to features provided)\n\n### AI Feature Roadmap\n\nTimeline: [X months]North Star: [Metric]\n\n---\n\n### Feature Scores\n\n[Feature 1]:\n- Impact: [Score] - [Why]\n- Feasibility: [Score] - [Why]\n- Readiness: [Score] - [Why]\n- Confidence: [Score] - [Why]\n\n[Feature 2]:\n[Same structure]\n\n---\n\n### Dependencies\n\n[Feature X] requires:\n- [Infrastructure need]\n- [Prior feature]\n- [Learning from earlier phase]\n\n---\n\n### Phased Roadmap\n\nPhase 1 (Months 1-2): [Theme]Shipping:\n- [Feature A]\n- [Feature B]\n\nWhy this sequence:\n[Dependencies and reasoning]\n\nSuccess criteria:\n- [Metric]: [Target]\n\nEnables Phase 2 by:\n[What becomes possible]\n\nRisks:\n- [Risk]: [Mitigation]\n\n---\n\nPhase 2 (Months 3-4): [Theme]Only possible after Phase 1 because:\n[Specific dependencies]\n\n[Same structure]\n\n---\n\nPhase 3 (Months 5-6): [Theme]\n\n[Same structure]\n\n---\n\n### Not Now\n\n[Feature]: [Why not + when to revisit]\n\n</ai_roadmap_prioritization>",
    "technique": "Why task decomposition: Prioritization has multiple evaluation criteria that need systematic assessment",
    "tools": "Claude or ChatGPT Project",
    "useCase": "You have 10+ AI feature ideas and need to sequence them strategically"
  },
  {
    "name": "Writing PRDs",
    "category": "PM Artifacts",
    "prompt": "<prd_generator>\n\n<pm_inputs>\nAnswer these questions and upload any relevant materials:\n\nCORE CONTEXT:\n1. What are you building? (feature/product description)\n2. What user problem does this solve? (pain point, not solution)\n3. Who are the target users? (specific segments with characteristics)\n4. What's the current state? (baseline metrics, user complaints, workarounds)\n5. What does success look like? (specific outcomes and metrics)\n6. Why now? (market timing, competitive pressure, strategic priority)\n7. Why us? (unique advantage or capability)\n\nRESEARCH & VALIDATION:\n8. What user research validates this problem? (interviews, data, surveys)\n9. What have you tried before? (previous attempts and why they failed)\n10. What do competitors do? (specific solutions and their limitations)\n11. What's the cost of not solving this? (churn, revenue loss, support burden)\n\nCONSTRAINTS & DEPENDENCIES:\n12. When must this ship and why? (hard deadlines with business rationale)\n13. Who's available to build this? (team capacity and timeline)\n14. What technical debt or constraints affect this? (legacy systems, architecture)\n15. What political or stakeholder commitments exist? (promises made, expectations set)\n\nUPLOADS:\n- User research transcripts or recordings\n- Competitive analysis documents\n- Prototypes, mockups, or design files\n- Current analytics/metrics dashboards\n- Your company's PRD template or preferred headers (if not provided, default structure will be used)\n- Relevant technical documentation\n- Stakeholder feedback or requirements\n</pm_inputs>\n\n<prd_generation_process>\n\nYou are a senior product manager known for writing PRDs that engineering teams trust and actually enjoy reading. Your PRDs anticipate questions before they're asked, surface hidden complexity early, and make trade-offs explicit.\n\nPHASE 1: CRITICAL ANALYSIS\nBefore writing anything, analyze what's been provided:\n\n1. PROBLEM VALIDATION\n- Is this a real user problem or a solution in disguise?\n- Are we solving for a vocal minority or a meaningful segment?\n- What's the evidence strength? (anecdote < pattern < data < experiment)\n- Red flag check: Does this smell like feature creep, checkbox feature, or executive pet project?\n\n2. SCOPE CLARITY\n- What's the atomic version that delivers value? (MVP thinking)\n- What are we explicitly NOT doing? (these often matter more than what's in scope)\n- Where will scope creep come from? (anticipate the \"while we're at it\" requests)\n- What's the difference between Phase 1 and \"complete solution\"?\n\n3. HIDDEN COMPLEXITY AUDIT\nAsk about each of these until you find gaps:\n- Edge cases: What happens when users do unexpected things?\n- Scale: Does this work for 10 users? 10,000? 10 million?\n- States: What are all the states this feature can be in? (loading, error, empty, partial, complete)\n- Permissions: Who can see/do what? What about admins, read-only users, guests?\n- Integrations: What breaks when this interacts with existing features?\n- Data: Where does data come from? How do we handle missing/malformed/stale data?\n- Failures: What happens when APIs fail, networks drop, services are down?\n- Concurrency: What if multiple users edit simultaneously?\n- Migration: How do existing users/data transition to this new feature?\n- Rollback: If we must turn this off, what breaks?\n\n4. DEPENDENCY MAPPING\nIdentify and categorize all dependencies:\n- Blocking: Can't start without these\n- Critical path: Delays here delay launch\n- Nice-to-have: Doesn't block MVP\n- External: Third-party services, other teams, legal/security review\nFor each dependency: What's the risk and what's the mitigation?\n\nPHASE 2: PRD STRUCTURE\nUse the provided template if uploaded. If no template provided, use this default structure:\n\n## Problem\nDescribe the problem we are trying to solve in 1-2 sentences. Someone should be able to read this and communicate the customer & business value + risks. Highlight evidence or insights you have to support this.\n\n## High Level Approach\nDescribe the rough shape of how we might tackle the problem. This should be a few sentences max.\n\n## Narrative\nShare (hypothetical) stories to paint a picture of what life looks like for customers today. Describe common and edgy use cases to consider when designing the solution.\n\n## Goals\nDescribe high-level goals, ideally in priority order and not too many. Include measurable (metrics) and immeasurable (feelings) goals. Keep it short and sweet.\n\n### Metrics\nHighlight specific, operationalized north star metric, secondary metrics, and guardrail metrics. Be clear what amount of movement in the guardrail metric would be unacceptable.\n\n### Impact Sizing Model\nCreate a model taking the known information to calculate impact size against the key identified metrics, as well as relevant bottom-line metrics like revenue/profit. State the estimated impact on the north star and bottom-line metrics. Show the calculation steps to get there.\n\n## Non-goals\nList explicit areas we do not plan to address. Explain why they are not goals. These are as important and clarifying as the goals.\n\n## Solution Alignment\nDraw the perimeter. Do not force others to identify scope.\n\n## Key Features\n\n### Plan of Record\nList the features that shape the solution, ideally in priority order. Think of this like drawing the perimeter of the solution space. Draw the boundaries so the team can focus on how to fill it in. Challenge the size to see if a smaller component can be shipped independently.\n\n### Future Considerations\nOptionally list features you are saving for later. These might inform how you build now.\n\n## Key Flows\nShow what the end-to-end experience will be for customers. Get detailed here screen by screen, not high level. Highlight what screens a user is on, what they click, and the result.\n\n## Key Logic\nList rules to guide design and development. Address common scenarios and edge cases. Make it so an engineer feels everything is thought through.\n\n## Launch Plan\nDefine the various phases that will get this product to market, the purpose of each phase, and the criteria you must meet to move on to the next one. Put this into a table.\n\nHighlight whether it will be an A/B test and how long would be needed for stat sig results based on north star metric goal.\n\nHighlight risks and dependencies that can throw a wrench in timelines or progress (and ideally contingency plans). Create a table.\n\n## Key Milestones\nHighlight the key developmental and design steps to go. Put this in a table.\n\n---\n\nPHASE 3: QUALITY CHECKS & SELF-IMPROVEMENT\nAfter drafting the PRD, run these diagnostic checks and improve the draft:\n\n1. THE ENGINEER TEST\nRead the PRD as a skeptical engineer:\n- Can they start building without asking 10 clarifying questions?\n- Are edge cases specified, not left as \"we'll figure it out\"?\n- Are integration points clear?\n- Is success measurable, not subjective?\n\nSELF-CRITIQUE: Identify 3 specific places where an engineer would get stuck or need clarification. Fix them.\n\n2. THE DESIGNER TEST\nRead as a product designer:\n- Are user flows complete?\n- Are all states specified (loading, error, empty, success)?\n- Are interaction patterns clear?\n- Is accessibility mentioned?\n\nSELF-CRITIQUE: Find 2 missing states or edge cases in the flows. Add them to Key Logic.\n\n3. THE QA TEST\nRead as a QA engineer:\n- Can they write test cases from this?\n- Are error conditions specified?\n- Are performance expectations quantified?\n- Is the definition of \"done\" unambiguous?\n\nSELF-CRITIQUE: List 3 scenarios that would be hard to test based on the current PRD. Clarify them in Key Logic.\n\n4. THE EXECUTIVE TEST\nRead as a busy executive:\n- Can they understand the why in 2 minutes?\n- Is the impact quantified?\n- Are risks and mitigations clear?\n- Is the ask explicit?\n\nSELF-CRITIQUE: If an exec skimmed only the Problem, Goals, and Impact Sizing Model sections, would they have enough to make a go/no-go decision? If not, strengthen those sections.\n\n5. THE FUTURE-YOU TEST\nImagine reading this 6 months from now:\n- Will you remember why decisions were made?\n- Are trade-offs documented?\n- Is context preserved?\n\nSELF-CRITIQUE: Add a sentence or two in High Level Approach or Non-goals explaining the key trade-off or alternative considered.\n\n6. THE COMPLETENESS CHECK\n- Problem section: Does it include evidence/insights?\n- Impact Sizing Model: Are calculations shown step-by-step with clear assumptions?\n- Key Logic: Are common scenarios AND edge cases both covered?\n- Launch Plan: Are risks AND contingency plans both documented?\n- Non-goals: Are reasons explained for each?\n\nSELF-CRITIQUE: Flag any section that feels rushed or incomplete. Expand it.\n\n7. THE SPECIFICITY CHECK\nSearch the PRD for vague language:\n- \"Improve user experience\" → Replace with specific metric\n- \"Many users\" → Replace with actual numbers\n- \"Better performance\" → Replace with quantified target\n- \"If possible\" or \"Maybe\" → Replace with definitive scope decision\n- \"We'll figure it out\" → Replace with open question or decision\n\nSELF-CRITIQUE: Find and fix 3 instances of vague language.\n\nPHASE 4: IMPACT VALIDATION\nAfter self-improvement, validate your Impact Sizing Model:\n\nSANITY CHECKS:\n- Are the assumptions clearly stated?\n- Is the math shown step-by-step?\n- Does the estimated impact justify the effort?\n- What's the confidence level? (High/Medium/Low - be honest)\n- What could make this estimate wrong by 50%+?\n\nSHOW YOUR WORK:\nFor the north star metric impact:\n- Current baseline: [number]\n- Estimated change: [number]\n- Calculation: [show the formula]\n- Key assumptions: [list them]\n- Confidence level: [High/Medium/Low] because [reasoning]\n\nFor bottom-line impact (revenue/profit):\n- Show the conversion from north star metric to dollars\n- Be conservative (don't assume 100% adoption on day 1)\n- Account for ramp time\n\nRED FLAGS TO CALL OUT:\n- \"This estimate is based on limited data\"\n- \"We're assuming X but haven't validated it\"\n- \"This could be 2x lower if Y doesn't work\"\n\n</prd_generation_process>\n\n<output_format>\n1. Start with gap analysis (1-2 paragraphs):\n- What's missing or unclear from the inputs that will weaken the PRD?\n- What critical questions need answers before this is ready?\n\n2. Generate the complete PRD following the structure above\n\n3. Self-improvement commentary (after the PRD):\n- Changes made during quality checks\n- Remaining weak spots or assumptions\n- What you'd want to validate before presenting this\n\n4. End with action items:\n- Top 3 risks that need mitigation before kickoff\n- The single most important question to answer before proceeding\n- Recommended next steps with owners and timelines\n\n5. Pro tips for this specific PRD:\n- Common pitfalls for this type of feature\n- What similar projects have taught us\n- Where scope creep typically comes from\n</output_format>\n\n<meta_guidance>\nYour PRD should be:\n- Comprehensive but scannable (use headers, bullets, bold for key points)\n- Specific not vague (\"improve conversion by 15%\" not \"better user experience\")\n- Honest about uncertainty (flag assumptions clearly)\n- Forward-looking (anticipate questions before they're asked)\n- Decision-focused (give stakeholders what they need to say yes/no)\n\nThe self-improvement phase is CRITICAL:\n- Don't just write a PRD - actively look for holes and fix them\n- Call out your own weak assumptions\n- Strengthen vague sections before presenting\n- Show that you've thought through edge cases\n\nAvoid:\n- Jargon without definition\n- Solutions masquerading as problems\n- Unmeasurable success criteria\n- Handwaving over complexity\n- \"Nice to have\" features in MVP\n- Assumptions buried as facts\n- Vague language that lets you off the hook\n\nRemember: A great PRD makes the hard decisions explicit, surfaces hidden complexity early, and gives everyone confidence that we're building the right thing the right way. The self-critique process is what separates good PRDs from great ones.\n</meta_guidance>\n\n</prd_generator>",
    "technique": "Quote specific user feedback, chain-of-thought analysis",
    "tools": "Claude or ChatGPT Project",
    "useCase": "Need to create comprehensive product requirements document for new feature"
  },
  {
    "name": "Synthesizing User Research",
    "category": "Discovery",
    "prompt": "<user_research_synthesizer>\n\n<research_inputs>\nUpload your research materials and answer these questions:\n\nRESEARCH MATERIALS:\n- Interview transcripts (upload all)\n- Survey results and raw data\n- User feedback from support tickets, reviews, or forums\n- Observational notes or usability test recordings\n- Analytics data showing behavioral patterns\n- Any existing synthesis or insights documents\n\nCONTEXT QUESTIONS:\n1. What are you trying to learn? (research goals/questions)\n2. Who are these users? (segments, demographics, use cases)\n3. What decisions will this research inform? (product, strategy, roadmap)\n4. Are there any hypotheses you're testing?\n5. What's the timeline for using these insights? (urgency level)\n6. Who's the audience for the synthesis? (executives, designers, engineers, all)\n7. What format do you need? (report, presentation, insights doc, user personas)\n</research_inputs>\n\n<synthesis_process>\n\nYou are a senior UX researcher known for finding the signal in the noise and translating user feedback into actionable product insights. Your syntheses reveal patterns that others miss and prioritize insights that drive decisions.\n\nPHASE 1: IMMERSION & INVENTORY\nFirst, understand what you're working with:\n\n1. SCAN ALL MATERIALS\n- How many users/participants total?\n- What research methods were used? (interviews, surveys, observations, etc.)\n- What date ranges does this cover?\n- Are there different user segments represented?\n- What was the original research goal?\n\n2. IDENTIFY DATA QUALITY\n- Which sources are strongest? (direct quotes, behavioral data, repeated patterns)\n- Which sources are weakest? (secondhand reports, vague feedback, one-off comments)\n- Are there any biases in who was included/excluded?\n- What's missing that would strengthen the synthesis?\n\nPHASE 2: THEMATIC ANALYSIS\nUse rigorous qualitative analysis:\n\n1. OPEN CODING (First Pass)\nRead through all materials and tag every distinct concept, pain point, desire, or behavior mentioned. Cast a wide net - don't filter yet.\n\nFor each code:\n- Quote the specific user feedback (exact words when possible)\n- Note which user(s) said it\n- Note the context (what they were doing, trying to accomplish)\n\n2. PATTERN IDENTIFICATION (Second Pass)\nGroup codes into themes:\n- Which concepts appear repeatedly across multiple users?\n- Which appear across different research methods? (stronger signal)\n- Which only appear once? (might be outlier or emerging need)\n- Are there contradictions? (different user segments wanting different things)\n\nFor each theme, calculate:\n- Frequency: How many users mentioned this? (X out of Y participants)\n- Intensity: How strongly did they feel? (blocker vs. annoyance)\n- Segment distribution: Which user types does this affect?\n\n3. FIND THE \"WHY\" BEHIND THE \"WHAT\"\nUsers often describe symptoms, not root causes:\n- If users say \"this is too slow\" → Why does speed matter in their workflow?\n- If users say \"I can't find X\" → Why were they looking for X? What were they trying to do?\n- If users say \"I want feature Y\" → What problem would Y solve for them?\n\nExtract the underlying jobs-to-be-done, not just feature requests.\n\nPHASE 3: PRIORITIZATION FRAMEWORK\nNot all insights are equal. Prioritize by:\n\n1. IMPACT POTENTIAL\n- Critical: Blocking users from core workflows, driving churn\n- High: Causing significant friction, daily pain points\n- Medium: Occasional frustration, workarounds exist\n- Low: Nice-to-have, edge cases\n\n2. EVIDENCE STRENGTH\n- Strong: Mentioned by multiple users, observed in behavior data, appears across methods\n- Moderate: Mentioned by several users, consistent story\n- Weak: One or two mentions, anecdotal, might be outlier\n\n3. ACTIONABILITY\n- Clear: We know what to build to address this\n- Fuzzy: We understand the problem but solution is unclear\n- Exploratory: Needs more research to understand\n\nCreate a 2x2 matrix: Impact (High/Low) vs. Evidence Strength (Strong/Weak)\nInsights in \"High Impact + Strong Evidence\" quadrant are your top priorities.\n\nPHASE 4: SYNTHESIS OUTPUT\n\nGenerate a comprehensive research synthesis with these sections:\n\n## Executive Summary\n- Key findings (top 3-5 insights) in priority order\n- Bottom line: What should we do based on this research?\n- Confidence level in these findings (and what would increase confidence)\n\n## Research Overview\n- Participants: [number, segments, how recruited]\n- Methods: [interviews, surveys, etc. with sample sizes]\n- Timeline: [when research was conducted]\n- Goals: [what we were trying to learn]\n\n## Key Insights\n\nFor each insight (top 8-10):\n\n### [Insight Title - User-Centric, Not Feature-Focused]\nWhat we learned: [1-2 sentence summary]\n\nEvidence:\n- Quote from User A: \"[exact words]\"\n- Quote from User B: \"[exact words]\"\n- Behavioral data: [specific numbers/patterns observed]\n- Frequency: [X out of Y users mentioned this]\n\nWhy this matters:\n[Impact on user experience, business metrics, product strategy]\n\nUser segments affected:\n[Which types of users care most about this]\n\nCurrent user workarounds:\n[How are users trying to solve this now, if at all]\n\nPotential solutions to explore:\n[2-3 directions we could take, not prescriptive]\n\nPriority: Critical/High/Medium/Low\nEvidence strength: Strong/Moderate/Weak\nActionability: Clear/Fuzzy/Needs more research\n\n## Themes & Patterns\nGroup related insights into larger themes:\n- What are the 3-4 big patterns across all findings?\n- How do these themes connect to our product strategy?\n- Which themes are table-stakes vs. differentiators?\n\n## Segment-Specific Insights\nIf relevant, break out findings by user segment:\n- What does Segment A uniquely care about?\n- Where do segments agree vs. disagree?\n- Should we optimize for one segment over another?\n\n## Contradictions & Open Questions\nBe honest about what's unclear:\n- Where did users contradict each other?\n- What questions does this research raise?\n- What would we want to learn next?\n\n## Recommended Next Steps\nBased on these insights:\n\nImmediate Actions (do now):\n- [Specific, actionable items with owners]\n\nShort-term (next quarter):\n- [Research to validate, features to explore]\n\nLong-term (future strategy):\n- [Bigger opportunities to investigate]\n\nWhat NOT to do:\n- [Ideas this research invalidated]\n\n</synthesis_process>\n\n<quality_checks>\n\nAfter creating the synthesis, validate it:\n\n1. THE QUOTE TEST\n- Is every major insight backed by at least 2-3 direct user quotes?\n- Are quotes in users' own words, not paraphrased?\n- Do quotes capture the emotion, not just the fact?\n\n2. THE FREQUENCY TEST\n- Is it clear how many users mentioned each insight? (X out of Y)\n- Have you distinguished between \"every user said this\" vs. \"one user mentioned it\"?\n- Are priorities based on patterns, not individual voices?\n\n3. THE ACTIONABILITY TEST\n- Can a PM read this and know what to consider building?\n- Can a designer read this and know what problems to solve?\n- Can an executive read this and make strategic decisions?\n\n4. THE HONESTY TEST\n- Have you acknowledged weak signals and contradictions?\n- Are you clear about what you don't know?\n- Have you avoided cherry-picking quotes to support pre-existing beliefs?\n\n5. THE SEGMENT TEST\n- If different user types want different things, is that clear?\n- Have you called out any underrepresented segments?\n- Are you clear about who this research does and doesn't represent?\n\nSELF-CRITIQUE: Improve any sections that fail these tests before finalizing.\n\n</quality_checks>\n\n<output_format>\n\n1. Start with the Executive Summary - Decision-makers should get the key takeaways in 2 minutes\n\n2. Full synthesis document following the structure above\n\n3. Appendix materials (separate from main doc):\n- Full quotes database organized by theme\n- Participant details table\n- Raw frequency counts for each code/theme\n- Methodology notes and any limitations\n\n4. One-pager version for busy stakeholders:\n- Top 3 insights only\n- 1 representative quote each\n- Recommended actions\n- Fits on one page/slide\n\n</output_format>\n\n<meta_guidance>\n\nGreat research synthesis:\n- Surfaces non-obvious patterns, not just confirmation of what we expected\n- Uses users' exact words to bring findings to life\n- Distinguishes between what users say and what they actually need\n- Makes priorities clear through evidence, not opinion\n- Acknowledges uncertainty and contradictions honestly\n- Connects insights to business/product decisions\n\nAvoid:\n- Death by bullet points (use quotes and stories)\n- Treating all feedback equally (prioritize!)\n- Reporting features users asked for without understanding why\n- Cherry-picking quotes to support pre-existing beliefs\n- Burying key insights deep in the document\n- Vague insights like \"users want it to be easier\" without specifics\n\nRemember: Your job is not to report everything users said. It's to find the patterns, prioritize the insights, and make it easy for your team to make good decisions. When in doubt, include the exact user quote - it's more powerful than your paraphrase.\n\n</meta_guidance>\n\n</user_research_synthesizer>",
    "technique": "Quote specific user feedback to support each insight",
    "tools": "NotebookLM, Gemini",
    "useCase": "Have large amounts of user research data that needs to be analyzed and synthesized into actionable insights"
  },
  {
    "name": "Feature Prioritization",
    "category": "Strategy & Planning",
    "prompt": "<feature_prioritization>\n\n<prioritization_inputs>\nFEATURES TO EVALUATE:\n1. [Feature]: [What it does]\n2. [Feature]: [What it does]\n3. [Feature]: [What it does]\n[Add more]\n\nYOUR CONTEXT:\n[Add company goals, team size, strategic priorities to your Project]\n</prioritization_inputs>\n\n<prioritization_framework>\n\nYou prioritize by ROI - dollar impact vs dollar cost. Your process:\n\nSTEP 1: Calculate impact (annual $)\n\nFor each feature estimate:\n\nRevenue impact:\n- New sales: [# deals × $deal size]\n- Expansion: [# customers × $expansion]\n- Retention: [% churn reduced × $ARR at risk]\n\nCost savings:\n- Support reduction: [% tickets × $cost per ticket]\n- Operational efficiency: [time saved × hourly rate]\n\nStrategic value:\n- Market position: [$ we don't lose to competitor]\n- Platform value: [Enables $ from future features]\n\nShow your work. Force yourself to quantify.\n\n---\n\nSTEP 2: Calculate cost (total $)Development:\n- Engineering: [weeks × $cost per eng-week]\n- Design: [weeks × $cost per week]\n- PM/QA: [loaded cost]\n\nOngoing:\n- Maintenance: [monthly cost × 36 months]\n- Support: [incremental load]\n- Infrastructure: [hosting, tools]\n\nHidden costs:\n- Technical debt created\n- Opportunity cost of not building [alternative]\n\n---\n\nSTEP 3: Calculate ROIROI = Annual Impact / Total CostBut also consider:\n- Time to value (when does $ start?)\n- Reversibility (can we undo if wrong?)\n- Strategic importance (unlocks market?)\n- Learning value (what do we discover?)\n\n---\n\nSTEP 4: Force tradeoffsCompounding vs One-time:\nDoes this enable future features or dead end?\n\nSignal vs Noise:\nWho's asking: 1 loud customer or 50 quiet ones?\n\nStrategic tax:\nSome low-ROI features are required (compliance, enterprise)\nFlag these explicitly\n\nNow prioritize the provided features by ROI.\n\n</prioritization_framework>\n\n---\n\n## Example Prioritization\n\n### Feature Prioritization Analysis\n\n---\n\n### High ROI (Build First)\n\n1. [Feature Name]Impact: $[X]K annually\n- [$Y]K new revenue ([breakdown])\n- [$Z]K retention ([breakdown])\n\nCost: $[A]K\n- [B] eng-weeks × $[C]\n- [Ongoing costs]\n\nROI: [X]x\n\nTime to value: [When]\n\nRationale: [Why this wins - specific dollars]\n\nRisk: [What must be true]\n\n---\n\n2. [Feature Name]\n[Same structure]\n\n---\n\n### Medium ROI (If Capacity)\n\n[List with abbreviated detail]\n\n---\n\n### Low ROI (Decline)\n\n[Feature]: $[X]K / $[Y]K = [Z]x ROI\n- Why not: [Low impact relative to effort]\n- Revisit if: [What would change it]\n\n---\n\n### Strategic Must-Haves\n\n[Feature]: [Why required despite low ROI]\n\n---\n\n### Need More Info\n\n[Feature]: Can't estimate because [specific unknown]\n- Get: [Data needed] then re-evaluate\n\n</feature_prioritization>",
    "technique": "Before scoring, reason through the trade-offs",
    "tools": "ChatGPT Projects, Claude Projects, DeepSeek",
    "useCase": "Need to prioritize competing feature requests from multiple stakeholders with limited engineering capacity"
  },
  {
    "name": "Building Product Roadmaps",
    "category": "PM Artifacts",
    "prompt": "<product_roadmap>\n\n<roadmap_inputs>\nWHERE YOU NEED TO BE:\n[6-month vision - capabilities, not features]\n\nBUSINESS GOAL:\n[Revenue target, market position, competitive defense]\n\nHARD CONSTRAINTS:\n- Timeline: [Why this timeframe]\n- Commitments: [What must ship]\n- Resources: [Team size, any bottlenecks]\n\nCURRENT STATE:\n[What exists today, technical debt]\n</roadmap_inputs>\n\n<roadmap_framework>\n\nYou build roadmaps where each phase enables the next. Your process:\n\nSTEP 1: Work backwards\n\nStart from 6-month goal:\n- What must exist for this to be true?\n- What must be built before that?\n- What's the first domino?\n\nSTEP 2: Identify phase themes\n\nEach 2-month phase needs:\n- Clear theme (not feature list)\n- Why this must come first\n- What it enables\n- Success metric\n\nSTEP 3: Map dependencies\n\nTechnical: Can't build B without A\nLearning: Need to discover X to build Y\nMarket: Need proof point before next segment\nResource: Can't parallelize without more people\n\nSTEP 4: Build in reality\n\nAssume 20% of time goes to unplanned work\nAdd slack between phases\nHave kill criteria if phase fails\n\nSTEP 5: Show the chain\n\nFor each phase explain:\n- Why this sequence works\n- What could break it\n- What you learn\n- How it unlocks next phase\n\nNow create a phased roadmap for the goal described.\n\n</roadmap_framework>\n\n---\n\n## Example Roadmap\n\n### 6-Month Roadmap: [Goal]\n\nVision: [End state]Why: [Business outcome]\n\n---\n\n### Phase 1 (M1-2): [Theme]\n\nBuilding:\n[Key initiatives]\n\nWhy first:\n[Dependency - creates foundation for X, learns Y]\n\nSuccess metric:\n[How we know it worked]\n\nEnables Phase 2 by:\n[What becomes possible]\n\nRisk:\n[What could derail] - [Mitigation]\n\n---\n\n### Phase 2 (M3-4): [Theme]\n\nOnly possible after Phase 1 because:\n[Specific dependency]\n\n[Same structure]\n\n---\n\n### Phase 3 (M5-6): [Theme]\n\n[Same structure]\n\n---\n\n### Why This Sequence Works\n\n[The chain of reasoning - how each phase unlocks the next]\n\n</product_roadmap>",
    "technique": "Chain your reasoning - explain why each phase must come before the next",
    "tools": "Claude Projects, Manus, Gemini",
    "useCase": "Need to create multi-month roadmap balancing competing priorities and constraints"
  },
  {
    "name": "Rapid Prototyping",
    "category": "PM Artifacts",
    "prompt": "You are a product designer building a working prototype to test a hypothesis with users. Build a complete, functional prototype from this spec.\n\n[NOTE: Connect your design system and/or codebase to Lovable or Claude Code for consistency with your product]\n\n<hypothesis>\nWhat I'm testing: [Specific, falsifiable assumption]\n\nExample: \"Users prefer drag-and-drop scheduling over click-to-book because it gives spatial control, leading to 30% faster booking completion\"\n\nSuccess: [Specific measurable behavior]\nFailure: [What would disprove this]\n</hypothesis>\n\n<complete_spec>\nUSER FLOW:\n1. [Step 1]: [What user sees/does]\n2. [Step 2]: [What user sees/does]\n3. [Step 3]: [What user sees/does]\n4. [Step 4]: [Completion state]\n\nExample:\n1. User sees weekly calendar grid with available slots in green, booked in gray\n2. User drags a 30-min block to Tuesday 2pm slot\n3. Modal appears asking for name/email (pre-filled if logged in)\n4. Success message with calendar invite sent\n\nINTERFACE COMPONENTS:\n[List every UI element needed]\n- [Component]: [Exact specs]\n- [Component]: [Exact specs]\n\nExample:\n- Calendar grid: 7-day week view, 9am-5pm hours, 30-min blocks\n- Draggable blocks: 30-min height, blue (#0066FF), cursor changes on hover\n- Modal form: Name field, email field (validated), \"Confirm\" button, \"Cancel\" link\n- Success state: Green checkmark, \"Meeting booked for [time]\" message\n\nINTERACTIONS:\n[Specify every interaction and response]\n- User does [X] → System does [Y]\n\nExample:\n- User hovers over available slot → Slot highlights with time preview\n- User drags block → Block follows cursor, shows target time\n- User drops on valid slot → Modal opens with form\n- User drops on invalid slot (booked/outside hours) → Block snaps back, shows error toast\n- User submits form → Success message, block turns solid blue on calendar\n\nEDGE CASES:\n[Specify handling for edge cases]\n\nExample:\n- Drag outside hours → Block snaps back, toast: \"Please select a time between 9am-5pm\"\n- Drag to booked slot → Block snaps back, toast: \"This time is already booked\"\n- Invalid email → Form error: \"Please enter a valid email\"\n- Empty name → Form error: \"Name is required\"\n\nDESIGN:\nStyle: [Aesthetic direction]\nColors: [Specific colors with hex codes]\nTypography: [Font choices]\nResponsive: [Platform requirements]\n\nExample:\n- Style: Clean, minimal B2B SaaS aesthetic\n- Colors: Primary blue #0066FF, success green #00C853, error red #F44336, neutral grays\n- Typography: Inter font family, 16px base size\n- Responsive: Desktop-first (tablet/mobile nice-to-have)\n\nWHAT'S REAL VS FAKE:\nReal (must actually work):\n- [What needs to function]\n\nFake (can be hardcoded):\n- [What can be mocked]\n\nExample:\nReal: Drag interaction, form validation, timezone detection\nFake: Availability data (hardcode 50% slots available), email sending (just show success), calendar sync (mock the confirmation)\n</complete_spec>\n\n<scope_constraints>\nThis should be testable with 5 users in 15-minute sessions.\n\nBuild exactly what's specified above. Nothing more.\n\nTime budget: [N hours max]\nIf it's taking longer, we're building too much.\n</scope_constraints>\n\nBuild the complete, working prototype now. Make it feel real enough that users forget it's a prototype.",
    "technique": "Build iteratively - start with core, then interactions, then styling\n\n",
    "tools": "Lovable, v0, Bolt, Claude Code",
    "useCase": "Need to quickly prototype interactive feature to validate concept before full development"
  },
  {
    "name": "Analyze Feature Requests",
    "category": "Discovery",
    "prompt": "<feature_request_analyzer>\n\n<request_inputs>\nUpload your feature request data and answer these questions:\n\nFEATURE REQUEST DATA:\n- Raw feature requests (CSV, spreadsheet, support tickets, sales notes)\n- CRM data with customer context (company size, revenue, segment)\n- Usage data (which customers are active vs. churning)\n- Previous feature request history (what we've heard before)\n\nCONTEXT QUESTIONS:\n1. Where are these requests coming from? (sales calls, support tickets, in-app feedback, user interviews)\n2. What's the time period? (last month, quarter, year)\n3. How many total requests do you have?\n4. What do you already know? (patterns you suspect but haven't validated)\n5. What decisions will this inform? (roadmap, prioritization, customer conversations)\n6. Are there specific customers whose feedback matters more? (enterprise, high-paying, strategic)\n7. What's your current prioritization challenge? (too many requests, conflicting asks, unclear patterns)\n</request_inputs>\n\n<analysis_process>\n\nYou are a product strategist who excels at finding signal in noisy customer feedback. You distinguish between what customers ask for versus what they actually need, and identify patterns that reveal strategic opportunities.\n\nPHASE 1: DATA CLEANING & ENRICHMENT\n\n1. STANDARDIZE THE CHAOS\nFeature requests come in messy. Clean them up:\n- Remove duplicates (same request worded differently)\n- Standardize terminology (\"reporting\" = \"analytics\" = \"dashboards\")\n- Extract the actual request from conversation context\n- Flag vague requests that need clarification (\"make it better\", \"improve UX\")\n\n2. ENRICH WITH CONTEXT\nFor each request, capture:\n- Customer details: Company name, segment, ARR, tenure\n- Source: Sales call, support ticket, user interview, in-app feedback\n- Urgency signals: Blocker, nice-to-have, deal risk, churn risk\n- Competitive context: \"Competitor X has this\"\n- Frequency: First time mentioned or repeated ask\n- Workarounds: What are they doing instead?\n\n3. CATEGORIZE BY TYPE\nNot all requests are created equal:\n\nFeature Requests - New capabilities\n- Example: \"Add SSO integration\"\n- What to track: Complexity, strategic value\n\nEnhancements - Improvements to existing features\n- Example: \"Export should include more fields\"\n- What to track: Impact on adoption, usage data\n\nBug Reports Disguised as Requests - Things that should already work\n- Example: \"Make loading faster\" (it shouldn't be slow)\n- What to track: How broken is it currently\n\nSolutions Looking for Problems - Customer proposing implementation\n- Example: \"Add a button that does X\"\n- What to track: Underlying need, not proposed solution\n\nComplaints - Venting, not actionable\n- Example: \"This sucks\"\n- What to track: Which part actually needs fixing\n\nPHASE 2: PATTERN ANALYSIS\n\n1. CLUSTER BY UNDERLYING NEED\nGroup requests that solve the same problem:\n\nExample clusters:\n- Better Reporting (requests for exports, dashboards, analytics, custom reports)\n- Collaboration (requests for comments, sharing, notifications, @mentions)\n- Mobile Access (requests for app, responsive design, offline mode)\n\nFor each cluster:\n- Theme name: [Clear, user-centric description]\n- Underlying need: [What problem are users trying to solve?]\n- Number of requests: [X total, Y unique customers]\n- ARR represented: $[amount] from customers asking\n- Customer segments: [Enterprise: X%, SMB: Y%]\n- Sample requests: [3-4 representative quotes]\n\n2. FREQUENCY ANALYSIS\nHow often is this requested?\n\nHigh Frequency (mentioned by 20%+ of requesting customers)\n- Clear market signal\n- Likely competitive requirement\n- Risk if we don't build\n\nMedium Frequency (5-20% of customers)\n- Specific segment need\n- Potential differentiator\n- Validate before committing\n\nLow Frequency (<5% of customers)\n- Might be edge case\n- Could be emerging need\n- Might be customer-specific\n\nFor each cluster, calculate:\n- Total mentions across all time\n- Unique customers requesting\n- Trend: Increasing, stable, or decreasing over time\n\n3. SEGMENT ANALYSIS\nDo different customer types want different things?\n\nBreak down by:\n- Company size: Enterprise vs. SMB vs. Startup\n- Industry: Healthcare vs. Finance vs. SaaS etc.\n- Geography: US vs. EU vs. APAC\n- Product tier: Free vs. Paid vs. Enterprise\n- Customer health: Happy vs. At-risk vs. Churning\n\nKey questions:\n- Are high-value customers asking for different things than low-value?\n- Are churning customers asking for things current customers don't care about?\n- Is there a segment-specific opportunity (build for enterprise, ignore SMB)?\n\n4. URGENCY & IMPACT ANALYSIS\n\nFor each request cluster, assess:\n\nDeal Impact:\n- How many deals are blocked by not having this? [number]\n- Total ARR at risk: $[amount]\n- Quote from sales: \"[specific example of lost deal]\"\n\nChurn Risk:\n- How many customers threatened to leave over this? [number]\n- ARR at risk: $[amount]\n- Quote from customer: \"[their actual words about churning]\"\n\nAdoption Blocker:\n- Are customers signing up but not using product because of this?\n- Activation rate for customers who mention this: [%]\n- Quote: \"[why they're not using the product]\"\n\nCompetitive Pressure:\n- How many requests mention competitors having this? [number]\n- Which competitors: [list]\n- Quote: \"[Competitor X does this and it's better]\"\n\nCreate urgency tiers:\n- Critical: Blocking deals/causing churn right now\n- High: Significant friction, workarounds exist but painful\n- Medium: Nice to have, competitive pressure\n- Low: Individual preferences, edge cases\n\nPHASE 3: STRATEGIC SYNTHESIS\n\n1. THE TRUTH BEHIND THE REQUESTS\n\nFor top request clusters, analyze:\n\nWhat they're asking for:\n[Specific feature requested]\n\nWhat they actually need:\n[Underlying job-to-be-done]\n\nWhy the distinction matters:\n[How we might solve it differently]\n\nExample:\n- Asking for: \"CSV export with 50 fields\"\n- Actually need: \"Share data with non-users\"\n- Better solution: Scheduled email reports, shareable links\n\n2. PATTERN INSIGHTS\n\nIdentify non-obvious patterns:\n\nCompensating Behaviors:\n- What workarounds reveal pain: \"Customers copy-paste into Excel daily\"\n- What this tells us: [our workflow doesn't match theirs]\n\nRequest Combinations:\n- Which requests appear together: \"SSO + SCIM + Audit logs\"\n- What this reveals: [enterprise compliance bundle]\n\nEvolution Over Time:\n- New requests appearing: [emerging needs]\n- Declining requests: [we built it, or they stopped asking]\n\nProxy Requests:\n- Request as symptom: \"Add more filters\"\n- Real problem: [search/organization is fundamentally broken]\n\n3. PRIORITIZATION MATRIX\n\nCreate a 2x2 framework:\n\nImpact Axes:\n- X-axis: Customer value (deal impact, churn prevention, adoption)\n- Y-axis: Strategic value (differentiation, technical leverage, market position)\n\nPlot top 15-20 request clusters.\n\nQuadrants:High Customer Value + High Strategic Value → BUILD NOW\n- [List requests here with ARR impact]\n\nHigh Customer Value + Low Strategic Value → QUICK WINS\n- [Features that satisfy customers but don't differentiate]\n\nLow Customer Value + High Strategic Value → FUTURE BETS\n- [Strategic but not urgent, consider for later]\n\nLow Customer Value + Low Strategic Value → DON'T BUILD\n- [Politely decline or offer workarounds]\n\n4. BUILD vs. BUY vs. WORKAROUND\n\nFor top requests, recommend approach:\n\nBuild In-House:\n- Core differentiation\n- Unique to our product\n- Customer pays premium\n\nBuy/Integrate:\n- Commodity capability\n- Best-in-class solution exists\n- Faster time-to-value\n\nWorkaround/Documentation:\n- Edge case\n- Existing feature can do this (they don't know)\n- Document the workflow\n\nPolitely Decline:\n- Out of scope\n- Wrong customer segment\n- Misaligned with strategy\n\nPHASE 4: OUTPUT DELIVERABLES\n\n## Executive Summary\nTotal Requests Analyzed: [number] requests from [number] unique customers representing $[ARR]\n\nTop 3 Insights:\n1. [Most important pattern found]\n2. [Second most important finding]\n3. [Surprising or counterintuitive insight]\n\nTop 5 Request Clusters to Act On:\n[Prioritized list with ARR impact and recommended action]\n\nRequests to Decline:\n[What we should explicitly not build and why]\n\n## Full Analysis Report\n\n### Request Clusters (Top 15)\n\nFor each cluster:\n\n#### [Cluster Name]\nThe Ask: [What customers are requesting]\nThe Need: [Underlying problem to solve]\nFrequency: X requests, Y customers, $Z ARR\nUrgency: Critical/High/Medium/Low\nEvidence:\n- [Customer quote 1]\n- [Customer quote 2]\n- [Deal impact data]\n\nSegment Breakdown:\n- Enterprise: [%]\n- SMB: [%]\n- Churning customers: [%]\n\nCompetitive Context:\n[Which competitors have this, customer quotes mentioning competitors]\n\nRecommendation: Build / Buy / Workaround / Decline\nRationale: [Why this recommendation]\nEstimated Effort: [T-shirt size]\nEstimated Impact: [ARR, churn reduction, conversion lift]\n\n### Segment-Specific Insights\n[What Enterprise wants vs. SMB vs. different industries]\n\n### Requests We Should NOT Build\n[Explicit no's with reasoning]\n\n### Data Quality Notes\n[Any biases, gaps, or limitations in the analysis]\n\n## Actionable Next Steps\n\nImmediate (Next Sprint):\n- [ ] [Action with owner]\n- [ ] [Action with owner]\n\nShort-term (This Quarter):\n- [ ] [Initiative with owner]\n- [ ] [Validation needed before committing]\n\nLong-term (Next Year):\n- [ ] [Strategic bet to explore]\n\nCustomer Communication:\n- [ ] Respond to top 10 requesting customers with roadmap update\n- [ ] Document workarounds for common requests\n- [ ] Update sales team on what's coming vs. what's not\n\n</analysis_process>\n\n<quality_checks>\n\n1. PATTERN VALIDITY\n- Are clusters based on underlying need, not surface feature requests?\n- Have you validated that \"reporting\" requests are all solving the same problem?\n\nSELF-CRITIQUE: Find 2 clusters that might actually be solving different needs. Split them.\n\n2. BIAS CHECK\n- Are you over-weighting loud customers vs. quiet majority?\n- Are enterprise requests drowning out SMB legitimate needs?\n- Are recent requests over-represented vs. long-standing pain?\n\nSELF-CRITIQUE: Identify one segment you might be ignoring.\n\n3. EVIDENCE STRENGTH\n- Is every priority backed by specific customer quotes and ARR numbers?\n- Are you distinguishing \"everyone wants this\" from \"3 vocal customers want this\"?\n\nSELF-CRITIQUE: Find vague claims like \"customers want better UX\". Quantify them.\n\n4. ACTIONABILITY\n- Can product team start working on this Monday?\n- Are recommendations clear (build/buy/decline)?\n- Is the \"why\" behind prioritization explicit?\n\nSELF-CRITIQUE: Add one sentence to each top recommendation explaining the trade-off.\n\n</quality_checks>\n\n<output_format>\n\nFor executives:\n- 1-page summary with top 5 clusters, ARR impact, recommendations\n\nFor product team:\n- Full analysis with customer quotes, segment breakdowns, prioritization rationale\n\nFor customer-facing teams:\n- What we're building, what we're not, how to respond to requests\n\nFollow-up artifacts:\n- Shareable roadmap update for requesting customers\n- Internal FAQ for sales/support (\"When will we build X?\")\n\n</output_format>\n\n<meta_guidance>\n\nGreat feature request analysis:\n- Reveals patterns customers don't see themselves\n- Distinguishes must-haves from nice-to-haves with evidence\n- Makes hard trade-offs explicit (build this, not that)\n- Connects requests to business metrics (ARR, churn, deals)\n- Honest about what you're NOT building\n\nAvoid:\n- Treating all requests equally (\"customers want 47 things\")\n- Reporting features word-for-word without finding patterns\n- Prioritizing by whoever yelled loudest\n- Ignoring the \"why\" behind requests\n- Building everything (inability to say no)\n\nRemember: Your job isn't to build everything customers ask for. It's to understand what they need, find the patterns, and build the 20% of features that solve 80% of problems. When in doubt, talk to the customers directly—don't just read requests, understand the context.\n\n</meta_guidance>\n\n</feature_request_analyzer>",
    "technique": "Pattern recognition, clustering similar requests, extracting underlying needs",
    "tools": "Claude Projects, Airtable, Linear",
    "useCase": "Flooded with feature requests from customers, sales, support—need to find patterns and prioritize"
  },
  {
    "name": "Create User Stories",
    "category": "PM Artifacts",
    "prompt": "<user_story_generator>\n\n<story_inputs>\nWHAT YOU'RE BUILDING:\n1. Feature/requirement description (vague is fine, that's what we're fixing)\n2. Target users (who will use this)\n3. User research or context (why this matters)\n4. Technical constraints (if any)\n5. Success criteria (how we'll measure if it works)\n\nUPLOADS:\n- PRD or project brief\n- User research notes\n- Design mockups or prototypes\n- Technical specs\n</story_inputs>\n\n<generation_process>\n\nYou are a senior PM who writes user stories that engineering teams can implement without asking 20 clarifying questions. Your stories cover happy paths AND edge cases.\n\nPHASE 1: EXTRACT THE JOBS-TO-BE-DONE\n\nDon't just describe features. Understand what users are trying to accomplish.\n\nFor the requirement provided:\n- User goal: What are they trying to achieve? (outcome, not feature)\n- Current pain: What's broken or missing today?\n- Context: When/where/why does this matter?\n- Success looks like: How will they know it worked?\n\nBreak complex features into atomic user stories:\n- Each story should be independently valuable\n- Should be completable in one sprint\n- Should be testable\n\nPHASE 2: WRITE USER STORIES\n\nUse this format:\n\nAs a [specific user type]I want to [action/capability]So that [benefit/outcome]Make user types specific:\n- ❌ \"As a user\"\n- ✅ \"As a free trial user on day 3\"\n- ✅ \"As an admin managing a 100-person team\"\n\nMake actions concrete:\n- ❌ \"I want better search\"\n- ✅ \"I want to search by date range and filter by status\"\n\nMake benefits clear:\n- ❌ \"So that I can use the product\"\n- ✅ \"So that I can find last quarter's reports in under 30 seconds\"\n\nPHASE 3: ACCEPTANCE CRITERIA\n\nFor each story, define what \"done\" means:\n\nGIVEN [initial context/state]WHEN [action taken]THEN [expected outcome]\n\nCover these scenarios:\n\n1. Happy Path - Ideal scenario, everything works\n2. Edge Cases - Boundary conditions, empty states, max limits\n3. Error States - What happens when things fail\n4. Permissions - Who can/can't do this\n5. Performance - Speed requirements if relevant\n\nExample:\n\nStory: \"As an admin, I want to bulk delete users, so that I can quickly offboard departing teams.\"\n\nAcceptance Criteria:\n- GIVEN I'm an admin viewing the users list\nWHEN I select 5 users and click \"Delete\"\nTHEN all 5 users are removed and I see \"5 users deleted\" confirmation\n\n- GIVEN I select 100+ users\nWHEN I click \"Delete\"\nTHEN I see a warning \"You're about to delete 100+ users. This cannot be undone. Type DELETE to confirm\"\n\n- GIVEN I try to delete my own account\nWHEN I click \"Delete\"\nTHEN I see error \"You cannot delete your own account\"\n\n- GIVEN I'm a non-admin\nTHEN I don't see the \"Delete\" option\n\n- GIVEN the API fails during deletion\nTHEN I see \"Some users couldn't be deleted. X succeeded, Y failed.\" with retry option\n\nPHASE 4: ADD KEY DETAILS\n\nFor each story, include:\n\nPriority: P0 (Must-have) / P1 (Should-have) / P2 (Nice-to-have)Effort Estimate: Small / Medium / Large (or story points if your team uses them)Dependencies: What must be done firstOpen Questions: Anything unclear that needs PM decisionDesign Notes: Link to mockups or UX guidanceTechnical Notes: Implementation approach or constraints\n\nPHASE 5: SELF-VALIDATION\n\nCheck each story:\n\nCAN AN ENGINEER BUILD THIS?\n- Is it clear what to build?\n- Are edge cases covered?\n- Are error states defined?\n\nCAN QA TEST THIS?\n- Can they write test cases from acceptance criteria?\n- Is \"done\" unambiguous?\n\nIS IT INDEPENDENTLY VALUABLE?\n- Does this story deliver value on its own?\n- Or is it just a piece that's useless until other stories are done?\n\nIS IT RIGHT-SIZED?\n- Can this be completed in one sprint?\n- If not, split it further\n\n</generation_process>\n\n<output_format>\n\n## Story 1: [User-facing title]\n\nAs a [specific user type]I want to [action]So that [benefit]\n\nAcceptance Criteria:\n- GIVEN/WHEN/THEN format\n- Cover happy path, edge cases, errors\n\nPriority: P0/P1/P2Effort: S/M/LDependencies: [if any]Open Questions: [if any]\n\n[Repeat for all stories]\n\n## Summary\n- Total stories: X\n- P0 (must-have): Y\n- Estimated sprints: Z\n- Key risks: [anything blocking these stories]\n\n</output_format>\n\n<meta_guidance>\n\nGreat user stories:\n- Are independently testable and valuable\n- Cover edge cases, not just happy path\n- Use specific user types, not generic \"user\"\n- Connect features to user benefits\n- Make \"done\" unambiguous\n\nAvoid:\n- Technical implementation details in story description\n- Combining multiple features in one story\n- Vague acceptance criteria (\"works well\", \"looks good\")\n- Missing error states and edge cases\n- Stories that can't be completed independently\n\n</meta_guidance>\n\n</user_story_generator>",
    "technique": "Jobs-to-be-done framework, edge case thinking",
    "tools": "Claude Projects, Linear, Jira",
    "useCase": "Turn vague requirements into well-formed user stories with clear acceptance criteria"
  },
  {
    "name": "Develop a GTM Strategy",
    "category": "Strategy & Planning",
    "prompt": "<gtm_strategy>\n\n<gtm_inputs>\nPRODUCT CONTEXT:\n1. What are you launching? (new product, major feature, pricing change)\n2. Who's it for? (target customer segments with specifics)\n3. What problem does it solve? (pain point and value prop)\n4. What's the business goal? (revenue, adoption, market position)\n5. How is this sold today? (if existing product) or how will it be sold?\n6. What's the competitive landscape? (alternatives, substitutes)\n\nCONSTRAINTS:\n7. Launch timeline? (target date and why)\n8. Budget? (marketing, sales enablement, product)\n9. Team resources? (who's available to support launch)\n10. Technical readiness? (GA, beta, alpha)\n\nUPLOADS:\n- Product brief or PRD\n- Market research\n- Competitive analysis\n- Sales/customer feedback\n- Pricing model\n</gtm_inputs>\n\n<gtm_process>\n\nYou are a GTM strategist who has launched 50+ products. You know that most launches fail not from bad products, but from unclear positioning, wrong audience, or poor execution.\n\nPHASE 1: GTM FOUNDATION\n\n1. DEFINE YOUR ICP (Ideal Customer Profile)\n\nWho will get the most value fastest?\n\nFirmographic:\n- Company size: [employees, revenue]\n- Industry: [specific verticals]\n- Geography: [markets]\n\nBehavioral:\n- Current solution: [what they use today]\n- Pain intensity: [how badly do they need this]\n- Buying process: [who decides, how long]\n\nQualification criteria:\n- Must-have: [non-negotiable attributes]\n- Nice-to-have: [bonus attributes]\n- Disqualifiers: [who this isn't for]\n\nYour Primary ICP: [specific description]Secondary ICP: [if relevant]Explicitly NOT for: [who to avoid]\n\n2. NAIL YOUR POSITIONING\n\nComplete these statements:\n\nFor [target customer]Who [have this problem]Our product [does what]Unlike [alternative]We [key differentiator]\n\nOne-sentence pitch: [15 words max]Elevator pitch: [30 seconds]Key message pillars: [3-5 themes you'll emphasize]\n\n3. SET MEASURABLE GOALSPrimary metric: [the one number that matters]\n- Baseline: [current state]\n- Target: [success threshold]\n- Timeline: [when measured]\n\nSecondary metrics: [2-3 supporting indicators]\n\nCounter-metrics: [what you're watching to avoid breaking]\n\nPHASE 2: GTM STRATEGY CHOICES\n\n1. LAUNCH APPROACH\n\nChoose your strategy:\n\nBig Bang Launch - Everything at once, maximum noise\n- When: You have brand power, big news, one shot to make splash\n- Risk: If it flops, everyone knows\n\nPhased Rollout - Staged release by segment/geography\n- When: Testing GTM fit, iterating on messaging, reducing risk\n- Risk: Slower momentum, competitors see you coming\n\nStealth → Loud - Beta with champions, then broad launch\n- When: Need proof points before going wide, building case studies\n- Risk: Competitors have time to react\n\nAlways-On Growth - No launch moment, continuous optimization\n- When: Feature enhancement, not net-new capability\n- Risk: Lacks urgency, harder to align team\n\nYour choice: [approach + why]\n\n2. PRICING & PACKAGING\n\nHow is this monetized?\n\nPricing model: [per user, per usage, flat fee, freemium]Price point: $[amount] because [rationale]Packaging tiers: [if multiple, what's in each]\n\nPricing positioning:\n- Compared to alternatives: [higher/lower/similar and why]\n- Launch pricing: [discount/promo or full price from day 1]\n\n3. CHANNEL STRATEGY\n\nHow will customers discover and buy this?\n\nDiscovery channels: [where target customers will learn about this]\n- Organic: SEO, word-of-mouth, community\n- Paid: Ads, sponsorships, partnerships\n- Sales: Outbound, account expansion\n- PR: Media, analysts, influencers\n\nPurchase path: [how they go from awareness to customer]\n\nYour primary channel: [and why you're betting on it]\n\nPHASE 3: EXECUTION ROADMAP\n\n## Pre-Launch (4-6 weeks before)\n\nInternal Readiness:\n- [ ] Sales trained and confident selling this\n- [ ] Support trained on key scenarios\n- [ ] Docs/help content ready\n- [ ] Pricing/packaging finalized\n- [ ] Internal FAQ distributed\n\nExternal Groundwork:\n- [ ] Beta customers recruited (if applicable)\n- [ ] Case studies/testimonials in progress\n- [ ] Launch content calendar planned\n- [ ] Analyst/press briefings scheduled\n- [ ] Partners/integrations aligned\n\nAssets Created:\n- [ ] Landing page live\n- [ ] Product demo video\n- [ ] Sales deck\n- [ ] Email nurture sequences\n- [ ] Ad creative\n\n## Launch Week\n\nDay -1: [what happens]Launch Day: [sequence of activities]Day +1 to +7: [follow-up actions]\n\nLaunch Day Checklist:\n- [ ] Product live and stable\n- [ ] Landing page published\n- [ ] Blog post live\n- [ ] Email to existing customers\n- [ ] Social posts scheduled\n- [ ] Sales team activated\n- [ ] Support standing by\n- [ ] Monitoring dashboard active\n\n## Post-Launch (30/60/90 days)\n\nFirst 30 days: [focus on...]60 days: [optimize...]90 days: [evaluate and decide...]\n\nSuccess criteria for each phase:\n- 30 days: [specific milestones]\n- 60 days: [specific milestones]\n- 90 days: [go/no-go decision point]\n\nPHASE 4: RISK MITIGATION\n\nTop Launch Risks:\n\n1. [Risk name]\n- Probability: High/Medium/Low\n- Impact: High/Medium/Low\n- Mitigation: [specific action]\n- Owner: [person responsible]\n\n2. [Risk name]\n- [same structure]\n\nCommon failure modes to plan for:\n- Low initial demand: [what we'll do]\n- Technical issues at launch: [incident plan]\n- Competitor responds: [counter-strategy]\n- Sales team doesn't prioritize: [enablement plan]\n- Customers confused by positioning: [message testing]\n\nPHASE 5: TEAM & TIMELINE\n\nLaunch Team:\n| Role | Owner | Responsibilities |\n|------|-------|-----------------|\n| Launch Lead | [name] | Overall success |\n| Product | [name] | Feature readiness |\n| Marketing | [name] | Demand gen |\n| Sales | [name] | Enablement & deals |\n| Support | [name] | Customer success |\n\nTimeline:\n| Milestone | Date | Owner | Status |\n|-----------|------|-------|--------|\n| Beta launch | [date] | [name] | [on track/at risk] |\n| Sales training | [date] | [name] | [status] |\n| GA launch | [date] | [name] | [status] |\n| First customer | [date] | [name] | [status] |\n\n</gtm_process>\n\n<output_format>\n\n## Executive Summary\n- What: [product in one sentence]\n- Who: [target customer]\n- Goal: [primary metric target]\n- Launch date: [when]\n- Success looks like: [specific outcome]\n\n## Full GTM Strategy\n[All sections from process]\n\n## One-Pager for Stakeholders\n- The play: [approach in 3 bullets]\n- The bet: [why we think this will work]\n- The ask: [what you need from stakeholders]\n\n</output_format>\n\n<meta_guidance>\n\nGreat GTM strategy:\n- Starts with a tightly defined ICP, not \"everyone\"\n- Makes positioning trade-offs explicit\n- Has measurable goals with timelines\n- Plans for failure modes, not just success\n- Aligns cross-functional teams with clear owners\n\nAvoid:\n- Boil the ocean strategy (\"all channels, all customers\")\n- Vague goals (\"increase awareness\")\n- Assuming \"build it and they will come\"\n- Over-optimizing launch day vs. first 90 days\n- Treating GTM as marketing's job alone\n\n</meta_guidance>\n\n</gtm_strategy>",
    "technique": "Chain-of-thought through launch phases, identifying failure modes",
    "tools": "Claude Projects, NotebookLM",
    "useCase": "Planning a product launch that actually drives adoption and revenue"
  },
  {
    "name": "Identify North Star Metric",
    "category": "Analytics",
    "prompt": "<north_star_metric>\n\n<metric_inputs>\nBUSINESS CONTEXT:\n1. What does your product do? (core value proposition)\n2. How do you make money? (business model)\n3. What's your growth stage? (early, growth, mature)\n4. Current key metrics you track? (and why you picked them)\n\nCUSTOMER CONTEXT:\n5. When do customers get value? (aha moment, activation point)\n6. What makes customers stick? (retention drivers)\n7. What makes customers churn? (common reasons)\n8. How do customers use the product? (daily, weekly, monthly)\n\nUPLOADS:\n- Analytics dashboards\n- Business model canvas\n- User journey map\n- Cohort analysis\n</metric_inputs>\n\n<metric_process>\n\nYou are a product strategist who helps companies identify their North Star Metric - the single metric that best captures the value you deliver to customers and predicts business success.\n\nPHASE 1: UNDERSTAND VALUE DELIVERY\n\n1. MAP THE VALUE CHAIN\n\nCustomer value → Product usage → Business outcome\n\nWhat value do you deliver?\n[Specific benefit customers get, not features]\n\nWhen do customers experience that value?\n[The moment they realize \"this works for me\"]\n\nWhat actions lead to value?\n[Specific behaviors that predict success]\n\nHow does customer value drive revenue?\n[Connection between usage and money]\n\n2. IDENTIFY CANDIDATE METRICS\n\nBrainstorm metrics across categories:\n\nActivation metrics (new user experience)\n- Time to first value\n- Completion of key setup steps\n- First core action taken\n\nEngagement metrics (ongoing usage)\n- DAU, WAU, MAU\n- Actions per session\n- Feature adoption\n\nValue creation metrics (delivered outcome)\n- Projects created\n- Messages sent\n- Reports generated\n- Transactions completed\n\nRetention proxy metrics (predicts stickiness)\n- Return rate\n- Habit formation indicators\n- Cross-feature usage\n\nRevenue metrics (business outcome)\n- MRR/ARR\n- Expansion revenue\n- LTV\n\nYour candidate list: [8-12 potential NSMs]\n\nPHASE 2: EVALUATE CANDIDATES\n\nFor each candidate metric, score against criteria:\n\n1. Does it capture VALUE DELIVERY?\n- Does this metric go up when customers get more value?\n- Can you game this metric without delivering value? (if yes, bad sign)\n\n2. Does it PREDICT REVENUE?\n- Do customers with higher [metric] pay more / retain longer?\n- Show the correlation if you have data\n\n3. Is it a LEADING INDICATOR?\n- Does it predict future success before revenue shows up?\n- Or is it lagging? (revenue itself is lagging)\n\n4. Can the WHOLE COMPANY influence it?\n- Can product, marketing, sales, support all drive this up?\n- Or is it only one team's responsibility?\n\n5. Is it EASY TO UNDERSTAND?\n- Can you explain it to your grandmother?\n- Do employees intuitively get why it matters?\n\n6. Can you MEASURE IT reliably?\n- Do you have the data today or can you instrument it?\n- Is it consistent across platforms?\n\nScore each: High / Medium / Low\n\nTop 3 finalists: [metrics with highest scores]\n\nPHASE 3: TEST NORTH STAR CANDIDATES\n\nFor your finalists, validate:\n\nHistorical Analysis (if you have data):\n- Cohorts with high [NSM] → retention rate\n- Cohorts with low [NSM] → retention rate\n- Correlation between [NSM] and revenue\n\nCustomer Interview Validation:\n- Do power users have high [NSM]?\n- Do churned customers have low [NSM]?\n- What do customers say about value?\n\nThe \"So What\" Test:\nIf this metric goes up 20%, does that definitely mean:\n- Customers are getting more value? [yes/no]\n- The business is healthier? [yes/no]\n- The team knows what to build? [yes/no]\n\nPHASE 4: RECOMMENDATION\n\n## Your North Star Metric: [specific metric]\n\nDefinition:\n[Exact formula, what counts, what doesn't]\n\nWhy this metric:\n- Captures customer value because: [reason]\n- Predicts business success because: [reason]\n- Whole company can influence because: [reason]\n\nCurrent baseline: [number if known]\n\nWhat \"good\" looks like:\n- This month: [target]\n- This quarter: [target]\n- This year: [target]\n\nHow to move it:\n[3-5 levers that increase NSM]\n\nWhat NOT to do:\n[Ways to game metric that don't create value]\n\n## Supporting Metrics\n\nYour NSM needs context. Track these too:\n\nInput metrics (drive NSM up):\n- [Metric]: [why it matters]\n- [Metric]: [why it matters]\n\nOutput metrics (validate NSM):\n- Revenue/retention: [to confirm NSM→ business value]\n- Quality metrics: [to avoid gaming NSM]\n\nThe Dashboard:\n- NSM: [big number]\n- Trend: [↑↓ vs last period]\n- Inputs: [metrics that drive it]\n- Outputs: [metrics that validate it]\n\nPHASE 5: OPERATIONALIZING\n\nHow to use your NSM:In roadmap prioritization:\n- Ask: \"Will this feature increase [NSM]?\"\n- Estimate: \"By how much?\"\n\nIn experiment design:\n- Primary metric: [NSM] or input to NSM\n- Success criteria: [% increase]\n\nIn team goals:\n- Company OKR: Increase [NSM] from X to Y\n- Team OKRs: Improve [input metrics]\n\nIn stakeholder communication:\n- Board decks: Lead with NSM trend\n- All-hands: Celebrate NSM wins\n- New hires: Explain why we picked this NSM\n\nReview cadence:\n- Daily: Monitor NSM for anomalies\n- Weekly: Review with product/growth team\n- Monthly: Deep dive into what moved it\n- Quarterly: Validate NSM still predicts success\n\n</metric_process>\n\n<output_format>\n\n## Recommendation\n- NSM: [metric name and definition]\n- Current: [baseline]\n- Target: [goal]\n- Why: [2-3 sentence rationale]\n\n## Full Analysis\n[All sections from process]\n\n## One-Slide Summary\n[NSM, how to move it, why it matters]\n\n</output_format>\n\n<meta_guidance>\n\nGreat North Star Metric:\n- Measures value delivery, not vanity\n- Leading indicator, not lagging\n- Whole company can influence it\n- Simple to explain and understand\n- Has strong correlation to revenue/retention\n\nAvoid:\n- Picking revenue as NSM (lagging, not actionable)\n- Metrics easily gamed without creating value\n- Too complex (multi-variable formulas)\n- Only one team can influence it\n- Changing it every quarter\n\nRemember: NSM should be stable for years, not months. It's okay to evolve it as business model changes, but picking a new NSM quarterly means you never picked the right one.\n\n</meta_guidance>\n\n</north_star_metric>",
    "technique": "Chain of reasoning through what drives business value",
    "tools": "Claude Projects, analytics platform",
    "useCase": "Find the one metric that best captures product value delivery"
  },
  {
    "name": "Competitor Analysis",
    "category": "Strategy & Planning",
    "prompt": "<competitor_analysis>\n\n<competitor_inputs>\nWHAT HAPPENED:\n1. What did your competitor do? (launched feature, announced product, changed pricing, got funding)\n2. Where did you see this? (upload: press release, blog post, demo video, screenshots, pitch deck)\n3. What's your initial concern? (will this hurt us, do we need to respond, are we behind)\n\nYOUR CONTEXT:\n4. Your product and how it makes money\n5. Customer segment overlap (are you going after same customers)\n6. Your current roadmap (what were you planning to build)\n\nUPLOADS:\n- Their announcement materials\n- Screenshots or demo videos\n- Customer reactions (Twitter, LinkedIn, Reddit, reviews)\n- Your internal Slack thread panicking about this (kidding, but also not kidding)\n</competitor_inputs>\n\n<analysis_framework>\n\nYou're a product strategist who's seen 100 competitor launches. Most fail. Most are less scary than they look. Your job: separate real threats from noise, then figure out what to do Monday morning.\n\nTHE REALITY:\nWhen a competitor ships something, your team's first instinct is panic. Your second instinct is \"we should build that too.\" Both instincts are usually wrong.\n\nThe right questions:\n- What problem are they actually solving? (not what they say, what they're really doing)\n- Why did they make this move now?\n- What does this reveal about their strategy?\n- Where are they making a bet we're not?\n- What can they NOT do because of this choice?\n\n---\n\n## PART 1: WHAT DID THEY ACTUALLY DO?\n\n### Reverse Engineer The Product\n\nMost announcements are vaporware or half-baked. Figure out what's real.\n\nBased on their materials, reconstruct:The User Flow (Screen by Screen)\nWalk through their product like a customer would:\n\n1. Entry point: Where does the user encounter this?\n- Homepage? In-product upsell? Checkout flow?\n- Screenshot/describe what they see\n\n2. Step-by-step flow:\n- Screen 1: [What user sees, what they can do]\n- Screen 2: [Next interaction]\n- Screen 3: [Outcome]\n\n3. Edge cases they're handling (or not):\n- What happens if user does X?\n- Error states shown?\n- Mobile vs desktop?\n\nExample: Apple BNPL\n- Entry point: Appears in Apple Pay checkout flow\n- User sees: \"Pay in 4 installments, no interest\" directly in Wallet\n- Flow: Select installment plan → Face ID → Done\n- Integration: Native to iOS, not a separate app\n- What they're NOT handling: Web checkout (iOS only initially)\n\nWhat this tells you:\n- How much they invested (polish level reveals commitment)\n- What they're optimizing for (speed? flexibility? control?)\n- What they're willing to punt on (tells you their priorities)\n\n### Decode The Positioning\n\nWhat job are they trying to own?\n\nTheir headline: [exact copy]\nTranslation: [what they're actually saying]\nSubtext: [what they're admitting they're NOT]\n\nExample:\n- Headline: \"The all-in-one workspace\"\n- Translation: \"We're going after Slack + Docs + PM tools\"\n- Subtext: \"We won't be best-in-class at any one thing\"\n\nWho is this for?\n- Their explicit target: [who they say]\n- Their actual target: [look at pricing, features, integrations]\n- Who this excludes: [importantly, who can't use this]\n\nThe key question: Are they moving upmarket or downmarket? This tells you everything.\n\n### Extract The Business Model\n\nHow are they monetizing this?\n- Free? (Land grab play)\n- Freemium? (Which features are gated tells you what they think is valuable)\n- Paid only? (Premium positioning)\n- Bundled? (Trying to increase ARPU)\n\nWhat does this reveal?\n- Free → They're subsidizing this with something else, find out what\n- Expensive → They think this is differentiated, but are customers paying?\n- Bundled → They can't sell it standalone (weakness)\n\nExample: Apple BNPL\n- Business model: No fees to users, merchant fees unclear\n- What this reveals: Using it as moat for Apple Pay, not profit center\n- Strategic implication: They can afford to lose money on this\n\n---\n\n## PART 2: WHY DID THEY DO THIS?\n\nMost PMs stop at \"what did they build.\"\nYou need to understand \"why now\" and \"what does this reveal.\"\n\n### Strategic Intent Detection\n\nWhy this move, why now?\n\nThink through the possibilities:\n- Defensive: Protecting installed base from threat\n- Offensive: Going after new market/customer\n- Bundling: Increasing switching costs\n- Platform play: Enabling ecosystem\n- Catching up: Finally building table stakes\n- Distraction: Throwing spaghetti at wall\n\nEvidence for each theory:\n[What in their execution supports each explanation]\n\nMost likely reason: [Your call]\n\n### What This Reveals About Their Strategy\n\nEvery product decision is a breadcrumb:\n\nThey're betting on:\n[The future they think is coming]\n\nThey're moving away from:\n[What they're de-prioritizing]\n\nThey can no longer do:\n[The doors this closes for them]\n\nExample: Apple BNPL\n- Betting on: Owning the entire payment stack, not just facilitating it\n- Moving away from: Being neutral platform for 3rd party payment options\n- Can no longer do: Partner deeply with Affirm, Klarna, etc.\n\n### The Constraint Analysis\n\nThis is where you find their vulnerability.\n\nWhat constraints are baked into this choice?Technical constraints:\n- Architecture decisions that limit them\n- Example: iOS-only = can't capture web/Android traffic\n\nBusiness model constraints:\n- Revenue model that creates misaligned incentives\n- Example: Per-seat pricing = disincentivized to make collaboration easier\n\nCustomer constraints:\n- Built for X customer = can't serve Y customer\n- Example: Enterprise-first = too complex for SMB\n\nStrategic constraints:\n- Committed to platform/ecosystem that limits flexibility\n- Example: Must work with existing Apple Pay = can't optimize checkout flow\n\nThe opportunity: These aren't bugs. These are permanent features. They can't fix them without breaking their strategy.\n\n---\n\n## PART 3: HOW DOES THIS AFFECT YOU?\n\n### Threat Level Assessment\n\nBe honest about impact:\n\nBest case (for them):\n[If they execute perfectly, what happens to you]\n\nMost likely case:\n[What probably happens given execution is hard]\n\nYour exposure:\n- Customer overlap: [High/Medium/Low]\n- Product overlap: [High/Medium/Low]\n- Time to impact you: [Immediate/Quarters/Years]\n\nReal threat level: [Critical/High/Medium/Low/Noise]\n\nThe hard truth: Most competitor launches don't matter. The ones that do, you usually see coming.\n\n### Direct Impact Analysis\n\nWhat immediately gets harder for you:Sales/Marketing:\n- Will prospects ask about this? [If yes, you need response]\n- Will they use this as price leverage?\n- Does this change buyer expectations?\n\nProduct:\n- Is this now table stakes? [Do customers expect this]\n- Does this make your roadmap obsolete?\n- Do you need to shift priorities?\n\nCustomers:\n- Will existing customers churn?\n- Will they demand this from you?\n- Can you position around it?\n\n### The Strategic Question: Should You Respond?\n\nMost PMs default to: \"We should build that too.\"\n\nThe right framework:\n\nDON'T RESPOND IF:\n- They're competing on a dimension you chose not to compete on\n- Their target customer isn't your target customer\n- You have a differentiated path that's better\n- This is their Hail Mary, not their strength\n\nFLANK RESPONSE IF:\n- You can solve the same job a different (better) way\n- You can serve the same customer in adjacent use case\n- You can move faster/cheaper/simpler\n\nCOPY IF (rarely):\n- This is table stakes and you're behind\n- Every competitor will have this\n- Customers will churn without it\n- You can build it 10x better\n\nGO BIGGER IF:\n- They validated demand but under-shot\n- You can leapfrog with more ambitious vision\n- Their version exposes them to your strength\n\n### Your Response Plan\n\nImmediate (This Week):\n- [ ] Sales response doc: How to talk about this\n- [ ] Customer outreach: Gauge reaction from top 10 accounts\n- [ ] Team alignment: Make decision on whether to respond\n\nShort-term (This Quarter):\n- If building response: [Specific scope, not \"build what they have\"]\n- If not responding: [How to position around it]\n- Monitoring: [What signals say we made wrong call]\n\nStrategic (This Year):\n- Does this change your strategy? [Yes/No/Maybe]\n- If yes: [Specifically how]\n- If no: [Why you're confident in your path]\n\n---\n\n## PART 4: WHAT THEY CAN'T DO (YOUR ADVANTAGE)\n\n### Exploit The Opening\n\nEvery move creates vulnerability.\n\nWhat can they NOT do now?They can't serve: [Customer segment this alienates]\nThey can't build: [Features that conflict with this]\nThey can't partner with: [Ecosystem they just burned]\nThey can't change: [Commitments they're locked into]\n\nYour move:\n[How you position into their new weakness]\n\n### The Judo Move\n\nBest response isn't matching their move. It's using their momentum against them.\n\nTheir strength:\n[What they're good at]\n\nFlip side weakness:\n[What that strength prevents them from doing]\n\nYour positioning:\n[How you own the opposite]\n\nExample:\n- Their strength: \"Enterprise-grade compliance\"\n- Weakness: Slow, complex, requires IT involvement\n- Your move: \"Get started in 5 minutes, no IT needed\"\n\n---\n\n## THE OUTPUT\n\n### One-Pager For Your Team\n\nWhat They Did:\n[2 sentences max]\n\nWhy They Did It:\n[Their strategic intent]\n\nReal Threat Level:\n[Critical/High/Medium/Low with brief rationale]\n\nWhat We're Doing:\n[Respond/Flank/Ignore with specific action]\n\nWhat Sales Says:\n[Exact talking points for next call]\n\n### Detailed Analysis\n\n[All sections above]\n\n### Follow-Up Actions\n\nResearch Tasks:\n- [ ] Test their product yourself: [Get account, walk through flows]\n- [ ] Talk to 5 customers: [Do they care? Will this affect their decision?]\n- [ ] Monitor metrics: [Which metrics will signal if this is working]\n\nDecision Points:\n- 30 days: [Re-assess threat level based on their traction]\n- 90 days: [Go/no-go on competitive response]\n\n</analysis_framework>\n\n<quality_check>\n\nDid you actually understand their product?\n- [ ] Can you draw their user flow on a whiteboard?\n- [ ] Do you know where in their flow this appears?\n- [ ] Have you identified what they're NOT handling?\n\nDid you understand their strategy?\n- [ ] Do you know WHY they built this?\n- [ ] Have you identified their constraints?\n- [ ] Do you know what they can't do because of this?\n\nDid you make a decision?\n- [ ] Respond / Flank / Ignore - you picked one\n- [ ] You have specific actions, not vague plans\n- [ ] You know what \"wrong\" looks like\n\n</quality_check>\n\n<meta_wisdom>\n\nThe uncomfortable truth:\n\nMost competitor analysis is theater. You build a deck, feel smart, then nothing changes.\n\nThe point of this analysis isn't to document what happened. It's to make a decision:\n- Do we respond?\n- How do we respond?\n- What do we tell customers?\n- What do we tell the team?\n\nIf you finish this analysis and don't know what to do Monday morning, you did it wrong.\n\nOn competition:\n\nCompetition isn't about features. It's about which customer, which job, which constraint.\n\nYour competitor just made a choice. That choice:\n- Opens doors for them\n- Closes doors for them\n- Creates space for you\n\nYour job: Find the space.\n\nOn copying competitors:\n\nThe weakest product strategy is \"build what they have.\"\n\nIf they're good, you'll always be behind.\nIf they're bad, you'll waste time.\n\nBetter question: \"What can we do that they structurally can't?\"\n\nThat's your answer.\n\n</meta_wisdom>\n\n</competitor_analysis>",
    "technique": "Reverse engineering product flows, constraint analysis, strategic intent detection",
    "tools": "Claude Project + Research, Perplexity Research",
    "useCase": "Competitor did something and want a thought partner to ideate on"
  },
  {
    "name": "Summarize a Customer Interview",
    "category": "Discovery",
    "prompt": "<customer_interview_synthesis>\n\n<interview_inputs>\nUPLOAD:\n- Interview recording or transcript\n- Your interview notes (messy is fine)\n- Screenshot of their product usage (if applicable)\n- Follow-up Slack/email conversation\n\nCONTEXT:\n1. Who did you interview? (role, company, how they use your product)\n2. Why did you talk to them? (churning, power user, prospect, feedback session)\n3. What were you trying to learn? (validation, discovery, win/loss)\n4. How long was the conversation? (30min, 1hr)\n</interview_inputs>\n\n<synthesis_framework>\n\nYou're a UX researcher who knows that most interview notes are useless because PMs hear what they want to hear, not what the customer said. Your job: Extract truth, preserve context, find the non-obvious insights.\n\nTHE REALITY:\n\nBad interview summaries: \"Customer wants feature X\"\nGood interview summaries: \"Customer is trying to accomplish Y, currently does Z workaround, mentioned X as possible solution but real problem is Y\"\n\nThe goal isn't transcription. It's understanding what the customer actually needs and whether it's a pattern or an outlier.\n\n---\n\n## PART 1: THE FACTUAL LAYER\n\n### Customer Profile\n- Name/Company: [if not confidential]\n- Role: [their job, not just title]\n- Company size/segment: [5 people vs 5000 matters]\n- How they use your product: [daily, weekly, monthly | power user, casual, barely adopted]\n- Tenure as customer: [day 1 or year 3 changes everything]\n- Customer health: [happy, neutral, churning]\n\n### Interview Setup\n- Date: [when]\n- Format: [video, phone, in-person]\n- Duration: [actual length]\n- Who was there: [from your team]\n- Interview goal: [what you were trying to learn]\n\n---\n\n## PART 2: WHAT THEY ACTUALLY SAID\n\n### Their Current Workflow\n\nThe Job They're Trying to Do:\n[In their words: what they're trying to accomplish]\n\nHow They Do It Today:\nStep 1: [their current process]\nStep 2: [next step]\nStep 3: [outcome]\n\nWhere Your Product Fits:\n[When/how they use your product in this workflow]\n\nWhere Your Product Doesn't Fit:\n[What they have to do outside your product]\n\nQuote: \"[Exact words about their workflow]\"\n\n### Pain Points (Ranked by Intensity)\n\nCRITICAL (blocking them, considering alternatives):\n- Pain: [specific problem]\n- Frequency: [daily, weekly, monthly]\n- Current workaround: [what they do now]\n- Cost of workaround: [time, money, frustration]\n- Quote: \"[Their exact words, especially emotional language]\"\n\nHIGH (significant friction, but manageable):\n- [same structure]\n\nMEDIUM (annoying, but not a priority):\n- [same structure]\n\nThe key insight: Watch their energy level when describing pain. If they lean forward, talk faster, or use strong language (\"it's insane that...\"), that's real pain.\n\n### What They Care About (vs What You Thought They'd Care About)\n\nYou expected them to care about:\n[What you thought going in]\n\nThey actually cared about:\n[What they spent time on]\n\nThey didn't mention at all:\n[What you asked about that they dismissed or ignored]\n\nThe surprise:\n[What caught you off guard]\n\nWhy this matters:\n[What this reveals about your assumptions]\n\n### Feature Requests (Decoded)\n\nDon't just write down what they asked for. Understand why.\n\nRequest 1:\n- What they asked for: \"[Feature X]\"\n- What they're actually trying to do: [underlying need]\n- Why this matters: [impact if solved]\n- Alternative solutions: [other ways to solve this need]\n- Quote: \"[Exact words]\"\n\nThe translation: Most feature requests are solutions, not problems. Your job is to extract the problem.\n\n---\n\n## PART 3: WHAT THEY DIDN'T SAY (But You Noticed)\n\n### Behavioral Signals\n\nPositive signals:\n- [ ] They pulled up your product during call (invested user)\n- [ ] They had specific examples ready (they think about this)\n- [ ] They asked when features are coming (they're planning around you)\n- [ ] They brought up your product unprompted (top of mind)\n\nWarning signals:\n- [ ] Vague answers (they don't actually use it much)\n- [ ] \"The team uses it\" but they don't (adoption issue)\n- [ ] Mentioned competitors (comparing options)\n- [ ] Asked about pricing/contract (considering leaving)\n\nWhat this tells you:\n[Your interpretation of their engagement level]\n\n### What They Avoided\n\nSometimes the most important thing is what they didn't say.\n\nYou asked about:\n[Topic you brought up]\n\nThey deflected/changed subject:\n[How they avoided it]\n\nWhy this matters:\n[What they might be hiding or uncomfortable discussing]\n\n### Energy Map\n\nWhere did they light up vs. check out?\n\nHigh energy topics:\n- [What made them animated, talk faster, lean in]\n- Why: [What this reveals they care about]\n\nLow energy topics:\n- [What made them give short answers, look away]\n- Why: [What this reveals they don't care about]\n\nThe pattern:\n[What they're optimizing for in their world]\n\n---\n\n## PART 4: THE STRATEGIC LAYER\n\n### Is This a Pattern or an Outlier?\n\nCompare to other interviews:\n- How many other customers have said similar things?\n- Is this pain unique to this customer's setup/segment?\n- Is this a leading indicator or an edge case?\n\nYour assessment:\n- Pattern: [High confidence this applies to many customers]\n- Segment-specific: [Applies to enterprise/SMB/etc.]\n- Outlier: [Unique to this customer's situation]\n\n### Jobs-to-be-Done Analysis\n\nThe job they hired your product to do:\n[What outcome they're pursuing]\n\nHow well are you doing that job?\n- What's working: [where you deliver value]\n- What's broken: [where you're failing]\n- What's missing: [gaps in the solution]\n\nAre they hiring other products to do parts of the job?\n[What else they use and why]\n\nThe strategic question: Should you own this entire job, or partner/integrate?\n\n### Confidence Level in Their Feedback\n\nNot all feedback is equally valuable.\n\nHigh confidence:\n- They use the product daily\n- They gave specific examples with data\n- Multiple people on their team confirmed same pain\n- They're willing to pay more to solve this\n\nMedium confidence:\n- They use it occasionally\n- Examples were somewhat vague\n- Based on memory, not current experience\n\nLow confidence:\n- They don't really use it\n- Speaking theoretically (\"I think users would want...\")\n- Contradicted themselves during conversation\n\nYour takeaway:\n[How much weight to give this feedback]\n\n---\n\n## PART 5: ACTION ITEMS\n\n### What You Learned\n\nValidated:\n- [Assumptions that were confirmed]\n\nInvalidated:\n- [Assumptions that were wrong]\n\nNew insights:\n- [Things you didn't know before]\n\n### What to Do Next\n\nProduct decisions:\n- [ ] [Specific feature to prioritize/deprioritize based on this]\n- [ ] [Workflow to investigate further]\n- [ ] [Integration to consider]\n\nMore research needed:\n- [ ] [Follow-up question to ask this customer]\n- [ ] [Other customers to interview to validate pattern]\n- [ ] [Usage data to check]\n\nCustomer-specific actions:\n- [ ] [Send them X resource]\n- [ ] [Follow up on Y in 2 weeks]\n- [ ] [Introduce them to Z team member]\n\n### Quotes to Remember\n\nThe 3 quotes that capture the essence:\n\n1. \"[Most important quote]\"\n- Why it matters: [context]\n\n2. \"[Second key quote]\"\n- Why it matters: [context]\n\n3. \"[Surprising quote]\"\n- Why it matters: [context]\n\nThese go in: Roadmap presentations, team syncs, strategy docs\n\n---\n\n## THE OUTPUT\n\n### One-Paragraph Summary\n\n[3-4 sentences: Who you talked to, what you learned, what it means]\n\n### Key Takeaways (For Busy Execs)\n\n1. [Insight 1 with evidence]\n2. [Insight 2 with evidence]\n3. [Insight 3 with evidence]\n\nWhat we're doing about it: [Specific action]\n\n### Full Write-Up\n\n[All sections above]\n\n### Shareable Quotes\n\n[Powerful quotes formatted for Slack/decks]\n\n</synthesis_framework>\n\n<quality_check>\n\nDid you extract truth or just transcribe?\n- [ ] Do you understand the underlying need, not just surface request?\n- [ ] Did you note what they DIDN'T say?\n- [ ] Did you assess if this is pattern or outlier?\n\nCan someone who wasn't there understand it?\n- [ ] Is context clear?\n- [ ] Are quotes preserved accurately?\n- [ ] Would this change someone's mind?\n\nIs it actionable?\n- [ ] Do you know what to do next?\n- [ ] Is it clear if this changes priorities?\n\n</quality_check>\n\n<meta_wisdom>\n\nOn customer interviews:\n\nMost PMs are bad at interviews because they're looking for validation, not truth.\n\nYou ask: \"Would you use feature X?\"\nThey say: \"Yes\"\nYou build it.\nThey don't use it.\n\nBetter approach:\n- Ask about their workflow, not your features\n- Watch what they get excited about\n- Listen for what they're trying to accomplish\n\nThe hard truth:\n\nOne interview means almost nothing.\nFive interviews showing the same pattern means something.\nTen interviews where five say X and five say Y means you have segmentation.\n\nYour job: Know which kind of signal you have.\n\nOn what customers say vs. what they need:\n\nCustomers are great at describing their pain.\nThey're terrible at prescribing solutions.\n\nWhen they say \"I want feature X,\" your job is to understand:\n- What are you trying to do?\n- Why isn't it working today?\n- How do you work around it?\n- What have you tried?\n\nThe solution might not be feature X.\n\nRemember:\n\nThe best interviews are conversations, not interrogations.\nThe best insights come from follow-up questions, not your script.\nThe best PMs know when to shut up and listen.\n\n</meta_wisdom>\n\n</customer_interview_synthesis>",
    "technique": "Jobs-to-be-done extraction, pain point intensity scoring, quote preservation",
    "tools": "Claude Projects, NotebookLM (if transcript), ChatGPT Projects",
    "useCase": "Just finished customer interview, need to extract insights before memory fades"
  },
  {
    "name": "Customer Outreach Planning",
    "category": "Discovery",
    "prompt": "\n\n<customer_outreach_plan>\n\n<outreach_inputs>\nYOUR RESEARCH GOAL:\n1. What are you trying to learn? (be specific, not \"get feedback\")\n2. What decision will this inform? (roadmap, positioning, pricing, GTM)\n3. What do you already know? (don't ask questions you can answer with data)\n4. What's your hypothesis? (what do you think is true that you're testing)\n5. How many conversations do you need? (10 depth interviews vs 100 survey responses)\n\nYOUR CONSTRAINTS:\n6. Timeline (when do you need answers)\n7. Budget (can you incentivize, or relying on goodwill)\n8. Access (do you have customer list, or cold outreach)\n\nUPLOADS:\n- Current customer list with segments\n- Usage data or analytics\n- Previous research (don't re-ask what you know)\n</outreach_inputs>\n\n<planning_framework>\n\nYou're a research lead who knows that bad research is worse than no research. Bad research gives you false confidence. Good research gives you uncomfortable truths.\n\n**THE REALITY:**\n\nMost customer outreach fails because:\n- You ask customers to design your product (they can't)\n- You ask leading questions (they tell you what you want to hear)\n- You talk to the wrong people (friendly customers who love everything)\n- You don't have a plan for what to do with what you learn\n\nGood outreach:\n- Has a specific research question\n- Knows who to talk to and why\n- Asks about behavior, not opinions\n- Results in a decision\n\n---\n\n## PART 1: CLARIFY WHAT YOU'RE ACTUALLY TRYING TO LEARN\n\n### Turn Vague Goals into Research Questions\n\n**Bad goal:** \"Get customer feedback\"\n**Good goal:** \"Understand if SMB customers would pay $50/month for advanced analytics\"\n\n**Your research question:**\n[Specific, answerable question]\n\n**Why this matters:**\n[The decision this will inform]\n\n**What good looks like:**\n[How you'll know you got a useful answer]\n\n### Identify Your Hypotheses\n\nYou already have beliefs. Make them explicit so you can test them.\n\n**Hypothesis 1:**\n- **What you think is true:** [specific belief]\n- **If you're right:** [what you'll do]\n- **If you're wrong:** [what you'll do instead]\n- **How you'll test it:** [specific questions/observations]\n\n**Example:**\n- Hypothesis: \"Enterprise customers churn because onboarding is too complex\"\n- If right: Invest in white-glove onboarding\n- If wrong: Look elsewhere for churn drivers\n- Test: Ask churned customers about onboarding vs. other factors\n\n### What You Can Answer Without Talking to Anyone\n\nDon't waste customer goodwill asking questions your data can answer.\n\n**Check your analytics first:**\n- Feature adoption rates\n- Drop-off points in flows\n- Time to value metrics\n- Cohort retention\n\n**Check support tickets:**\n- Common complaints\n- Feature requests\n- Confusion points\n\n**Check usage data:**\n- Who uses what features\n- When they use them\n- What workflows they follow\n\n**What you still need customer conversations for:**\n[The \"why\" behind the data]\n\n---\n\n## PART 2: WHO TO TALK TO (THIS IS HALF THE BATTLE)\n\n### Segment Your Target Respondents\n\nNot all customers are equal for research purposes.\n\n**Segment 1: Power Users**\n- **Why talk to them:** Understand what \"success\" looks like\n- **What to ask:** How they achieved value, what's missing\n- **Risk:** They're not representative of most users\n\n**Segment 2: Churned/At-Risk Customers**\n- **Why talk to them:** Learn what's broken\n- **What to ask:** Why they left, what would bring them back\n- **Risk:** They might just be griping, not giving useful feedback\n\n**Segment 3: Recent Sign-Ups**\n- **Why talk to them:** Understand onboarding, aha moments\n- **What to ask:** What confused them, what clicked\n- **Risk:** They don't have long-term perspective\n\n**Segment 4: Target Customers (Non-Users)**\n- **Why talk to them:** Understand why they don't buy\n- **What to ask:** Current solution, switching costs\n- **Risk:** Hard to get them to talk to you\n\n**Your target mix:**\n- [X] from Segment 1\n- [Y] from Segment 2\n- [Z] from Segment 3\n\n**Why this mix:** [Your reasoning]\n\n### Screening Criteria\n\nNot everyone who volunteers should be interviewed.\n\n**Must have:**\n- [ ] [Criterion 1: e.g., \"Uses product weekly\"]\n- [ ] [Criterion 2: e.g., \"Decision-maker for renewals\"]\n- [ ] [Criterion 3: e.g., \"In target segment\"]\n\n**Nice to have:**\n- [ ] [Bonus criterion]\n\n**Disqualifiers:**\n- [ ] [E.g., \"Only used trial, never paid\"]\n- [ ] [E.g., \"Employee of competitor\"]\n\n### Sample Size Reality Check\n\n**Qualitative research (interviews):**\n- 5 interviews: Pattern detection starts\n- 10 interviews: Confident in main themes\n- 15+ interviews: Diminishing returns unless highly segmented\n\n**Quantitative research (surveys):**\n- 30 responses: Directional insights\n- 100 responses: Reasonable confidence\n- 300+ responses: Statistical significance\n\n**Your target:** [Number and type]\n\n---\n\n## PART 3: THE RESEARCH PLAN\n\n### Interview Guide Design\n\n**The framework: Past → Present → Future**\n\n**PART 1: Context (5 min)**\nGet them talking about their world, not your product.\n\nQuestions:\n- \"Walk me through a typical [workday/workflow]\"\n- \"What does success look like in your role?\"\n- \"What are you spending time on this quarter?\"\n\n**Why this works:** You understand their priorities before you talk about your product.\n\n**PART 2: Behavior (20 min)**\nWhat they actually do, not what they say they do.\n\nQuestions:\n- \"Last time you used [feature], walk me through exactly what you did\"\n- \"Show me how you currently solve [problem]\" (screen share if possible)\n- \"What workarounds have you built?\"\n- \"What do you wish you could do but can't?\"\n\n**Why this works:** Behavior > opinions. Watch what they do.\n\n**PART 3: Evaluation (10 min)**\nNow you can ask about your product, but in context.\n\nQuestions:\n- \"How well does [product] fit into that workflow?\"\n- \"What's the hardest part about using it?\"\n- \"If you could wave a magic wand and fix one thing...\"\n- \"What almost made you not choose us?\"\n\n**Why this works:** You understand the gap between what you offer and what they need.\n\n**PART 4: Future (5 min)**\nUnderstand direction, not feature requests.\n\nQuestions:\n- \"How is your work changing in the next year?\"\n- \"What are you worried about?\"\n- \"What would make you a bigger advocate for us?\"\n\n**Why this works:** You're designing for where they're going, not where they are.\n\n**CRITICAL RULES:**\n- No leading questions (\"Don't you think X would be better?\")\n- No hypotheticals (\"Would you pay $10 for Y?\")\n- No yes/no questions (\"Do you like feature Z?\")\n- Ask \"why\" at least 3 times per topic\n\n### Survey Design (If Going Quantitative)\n\n**Survey structure:**\n\n**Section 1: Qualification (2-3 questions)**\nEnsure they're in your target segment.\n\n**Section 2: Current State (5-7 questions)**\nHow they solve the problem today.\n\n**Section 3: Your Product (5-7 questions if existing customers)**\nSatisfaction, usage, gaps.\n\n**Section 4: Priorities (3-5 questions)**\nWhat matters most to them.\n\n**Section 5: Willingness to Pay / Trade-offs (3-5 questions if testing pricing/positioning)**\nConjoint analysis or MaxDiff if sophisticated.\n\n**Section 6: Demographics (3-5 questions)**\nFor segmentation analysis.\n\n**Total length:** Under 10 minutes or completion rate tanks.\n\n**Key principles:**\n- Use scales consistently (1-5, 1-7, etc.)\n- Mix question types (rating scales, multiple choice, one open-ended at end)\n- Randomize answer order where possible\n- Test with 3 people before sending broadly\n\n### The Outreach Message\n\n**Subject line that works:**\n- ❌ \"Product feedback requested\"\n- ✅ \"15 minutes to help shape [specific feature]\"\n\n**Email template:**\nHi [Name],\n[Personal touch: reference their usage/situation]\nWe're talking to [specific segment] customers to understand [specific problem/workflow].\nWould you have 20 minutes in the next two weeks for a conversation? In return:\n• [Incentive: early access, Amazon gift card, feature prioritization]\n• You'll directly influence what we build next\n[Link to calendar]\nThanks,\n[Your name]\n\n\n**What works:**\n- Specificity (not \"feedback\" but \"how you use X feature\")\n- Time-bounded (20 min, not \"quick chat\")\n- Clear value exchange (what they get)\n- Easy to say yes (calendar link)\n\n---\n\n## PART 4: WHAT TO DO WITH WHAT YOU LEARN\n\n### Analysis Plan\n\n**After each interview:**\n- [ ] Write synthesis within 24 hours (memory fades fast)\n- [ ] Tag themes and patterns\n- [ ] Note surprising findings\n- [ ] Update hypothesis tracker\n\n**After 5 interviews:**\n- [ ] Review for patterns\n- [ ] Decide if you're learning new things or hearing repeats\n- [ ] Adjust interview guide if needed\n\n**After all interviews:**\n- [ ] Full synthesis report\n- [ ] Share findings with team\n- [ ] Make recommendation\n\n### Decision Framework\n\n**If you learn X, you'll do Y:**\n\n| Finding | Action |\n|---------|--------|\n| Customers say [X] | We'll prioritize [Y feature] |\n| Customers don't care about [X] | We'll stop building [Y] |\n| Customers are confused by [X] | We'll redesign [Y flow] |\n\n**Make this explicit before you start.** Otherwise findings sit in a doc and nothing changes.\n\n---\n\n## THE OUTPUT\n\n### Research Plan One-Pager\n\n**Research Question:**\n[What we're trying to learn]\n\n**Hypothesis:**\n[What we think is true]\n\n**Who We're Talking To:**\n- [X] power users\n- [Y] churned customers\n- [Z] prospects\n\n**Timeline:**\n- Outreach: [dates]\n- Interviews: [dates]\n- Synthesis: [dates]\n\n**Decision Point:**\n[What we'll do differently based on findings]\n\n### Full Research Plan\n\n[All sections above]\n\n### Interview Guide\n\n[Formatted, ready to use]\n\n### Outreach Templates\n\n[Email copy, screening questions, calendar invite]\n\n</planning_framework>\n\n<quality_check>\n\n**Is your research question actually answerable?**\n- [ ] Specific enough to know when you have an answer\n- [ ] Not \"what do customers want\" (they don't know)\n- [ ] Focused on behavior or needs, not feature requests\n\n**Are you talking to the right people?**\n- [ ] Target respondents can actually answer your question\n- [ ] Mix of segments (not just friendly customers)\n- [ ] Screening criteria will get you useful signal\n\n**Will this change anything?**\n- [ ] Clear decision framework\n- [ ] Timeline works for roadmap/launch\n- [ ] Stakeholders aligned on what happens based on findings\n\n</quality_check>\n\n<meta_wisdom>\n\n**On customer research:**\n\nBad research: \"What features do you want?\"\nGood research: \"Walk me through what you did last Tuesday.\"\n\nBad research: \"Would you pay for X?\"\nGood research: \"Show me how you solve this problem today, and what it costs you.\"\n\nBad research: Ask 50 customers via survey\nGood research: Watch 10 customers use your product\n\n**The uncomfortable truth:**\n\nCustomers will tell you they want your product to be \"faster, easier, cheaper.\"\n\nThis is useless. Everyone wants everything to be better.\n\nYour job: Understand *why* speed matters, *where* it's hard, and *what* cost they're willing to pay.\n\n**On who to talk to:**\n\nYour friendliest customers will tell you everything is great.\nYour angriest customers will tell you everything sucks.\n\nNeither is useful.\n\nTalk to:\n- Recent switchers (they compared you to alternatives)\n- Power users (they found value)\n- People who tried and left (they chose something else)\n\n**Remember:**\n\nResearch isn't about validation. It's about learning.\n\nIf you finish customer conversations feeling good because they agreed with you, you probably asked leading questions.\n\nIf you finish feeling uncomfortable because your assumptions were wrong, you probably learned something.\n\n</meta_wisdom>\n\n</customer_outreach_plan>",
    "technique": "Research question formulation, screening criteria, hypothesis validation framework",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to talk to customers but don't want to waste their time or yours with bad questions"
  },
  {
    "name": "Identify ICP (Ideal Customer Profile)",
    "category": "Strategy & Planning",
    "prompt": "<icp_definition>\n\n<icp_inputs>\nYOUR DATA:\n1. Current customer list with:\n- Company size, industry, geography\n- Revenue/ARR per customer\n- Retention rate by cohort\n- Time to value\n- Support ticket volume\n- NPS/satisfaction scores\n\n2. Win/loss data:\n- Who you beat competitors for\n- Who chose competitors\n- Why you won/lost\n\n3. Usage data:\n- Feature adoption by segment\n- Active vs. inactive customers\n- Expansion patterns\n\nUPLOADS:\n- Customer database export\n- Sales CRM data\n- Usage analytics\n- Customer success notes on best/worst customers\n</icp_inputs>\n\n<icp_framework>\n\nYou're a GTM strategist who knows that \"everyone is our ICP\" means you'll win no one. Great companies get focused. They know exactly who they're built for, and who they're not.\n\nTHE REALITY:\n\nMost companies chase every dollar. They sell to anyone who'll pay.\n\nResult:\n- Product tries to serve everyone, delights no one\n- Sales has 100 objection handlers, no sharp positioning\n- Support drowns in edge cases\n- High churn, low expansion\n\nThe alternative:\n- Pick your ICP\n- Build the perfect product for them\n- Win them at high rate\n- Expand from position of strength\n\n---\n\n## PART 1: ANALYZE YOUR BEST CUSTOMERS\n\n### Define \"Best\"\n\nDifferent companies optimize for different things:\n\nHighest LTV:\n- Customers who stay longest + expand most\n- Why: Compound revenue\n\nFastest Time-to-Value:\n- Customers who activate and see value quickly\n- Why: Efficient go-to-market\n\nLowest Cost-to-Serve:\n- Customers who don't need handholding\n- Why: Scalable business\n\nStrongest Advocacy:\n- Customers who refer others, write reviews\n- Why: Organic growth\n\nYour definition of \"best\":\n[What you optimize for]\n\n### Extract the Pattern\n\nPull your top 20 customers by your \"best\" definition.\n\nFirmographic patterns:\n| Attribute | Pattern | % of Top 20 |\n|-----------|---------|-------------|\n| Company size | [e.g., \"50-200 employees\"] | [%] |\n| Industry | [e.g., \"SaaS, Tech\"] | [%] |\n| Geography | [e.g., \"US, UK\"] | [%] |\n| Revenue | [e.g., \"$10M-$50M\"] | [%] |\n\nBehavioral patterns:\n- What do they use your product for? [primary use case]\n- How do they use it? [frequency, workflow]\n- Who uses it? [roles, team size]\n- What features do they use most? [specific features]\n\nSituational patterns:\n- What problem were they trying to solve? [pain point]\n- What were they using before? [previous solution]\n- Why did they switch? [trigger event]\n- What alternatives did they consider? [competitive set]\n\nThe pattern:\n[Your description of what makes these customers successful]\n\n### The Inverse: Your Worst Customers\n\nNow look at bottom 20 by churn rate or support burden.\n\nWhat's different:\n| Attribute | Bad Fit Pattern |\n|-----------|-----------------|\n| Company size | [e.g., \"< 10 employees\"] |\n| Industry | [e.g., \"Non-tech\"] |\n| Use case | [e.g., \"Trying to use it for X when built for Y\"] |\n\nWhy they struggled:\n- Product-market fit issues: [they wanted something you don't do]\n- Expectation mismatch: [they thought you were X, you're Y]\n- Wrong buying stage: [too early/late in their maturity]\n\nThe lesson:\n[What these customers taught you about who NOT to target]\n\n---\n\n## PART 2: BUILD YOUR ICP PROFILE\n\n### The Core ICP\n\nFirmographic:\n- Company size: [employees: X-Y, revenue: $A-$B]\n- Industry: [specific verticals, or horizontal]\n- Geography: [countries/regions]\n- Growth stage: [startup, growth, mature]\n\nTechnographic:\n- Tech stack: [what tools they already use]\n- Technical sophistication: [technical team, non-technical team]\n- Integration needs: [must integrate with X, Y, Z]\n\nBehavioral:\n- Primary use case: [specific job-to-be-done]\n- Buying process: [who decides, how long]\n- Budget: [typical deal size: $X-Y/year]\n\nSituational:\n- Trigger event: [what makes them buy now]\n- Raised funding\n- Hit X scale\n- Current solution failing\n- Compliance requirement\n- Current solution: [what they're using/replacing]\n- Pain intensity: [how badly they need this]\n\n### The Persona (Who You Sell To)\n\nEconomic Buyer:\n- Title: [e.g., VP Sales, CFO]\n- Cares about: [business outcomes]\n- Success metric: [how they're measured]\n\nTechnical Buyer:\n- Title: [e.g., Engineering Lead]\n- Cares about: [technical fit, security]\n- Veto power: [yes/no and why]\n\nEnd User:\n- Title: [e.g., Sales Rep, Analyst]\n- Cares about: [day-to-day workflow]\n- Adoption driver: [if they don't use it, you churn]\n\nChampion:\n- Who they usually are: [role that becomes internal advocate]\n- Why they champion you: [what's in it for them]\n\n### Must-Haves vs. Nice-to-Haves\n\nNot all ICP attributes are equal.\n\nMust-haves (Non-negotiable):\n- [ ] [Attribute 1: e.g., \"Must have 50+ employees\"]\n- [ ] [Attribute 2: e.g., \"Must use Salesforce\"]\n- [ ] [Attribute 3: e.g., \"Must have compliance need\"]\n\nWhy these are must-haves: [Your product literally can't deliver value without these]\n\nStrong preferences (Increase win rate):\n- [ ] [Attribute: e.g., \"Fast-growing companies\"]\n- [ ] [Attribute: e.g., \"Technical founder\"]\n\nNice-to-haves (Bonus but not required):\n- [ ] [Attribute]\n\nAnti-ICP (Explicitly NOT our customer):\n- [Who you should turn away and why]\n\n---\n\n## PART 3: VALIDATE YOUR ICP\n\n### The Data Check\n\nTest your ICP definition against your customer base.\n\nHow many customers fit ICP?\n- Total customers: [X]\n- Fit ICP: [Y] ([%])\n\nPerformance by ICP fit:\n| Metric | ICP Customers | Non-ICP |\n|--------|---------------|---------|\n| Avg ARR | $[X] | $[Y] |\n| Retention | [%] | [%] |\n| Time-to-value | [days] | [days] |\n| NPS | [score] | [score] |\n\nThe truth:\nIf ICP customers don't significantly outperform non-ICP, your ICP isn't focused enough.\n\n### The Sales Check\n\nGive your ICP to sales team.\n\nQuestions:\n- Can they immediately name 20 companies that fit?\n- Does this help them prioritize their pipeline?\n- Does this give them confidence to disqualify bad fits?\n\nIf they say \"but what about [exception]...\"\n- There will always be exceptions\n- Optimize for the pattern, not edge cases\n- You can always expand ICP later after you dominate core\n\n### The Product Check\n\nCan you build a product that's perfect for this ICP?\n- Is the use case focused enough?\n- Are their needs homogeneous enough?\n- Can you ignore non-ICP needs and make trade-offs?\n\nIf you can't say \"no\" to non-ICP feature requests, your ICP isn't tight enough.\n\n---\n\n## PART 4: GO-TO-MARKET IMPLICATIONS\n\n### How ICP Changes Your Strategy\n\nProduct:\n- Build for: [ICP use case]\n- Don't build for: [non-ICP use case]\n- Roadmap filter: \"Does this serve ICP?\"\n\nMarketing:\n- Channels: [where ICP hangs out]\n- Message: [ICP-specific pain points]\n- Content: [topics ICP cares about]\n\nSales:\n- Target list: [ICP firmographics → X companies]\n- Qualification: [ICP checklist → pass/fail fast]\n- Pricing: [optimized for ICP budget]\n\nCustomer Success:\n- Onboarding: [ICP-specific playbook]\n- Success metrics: [ICP-defined value]\n- Expansion: [ICP natural growth path]\n\n### Expansion Strategy\n\nAfter dominating core ICP:Adjacent ICP #1:\n- How they're similar: [overlap with core]\n- How they're different: [what you'd need to change]\n- Effort to serve: [High/Medium/Low]\n\nAdjacent ICP #2:\n- [Same structure]\n\nThe sequence:\n1. Win core ICP at 50%+ win rate\n2. Expand to adjacent #1\n3. Then adjacent #2\n\nDon't try to serve all ICPs at once. Sequential focus beats parallel mediocrity.\n\n---\n\n## THE OUTPUT\n\n### One-Page ICP\n\nWho we're built for:\n[3 sentence description]\n\nMust-haves:\n- [Criterion 1]\n- [Criterion 2]\n- [Criterion 3]\n\nWho we're NOT for:\n- [Anti-ICP description]\n\nWhy we win with ICP:\n[Your unique advantage for this customer]\n\n### Detailed ICP Profile\n\n[All sections above]\n\n### Sales Qualification Checklist\n\nPass/Fail Questions:\n- [ ] [Must-have 1]\n- [ ] [Must-have 2]\n- [ ] [Must-have 3]\n\nScoring Questions:\n- [Nice-to-have attributes with point values]\n\nQualification threshold:\n- Must pass all pass/fail\n- Must score X+ points\n\n### Target Account List\n\nCompanies that fit ICP:\n[List or criteria for list building]\n\nSources:\n- [Where to find them: LinkedIn Sales Nav, ZoomInfo, etc.]\n\n</icp_framework>\n\n<quality_check>\n\nIs your ICP actually focused?\n- [ ] Can you describe them in 3 sentences?\n- [ ] Can sales name 50 companies that fit?\n- [ ] Are you willing to turn away customers who don't fit?\n\nDoes it match your data?\n- [ ] ICP customers retain better than non-ICP\n- [ ] ICP customers expand more\n- [ ] ICP customers cost less to serve\n\nIs it actionable?\n- [ ] Clear qualification criteria\n- [ ] Specific enough to build product roadmap around\n- [ ] Tight enough to focus marketing\n\n</quality_check>\n\n<meta_wisdom>\n\nOn focus:\n\nWeak companies: \"Our ICP is companies with 1-10,000 employees who want to be more productive.\"\n\nStrong companies: \"Our ICP is Series A-B SaaS companies with 50-200 employees, technical founders, using Salesforce, who need to consolidate 3+ tools.\"\n\nThe difference: Specificity.\n\nThe uncomfortable truth:\n\nPicking an ICP means saying no to money.\n\nA company that doesn't fit your ICP might want to pay you.\nYour job: Say no.\n\nWhy?\n- They'll churn\n- They'll create support burden\n- They'll request features that hurt your ICP\n- They'll dilute your positioning\n\nShort-term pain, long-term gain.\n\nOn expansion:\n\nMost companies diffuse too early.\n\nThey win 20% of their \"ICP\" (which is too broad) and immediately expand to adjacent segments.\n\nBetter: Win 60%+ of a narrow ICP, then expand.\n\nWhy?\n- Strong reputation in focused segment\n- Clear product differentiation\n- Unit economics that work\n- Reference customers that matter\n\nRemember:\n\nYour ICP isn't \"who can use your product.\"\n\nIt's \"who do you win with at highest rate, lowest cost, and strongest retention.\"\n\nIf you try to serve everyone, you'll serve no one well.\n\nPick your ICP. Dominate them. Expand from strength.\n\n</meta_wisdom>\n\n</icp_definition>",
    "technique": "Segmentation analysis, retention correlation, value realization patterns",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to focus GTM on right customers, stop wasting time on bad-fit prospects"
  },
  {
    "name": "Brainstorm Experiments",
    "category": "Strategy & Planning",
    "prompt": "<experiment_brainstorming>\n\n<experiment_inputs>\nWHAT YOU'RE TRYING TO VALIDATE:\n1. What's your hypothesis? (specific belief about customers/product)\n2. What's the risk if you're wrong? (wasted engineering, wrong direction, lost opportunity)\n3. What decision does this inform? (build feature X, change pricing, pivot strategy)\n4. How much are you willing to invest to learn? (time, money, engineering resources)\n\nYOUR CONTEXT:\n5. Current product state (live, beta, pre-launch)\n6. Available resources (can you ship code, or just test messaging)\n7. Timeline (need answer in days, weeks, months)\n\nOPTIONAL UPLOADS:\n- PRD or feature spec you're validating\n- Customer research notes\n- Analytics showing current behavior\n</experiment_inputs>\n\n<experiment_framework>\n\nYou're a growth PM who's run 100+ experiments. You know that most teams skip validation and build the wrong thing. Your job: Design the cheapest, fastest test that produces real learning.\n\nTHE REALITY:\n\nMost PMs think: \"Let's build a prototype and test it.\"\nGreat PMs think: \"What's the smallest thing we can do to learn if this is worth building?\"\n\nBad experiments: Take weeks, need engineering, test multiple variables\nGood experiments: Take days, need minimal resources, test one thing\n\n---\n\n## PART 1: CLARIFY WHAT YOU'RE ACTUALLY TESTING\n\n### Turn Vague Ideas Into Testable Hypotheses\n\nVague: \"Will users like feature X?\"\nTestable: \"Will 30%+ of active users click 'Try Beta' when offered feature X?\"\n\nYour hypothesis:\n- We believe: [specific statement]\n- We'll know we're right if: [measurable outcome]\n- We'll know we're wrong if: [failure condition]\n\nExample:\n- We believe: Enterprise customers will pay $50/mo for SSO\n- Right if: 40%+ of enterprise prospects say \"yes\" when offered\n- Wrong if: <20% express interest\n\n### Identify What You Don't Know\n\nWhat do you need to learn?Desirability: Do customers want this?\nViability: Will they pay for it?\nFeasibility: Can we build it well enough?\nUsability: Can they actually use it?\n\nYour biggest unknown: [which one matters most right now]\n\n### Size the Risk\n\nIf you're wrong:\n- What gets wasted? [weeks of eng time, opportunity cost]\n- What gets delayed? [other features]\n- What's the cost? [$X in lost revenue, Y% churn risk]\n\nIf the risk is low: Maybe you don't need an experiment. Just ship it.\nIf the risk is high: You definitely need to validate first.\n\n---\n\n## PART 2: DESIGN EXPERIMENTS (CHEAP TO EXPENSIVE)\n\n### The Ladder of Evidence\n\nStart cheap. Only go expensive if cheap tests validate.\n\nLEVEL 1: FAKE DOOR TESTS (Hours to set up)\n\nThe idea: Put a button/page/email that describes the feature. See if people click.\n\nExperiment design:\n- What you show: [Fake feature announcement, landing page, in-app banner]\n- Call-to-action: [Button that says what]\n- What happens when they click: [\"Thanks for your interest, we'll notify you\" or \"Sign up for beta\"]\n- Success metric: [X% click-through rate]\n- Why this works: Tests demand without building anything\n\nExample:\n- Show \"New: SSO Integration\" banner to enterprise users\n- Click goes to \"Join waitlist\" form\n- If >30% click → Strong demand signal\n- If <10% click → No one cares\n\nPros: Fast, cheap, tests real behavior\nCons: Only tests interest, not actual usage\n\n---\n\nLEVEL 2: CONCIERGE TEST (Days to set up)\n\nThe idea: Manually do what the feature would do. See if customers value it.\n\nExperiment design:\n- What you offer: [Manual version of automated feature]\n- How you deliver it: [Email, Slack, spreadsheet]\n- To whom: [5-10 friendly customers]\n- Success metric: [Do they use it? Do they ask for more? Will they pay?]\n\nExample:\n- Instead of building analytics dashboard, send customers weekly email with their metrics\n- If they reply \"This is great, I check it every week\" → Build it\n- If they ignore emails → They don't actually want analytics\n\nPros: Tests actual value, not just interest\nCons: Doesn't scale, labor-intensive\n\n---\n\nLEVEL 3: WIZARD OF OZ / PROTOTYPE (Weeks to set up)\n\nThe idea: Build the UI but fake the backend. See if people try to use it.\n\nExperiment design:\n- What you build: [Clickable prototype or UI shell]\n- What's real: [Frontend, flows, visual design]\n- What's fake: [AI is actually human, \"results\" are hardcoded]\n- To whom: [Beta group, specific segment]\n- Success metric: [Usage rate, completion rate, satisfaction]\n\nExample:\n- Build \"AI Report Generator\" UI\n- Requests actually go to your analyst who writes reports\n- If users love it and use it weekly → Build the AI\n- If they try once and never return → Problem isn't real\n\nPros: Tests full experience, realistic\nCons: Still requires design/dev work, not scalable\n\n---\n\nLEVEL 4: BETA / LIMITED LAUNCH (Months to build)\n\nThe idea: Build real feature but limit to small group.\n\nExperiment design:\n- What you build: [Real feature, feature-flagged]\n- Who gets it: [10-100 users, specific segment]\n- What you measure: [Adoption, retention, satisfaction, willingness to pay]\n- Kill criteria: [If <X% adopt in 30 days, we kill it]\n\nPros: Real usage data, real technical validation\nCons: Expensive, slow, hard to kill after investing\n\n---\n\n### Choose Your Experiment\n\nFor your hypothesis, recommend:Experiment type: [Fake door / Concierge / Wizard of Oz / Beta]\n\nWhy this level:\n[Cheap enough to learn fast, expensive enough to be convincing]\n\nSpecific design:\n- What you'll build/show: [Exact description]\n- To whom: [Specific segment, size]\n- Timeline: [How long to run]\n- Success metric: [Specific number that means \"go\"]\n- Failure metric: [Number that means \"stop\"]\n- Cost: [Hours/days/weeks, $$]\n\n---\n\n## PART 3: EXPERIMENT EXECUTION PLAN\n\n### Setup Checklist\n\nBefore launch:\n- [ ] Hypothesis written down (so you can't move goalposts later)\n- [ ] Success/failure criteria defined (with numbers)\n- [ ] Tracking instrumented (can you measure what you need?)\n- [ ] Sample size calculated (do you have enough users?)\n- [ ] Timeline set (when do you make decision?)\n\nThe honesty check: If results are negative, will you actually kill this? If not, don't waste time on experiment.\n\n### What You'll Measure\n\nPrimary metric:\n[The one number that answers your hypothesis]\n\nSecondary metrics:\n- [Supporting evidence]\n- [Quality checks]\n\nQualitative signals:\n- [ ] User interviews with participants\n- [ ] Support tickets or feedback\n- [ ] What people say vs. what they do\n\n### Decision Framework\n\nIf results show:\n\n| Outcome | Action |\n|---------|--------|\n| Strong yes ([X]% success) | Build it with confidence |\n| Weak yes ([Y]% success) | Iterate on experiment or design |\n| Unclear / mixed | Run more experiments, different approach |\n| Strong no ([Z]% success) | Kill it, learn why, move on |\n\nThe hard part: Actually killing features when experiments fail.\n\n---\n\n## PART 4: MULTIPLE EXPERIMENT STRATEGIES\n\n### Test Multiple Variations\n\nIf you're not sure of the best approach:\n\nExperiment A: [Approach 1]\nExperiment B: [Approach 2]Experiment C: [Approach 3]\n\nWhy multiple: You're testing different hypotheses about what customers want\n\nExample:\n- A: Test if they want feature as premium add-on ($20/mo)\n- B: Test if they want it bundled in higher tier\n- C: Test if they want it free but with usage limits\n\nCheapest test for each:\n- A: Email to 100 customers offering add-on, measure interest\n- B: Mock up new pricing page, test click-through\n- C: Build free version with \"upgrade for unlimited\" prompt\n\n### Sequential Testing (Build Confidence)\n\nWeek 1: Fake door test\n- If >20% interest → Continue\n- If <10% interest → Stop\n\nWeek 2: Concierge test with 5 customers\n- If 4/5 use it regularly → Continue\n- If <3/5 engage → Stop\n\nWeek 4: Prototype with 20 customers\n- If >50% weekly active → Build real feature\n- If <30% active → Redesign or kill\n\nWhy sequential: Each stage reduces risk before bigger investment\n\n---\n\n## THE OUTPUT\n\n### Experiment Brief\n\nHypothesis:\n[What we believe]\n\nRecommended Experiment:\n[Type and approach]\n\nWhat we'll build/test:\n[Specific description]\n\nWho we'll test with:\n[Segment, size]\n\nSuccess looks like:\n[Metric > X%]\n\nFailure looks like:\n[Metric < Y%]\n\nTimeline:\n[X weeks to run, Y weeks to analyze]\n\nCost:\n[Resources needed]\n\nDecision:\n[What we'll do based on each outcome]\n\n### Multiple Experiment Options\n\nOption 1: Cheapest/Fastest\n[Details]\n\nOption 2: Medium Investment\n[Details]\n\nOption 3: Highest Confidence\n[Details]\n\nRecommendation: [Which to start with and why]\n\n</experiment_framework>\n\n<quality_check>\n\nIs this actually an experiment?\n- [ ] Tests a specific hypothesis (not \"let's see what happens\")\n- [ ] Has clear success/failure criteria (not subjective)\n- [ ] Costs less than just building it (otherwise just ship)\n\nWill you learn something actionable?\n- [ ] Results inform a real decision\n- [ ] You'll actually kill feature if experiment fails\n- [ ] You're testing the riskiest assumption first\n\nIs it the simplest test possible?\n- [ ] Can't make it cheaper/faster without losing signal\n- [ ] Not over-engineering the validation\n\n</quality_check>\n\n<meta_wisdom>\n\nOn experiments:\n\nMost teams don't run experiments because they're afraid of negative results.\n\nBut here's the truth: Negative results are good. They save you from building the wrong thing.\n\nPositive results are expensive (you have to build it).\nNegative results are cheap (you don't build it).\n\nThe key insight:\n\nThe goal isn't to validate your ideas.\nThe goal is to invalidate them as cheaply as possible.\n\nIf an idea survives 3 cheap experiments trying to kill it, then it's probably worth building.\n\nOn fake door tests:\n\nSome PMs worry: \"Isn't this lying to customers?\"\n\nNo. You're being honest: \"We're considering building X. Interested?\"\n\nWhat's dishonest is building something no one wants because you never asked.\n\nRemember:\n\nBuild to learn, not to ship.\n\nThe best PMs kill 80% of ideas before writing code.\nThe worst PMs build 100% of ideas and wonder why adoption sucks.\n\nExperiment early. Experiment often. Celebrate when experiments fail—you just saved the company from wasting resources.\n\n</meta_wisdom>\n\n</experiment_brainstorming>",
    "technique": "Hypothesis testing, minimum viable test design, learning-oriented framing",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to validate assumptions before committing to full build"
  },
  {
    "name": "Draft an Impact Sizing\n",
    "category": "Analytics",
    "prompt": "<impact_sizing>\n\n<sizing_inputs>\nWHAT YOU'RE SIZING:\n1. Feature/initiative description (what are you building)\n2. What metric will this impact? (conversion, retention, revenue, efficiency)\n3. Current baseline (what's the metric today)\n4. Your hypothesis about impact (educated guess of effect size)\n\nYOUR CONTEXT:\n5. Total addressable users (how many people could use this)\n6. Expected adoption rate (what % will actually use it)\n7. Time to impact (when do results show up)\n\nDATA YOU HAVE:\n8. Usage analytics (current behavior)\n9. Customer research (stated need intensity)\n10. Comparable features (what similar features did)\n\nUPLOADS (if available):\n- Analytics screenshot or export\n- Customer feedback mentioning this need\n- Competitor data on similar features\n</sizing_inputs>\n\n<sizing_framework>\n\nYou're a senior PM who knows that most impact sizing is either wildly optimistic (\"This will double revenue!\") or uselessly vague (\"This will improve engagement\"). Your job: Build a model that's honest, defensible, and shows your assumptions.\n\nTHE REALITY:\n\nBad impact sizing: \"This will increase conversion by 10%\"\nGood impact sizing: \"If 30% of users try it, and it improves their success rate from 40% to 50%, we'll see 1.5% lift in overall conversion. Here's the math and assumptions.\"\n\nThe goal isn't to be perfectly accurate. It's to:\n1. Make assumptions explicit\n2. Show your reasoning\n3. Identify what you're most uncertain about\n\n---\n\n## PART 1: UNDERSTAND THE VALUE CHAIN\n\n### Map How Feature → Metric\n\nThe feature:\n[What you're building]\n\nWho it affects:\n[Specific user segment]\n\nWhat they'll do differently:\n[Behavior change you expect]\n\nHow that impacts metric:\n[Chain of causation]\n\nExample:\n- Feature: One-click checkout\n- Affects: Mobile users (40% of traffic)\n- Behavior change: Fewer drop-offs at payment step\n- Impact chain: Mobile cart → checkout flow → conversion rate → revenue\n\n### Identify Your Assumptions\n\nEvery impact model is built on assumptions. Make them explicit.\n\nASSUMPTION 1: Adoption\n- Assumption: [X% of users will use this feature]\n- Based on: [Similar feature adoption, customer research, gut feel]\n- Confidence: [High/Medium/Low]\n\nASSUMPTION 2: Effect Size\n- Assumption: [Feature will improve [metric] by Y%]\n- Based on: [Competitor benchmarks, A/B test results, customer data]\n- Confidence: [High/Medium/Low]\n\nASSUMPTION 3: Time to Impact\n- Assumption: [Will take Z weeks to see full effect]\n- Based on: [Usage frequency, network effects, adoption curve]\n- Confidence: [High/Medium/Low]\n\nThe assumption you're most unsure about: [Which one and why]\n\n---\n\n## PART 2: BUILD THE MODEL\n\n### The Basic Formula\n\nImpact = (Total Users) × (Adoption Rate) × (Effect Size) × (Metric Value)\n\nLet's break it down step by step:\n\nSTEP 1: Size the Audience\n\nTotal users who could benefit: [X users/month]\n\nHow we got this number:\n- Total MAU: [number]\n- Segment this affects: [%]\n- Addressable audience: [calculation]\n\nSTEP 2: Estimate Adoption\n\nNot everyone will use a new feature.\n\nAdoption scenarios:\n- Conservative: [X%] adopt in first 90 days\n- Moderate: [Y%] adopt in first 90 days\n- Optimistic: [Z%] adopt in first 90 days\n\nWhy these numbers:\n- Similar features in our product: [X-Y% adoption]\n- Industry benchmarks: [Z% typical]\n- Our assumption: [Which scenario we believe]\n\nSTEP 3: Estimate Effect Size\n\nFor users who adopt, how much better do they do?\n\nCurrent state:\n- Baseline metric: [current performance]\n- Example: \"40% of mobile users complete checkout\"\n\nWith new feature:\n- Expected metric: [new performance]\n- Example: \"Estimate 50% complete checkout (25% relative lift)\"\n\nWhy this estimate:\n- Competitor data: [Company X reports Y% improvement]\n- Our data: [Similar change resulted in Z% improvement]\n- Customer research: [% who said this would help significantly]\n\nSTEP 4: Calculate ImpactConservative Case:\n- Users affected: [X]\n- Adoption: [Y%]\n- Effect size: [Z% improvement]\n- Result: [Impact on metric]\n\nModerate Case:\n- [Same structure with moderate assumptions]\n- Result: [Impact on metric]\n\nOptimistic Case:\n- [Same structure with optimistic assumptions]\n- Result: [Impact on metric]\n\n### Show Your Math (Worked Example)\n\nFeature: Add SSO login for enterprise\n\nBaseline:\n- Enterprise segment: 200 customers\n- Current activation rate: 60% (120 activate)\n- Each activated customer worth $50K/year ARR\n\nAssumptions:\n- 70% of enterprise customers will use SSO (140 customers)\n- SSO will increase activation from 60% to 75%\n- Takes 6 months to fully roll out\n\nCalculation:\n\nCurrent state:\n- 200 customers × 60% activation = 120 activated\n- 120 × $50K = $6M ARR from enterprise\n\nWith SSO:\n- SSO customers: 200 × 70% = 140\n- SSO activation: 140 × 75% = 105 activated via SSO\n- Non-SSO customers: 60 (still 60% activate) = 36 activated\n- Total activated: 105 + 36 = 141 activated\n\nNew ARR:\n- 141 × $50K = $7.05M ARR\n- Incremental impact: +$1.05M ARRSensitivity check:\n- If only 50% adopt SSO: +$600K ARR\n- If only 80% adopt SSO: +$1.2M ARR\n\n---\n\n## PART 3: VALIDATE YOUR ASSUMPTIONS\n\n### The Sanity Checks\n\nSmell test questions:1. Is this too good to be true?\n- If you're projecting >20% improvement to a major metric, you better have strong evidence\n- Big wins are rare; most features move metrics 1-5%\n\n2. Have you accounted for cannibalization?\n- Will new feature pull users from existing successful flows?\n- Are you double-counting benefits?\n\n3. What about adoption curve?\n- Impact isn't instant; how long to full adoption?\n- Have you accounted for ramp time?\n\n4. What could make you wrong?\n- List 3 ways your model breaks\n- What's the worst case scenario?\n\n### Identify Uncertainty\n\nWhat you're confident about:\n- [Assumption with strong data]\n\nWhat you're guessing about:\n- [Assumption with weak data]\n\nHow to de-risk:\n- [What experiment or analysis would increase confidence]\n\nExample:\n- Confident: \"40% of users are on mobile\" (we have analytics)\n- Guessing: \"One-click checkout will reduce drop-off by 20%\" (no data)\n- De-risk: Run fake door test showing one-click option, measure click rate\n\n---\n\n## PART 4: TRANSLATE TO BUSINESS IMPACT\n\n### From Metric to Money\n\nMost features impact intermediate metrics. Translate to business outcomes.\n\nIf you're improving conversion:\n\nCurrent:\n- 10,000 visitors/month\n- 5% convert = 500 customers\n- $100 average order = $50K revenue/month\n\nWith feature (+1.5% absolute conversion):\n- 10,000 visitors\n- 6.5% convert = 650 customers\n- +150 customers × $100 = +$15K/month = +$180K/yearIf you're improving retention:\n\nCurrent:\n- 1000 customers\n- 5% monthly churn = 50 leave each month\n- $50/month ARPU\n- Annual churned revenue = 50 × 12 × $50 = $30K lost\n\nWith feature (-1% absolute churn):\n- 4% monthly churn = 40 leave\n- Annual churned revenue = $24K\n- Saved $6K/year in churn\n\n### Cost-Benefit\n\nInvestment required:\n- Engineering: [X weeks]\n- Design: [Y weeks]\n- PM/QA: [Z weeks]\n- Total cost: [loaded cost ~$150-250/hr for eng]\n\nExpected return:\n- Conservative: [$ impact]\n- Moderate: [$ impact]\n- Optimistic: [$ impact]\n\nPayback period:\n- Investment / Monthly impact = [X months to break even]\n\nThe decision:\n[Is this worth building based on ROI?]\n\n---\n\n## THE OUTPUT\n\n### Impact Sizing Summary (One Page)\n\nFeature: [What we're building]\n\nTarget Metric: [What improves]\n\nExpected Impact:\n- Conservative: [X% improvement / $Y value]\n- Moderate: [X% improvement / $Y value]\n- Optimistic: [X% improvement / $Y value]\n\nKey Assumptions:\n1. [Assumption 1 with confidence level]\n2. [Assumption 2 with confidence level]\n3. [Assumption 3 with confidence level]\n\nInvestment: [Cost]\nPayback Period: [Months]\n\nRecommendation: [Build / Don't Build / Validate Assumptions First]\n\n### Detailed Model\n\n[All calculations from above with clear assumptions]\n\n### Assumption Testing Plan\n\nTo increase confidence, we should:\n1. [Experiment or analysis to validate key assumption]\n2. [Next experiment]\n\n</sizing_framework>\n\n<quality_check>\n\nAre your assumptions realistic?\n- [ ] Not assuming 100% adoption\n- [ ] Effect sizes are in line with similar features\n- [ ] You've accounted for ramp time\n\nCan someone challenge your model?\n- [ ] All assumptions are explicit\n- [ ] Math is shown step-by-step\n- [ ] You've identified what you're uncertain about\n\nIs it actionable?\n- [ ] Clear enough to inform prioritization\n- [ ] Identifies what would change your mind\n- [ ] Conservative enough to be believable\n\n</quality_check>\n\n<meta_wisdom>\n\nOn impact sizing:\n\nMost PMs make one of two mistakes:\n\nMistake 1: Wildly optimistic\n\"This will 10x our conversion rate!\"\n(No it won't. It'll move it 2-5% if you're lucky.)\n\nMistake 2: Refuse to estimate\n\"We can't know the impact until we ship.\"\n(True, but educated guesses beat no guesses.)\n\nThe right approach:\nMake an estimate. Show your assumptions. Update as you learn.\n\nThe key principle:\n\nImpact sizing isn't about being right.\nIt's about being honest about uncertainty.\n\nGood model: \"If 30% adopt and it works, we'll see $500K lift. But adoption could be 10-50%, so real range is $150K-$1M.\"\n\nBad model: \"This will generate $847,293 in year one.\"\n\nOn assumptions:\n\nThe most dangerous assumptions are the ones you don't write down.\n\nBecause if you don't write them, you can't test them.\nAnd if you can't test them, you'll believe them even when they're wrong.\n\nRemember:\n\nAll models are wrong. Some are useful.\n\nYour job isn't to predict the future perfectly.\nIt's to make the best decision you can with imperfect information.\n\nShow your work. State your assumptions. Update your beliefs when reality hits.\n\n</meta_wisdom>\n\n</impact_sizing>",
    "technique": "Assumption-based modeling, sensitivity analysis, showing your work",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to estimate the business impact of a feature to prioritize roadmap or get buy-in"
  },
  {
    "name": "Respond to Email",
    "category": "Productivity",
    "prompt": "<email_response>\n\n<email_inputs>\nPASTE THE EMAIL YOU NEED TO RESPOND TO:\n[Full email thread]\n\nYOUR CONTEXT:\n1. Who is this from? (customer, exec, team member, external)\n2. What do they want? (decision, update, approval, information)\n3. What's your relationship? (direct report, peer, boss, customer)\n4. Urgency level? (respond in 5 min vs. can wait)\n5. Tone needed? (formal, casual, apologetic, firm)\n\nOPTIONAL:\n- Related context (Slack threads, docs, previous decisions)\n- Your initial draft (if you started writing and got stuck)\n</email_inputs>\n\n<response_framework>\n\nYou're an executive communication coach who knows that most emails are too long, too vague, or too slow. Your job: Help write responses that are clear, complete, and get results.\n\nTHE REALITY:\n\nBad email: 3 paragraphs of context, buried request, vague next steps\nGood email: Clear ask, direct answer, explicit next steps\n\nMost people over-explain. The best emails are half the length you think they need to be.\n\n---\n\n## PART 1: UNDERSTAND WHAT THEY ACTUALLY WANT\n\n### Decode The Email\n\nWhat they're asking for:\n[Explicit request]\n\nWhat they actually need:\n[Underlying need, which might be different]\n\nWhat they're worried about:\n[Subtext, concern, or urgency driver]\n\nExample:\n- Asking: \"Can you send me the roadmap?\"\n- Actually need: \"I need to know if feature X will ship this quarter for my customer\"\n- Worried: \"I'm about to lose a deal if we don't have this feature\"\n\n### Determine Response Type\n\nDECISION NEEDED:\nThey're asking you to decide something\n→ Response: Clear yes/no with brief rationale\n\nINFORMATION REQUEST:\nThey need to know something\n→ Response: Direct answer, then context if needed\n\nUPDATE REQUEST:\nThey want status on project\n→ Response: Current state, what's next, when to expect update\n\nPROBLEM ESCALATION:\nSomething's broken and they need help\n→ Response: Acknowledge, action plan, timeline\n\nYour email is: [Which type]\n\n---\n\n## PART 2: STRUCTURE YOUR RESPONSE\n\n### The Formula\n\nLine 1: Direct answer or action\n[Yes/No/Here's the status/I'll do X by Y]\n\nLines 2-4: Brief context (if needed)\n[Why/how/what happened]\n\nLine 5: Next steps\n[What happens next, who does what, by when]\n\nThat's it. Most emails should be 3-5 sentences.\n\n### Write The Response\n\nDRAFT:\n\n[Opening - match their tone]\n\n[Direct answer - first sentence]\n\n[Context - 1-2 sentences max]\n\n[Next steps - specific and owned]\n\n[Closing]\n\n---\n\n## PART 3: RESPONSE PATTERNS BY TYPE\n\n### Pattern 1: Saying No (Hardest One)\n\nStructure:\n1. Acknowledge request\n2. Clear \"no\" with reason\n3. Alternative if possible\n\nExample:\n\n\"Thanks for checking on feature X for the Q2 roadmap.\n\nWe won't be shipping it in Q2—we're focused on [other priority] which addresses [customer need].\n\nWhat we can do: Get you early access to [alternative] which solves [80% of use case]. Would that work for your customer?\"\n\nKey: Don't apologize excessively. Be clear, not defensive.\n\n### Pattern 2: Giving Status Update\n\nStructure:\n1. Current status (one sentence)\n2. What's changed/progress\n3. Next milestone and date\n\nExample:\n\n\"PRD for SSO integration is in final review.\n\nCompleted: Technical feasibility, design mocks, customer validation (8 enterprises confirmed they'd upgrade)\nNext: Engineering sizing this week, targeting Q3 kickoff.\n\nI'll send full PRD by Friday.\"\n\nKey: Be specific about dates and deliverables.\n\n### Pattern 3: Making a Decision\n\nStructure:\n1. The decision\n2. Quick rationale\n3. What happens next\n\nExample:\n\n\"Let's go with Option B: Phased rollout starting with enterprise tier.\n\nRationale: Lower risk if there are bugs, and enterprise customers have been asking for this most urgently.\n\nNext: I'll update the PRD and sync with eng tomorrow on timeline.\"\n\nKey: Don't waffle. Pick something.\n\n### Pattern 4: Asking For Something\n\nStructure:\n1. What you need\n2. Why you need it (brief)\n3. When you need it by\n\nExample:\n\n\"Can you share last quarter's churn data by customer segment?\n\nI'm sizing the impact of the new onboarding flow and need to understand which segments churn fastest.\n\nNeed this by EOD Thursday to finalize the PRD.\"\n\nKey: Make it easy for them to help you.\n\n### Pattern 5: Apologizing/Recovering\n\nStructure:\n1. Acknowledge the problem\n2. What you're doing to fix it\n3. When they'll see resolution\n\nExample:\n\n\"You're right, we should have communicated the pricing change sooner—this caught your team off guard.\n\nHere's what I'm doing:\n- Email going to all customers today with 30-day notice\n- FAQ doc published\n- Office hours scheduled for next week\n\nI'll personally call your top 3 customers this afternoon.\"\n\nKey: Own it, fix it, move forward. Don't dwell.\n\n---\n\n## PART 4: TONE CALIBRATION\n\n### Match The Situation\n\nResponding up (to exec/boss):\n- Be concise\n- Lead with answer\n- Show you have it handled\n\nResponding to peer:\n- Collaborative tone\n- Assume good intent\n- Clear on who does what\n\nResponding to customer:\n- Empathetic\n- Solution-oriented\n- Set expectations clearly\n\nYour tone for this email: [Formal/Casual/Apologetic/Direct]\n\n### Remove Weakeners\n\nDelete these phrases:\n- ❌ \"Just following up...\"\n- ❌ \"I think maybe we could...\"\n- ❌ \"Sorry to bother you but...\"\n- ❌ \"Does that make sense?\"\n\nUse these instead:\n- ✅ \"Following up on...\"\n- ✅ \"We should...\"\n- ✅ [Just state your request]\n- ✅ \"Let me know if you need clarification.\"\n\n### The Confidence Edit\n\nRead your draft. Count the words \"just,\" \"maybe,\" \"possibly,\" \"probably.\"\n\nDelete them. See if it's still clear. (It is.)\n\n---\n\n## THE OUTPUT\n\n### Recommended Response\n\nSubject line: [If starting new thread or clarifying]\n\nEmail:\n\n[Opening]\n\n[Direct answer/action]\n\n[Brief context if needed]\n\n[Next steps with dates/owners]\n\n[Closing]\n\nEstimated length: [X sentences]\nTone: [Match to relationship and situation]\n\n### Alternative Versions\n\nVersion 1: More detailed\n[If they need more context]\n\nVersion 2: More direct\n[If relationship allows brevity]\n\nVersion 3: More formal\n[If external or sensitive]\n\n</response_framework>\n\n<quality_check>\n\nIs it clear?\n- [ ] They know what you're saying in first sentence\n- [ ] No ambiguity about next steps\n- [ ] No jargon without definition\n\nIs it complete?\n- [ ] Answers their actual question\n- [ ] Provides necessary context\n- [ ] Includes next steps with timeline\n\nIs it concise?\n- [ ] Could you delete a paragraph and it still makes sense?\n- [ ] Are you over-explaining?\n\n</quality_check>\n\n<meta_wisdom>\n\nOn email length:\n\nYour first draft is always too long.\n\nCut it in half. It's probably better.\n\nMost people write emails to think through the problem. That's fine for your draft. Delete 50% before sending.\n\nThe BLUF principle (Bottom Line Up Front):\n\nMilitary communication rule: Lead with the conclusion.\n\nDon't make people read 3 paragraphs to find out you're saying \"yes.\"\n\nOn tone:\n\nMatch their energy:\n- They're formal → You're professional\n- They're casual → You're friendly\n- They're stressed → You're calm and solution-oriented\n\nThe hard truth about saying no:\n\nMost PMs are bad at saying no because they try to soften it.\n\n\"Unfortunately, we're probably not going to be able to maybe prioritize this right now due to resource constraints...\"\n\nJust say: \"We won't be building this in Q2. Here's why and here's what we can do instead.\"\n\nClear no > vague maybe.\n\nRemember:\n\nYour job isn't to write the perfect email.\nIt's to communicate clearly and move things forward.\n\nSend it. Most emails don't deserve more than 5 minutes.\n\n</meta_wisdom>\n\n</email_response>",
    "technique": "Context extraction, tone matching, action-oriented responses",
    "tools": "Claude Project, Lindy, Relay.app",
    "useCase": "Inbox is full, need to respond to stakeholders/customers quickly and clearly"
  },
  {
    "name": "Prep for 1:1s\n",
    "category": "Productivity",
    "prompt": "<one_on_one_prep>\n\n<meeting_inputs>\nWHO ARE YOU MEETING WITH:\n1. Person's name and role (manager, direct report, peer, cross-functional partner)\n2. Your relationship (new, established, difficult, great)\n3. Meeting frequency (weekly, biweekly, monthly, ad-hoc)\n4. Last meeting notes (if you have them - what was discussed)\n\nCURRENT CONTEXT:\n5. What's happening in their world right now? (launches, issues, wins, stress)\n6. What's happening in your world? (what you need from them or want to share)\n7. Any tension points or difficult topics?\n8. Meeting length (30 min, 1 hr)\n\nOPTIONAL UPLOADS:\n- Previous 1:1 notes\n- Open action items\n- Project status docs\n- Performance review notes (if direct report)\n</meeting_inputs>\n\n<prep_framework>\n\nYou're a leadership coach who knows that most 1:1s are wasted. People show up unprepared, discuss only surface-level status, and leave without clarity. Your job: Make these 30-60 minutes actually matter.\n\nTHE REALITY:\n\nBad 1:1s: Rambling status updates neither person cares about\nGood 1:1s: Focused on what matters, surfaces real issues, builds relationship\n\nThe best 1:1s aren't meetings. They're conversations that make both people better at their job.\n\n---\n\n## PART 1: CLARIFY THE PURPOSE\n\n### Different Types of 1:1s\n\nWITH YOUR MANAGER:\n- Their job: Help you be successful, remove blockers, give guidance\n- Your job: Keep them informed, ask for help, align on priorities\n\nWITH YOUR DIRECT REPORT:\n- Your job: Coach them, unblock them, help them grow\n- Their job: Update you on progress, ask for guidance, flag issues\n\nWITH PEER/CROSS-FUNCTIONAL:\n- Mutual job: Align on shared work, resolve conflicts, coordinate\n\nThis meeting is: [Which type]\n\n### What Makes This Meeting Matter Right Now\n\nTopics that need discussion:HIGH PRIORITY (Must discuss):\n- [ ] [Urgent decision needed]\n- [ ] [Blocker affecting work]\n- [ ] [Tension point to address]\n\nMEDIUM PRIORITY (Should discuss if time):\n- [ ] [Strategic alignment]\n- [ ] [Feedback to give/get]\n- [ ] [Career/growth topic]\n\nLOW PRIORITY (Nice to discuss):\n- [ ] [General updates]\n- [ ] [Future planning]\n\nThe one thing that MUST come out of this meeting:\n[Specific outcome]\n\n---\n\n## PART 2: BUILD YOUR AGENDA\n\n### The Structure (Default Template)\n\nFIRST 5 MIN: THEIR TOPICS\n\"What's on your mind?\" or \"What do you want to talk about?\"\n\nLet them drive. They might have something urgent you don't know about.\n\nNEXT 15-20 MIN: YOUR KEY TOPICS\nThe 2-3 things you need to discuss (prioritized above)\n\nLAST 5-10 MIN: OPEN TIME\n- Action items review\n- Looking ahead\n- Anything else\n\n### Prepare Your Topics\n\nFor each topic you want to discuss:\n\nTOPIC 1: [Name it clearly]Context (1-2 sentences):\n[What they need to know to discuss this]\n\nThe question/discussion:\n[What you want to talk about]\n\nWhat you need from them:\n- Decision on X\n- Input on Y\n- Unblock Z\n- Just venting/thinking out loud\n\nExample:\n\nTopic: Roadmap prioritization for Q2\n\nContext: We have 3 big features we could build but only capacity for 1. All have customer demand.\n\nDiscussion: Walk through trade-offs and get your input on how to think about prioritization.\n\nNeed: Your perspective on what matters most strategically, then I'll make the call.\n\n---\n\n## PART 3: SPECIFIC PREP BY RELATIONSHIP TYPE\n\n### If Meeting With Your MANAGER\n\nQuestions to prepare:For decisions/guidance:\n- \"I'm deciding between X and Y. Here's my thinking... what am I missing?\"\n- \"I'm stuck on [problem]. What would you do?\"\n\nFor visibility:\n- \"Here's what's going well: [wins]\"\n- \"Here's what's at risk: [concerns]\"\n- \"Here's where I need help: [blockers]\"\n\nFor growth:\n- \"I want to get better at [skill]. What opportunities should I look for?\"\n- \"How am I doing on [thing we discussed last time]?\"\n\nPrep checklist:\n- [ ] List wins (make your manager look good)\n- [ ] List risks (no surprises)\n- [ ] Specific asks (not vague \"I need help\")\n- [ ] Updates on action items from last meeting\n\nThe key insight: Your manager's job is to help you be successful. Make it easy by being clear about what you need.\n\n### If Meeting With Your DIRECT REPORT\n\nYour preparation:Check in on their work:\n- What's their biggest challenge right now?\n- Are they blocked on anything?\n- What are they proud of?\n\nCheck in on them as a person:\n- How are they doing? (energy level, stress, motivation)\n- Are they growing? Learning?\n- Do they feel supported?\n\nTopics you need to cover:\n- [ ] Feedback (both positive and constructive)\n- [ ] Career development\n- [ ] Course corrections\n- [ ] Context they're missing\n\nQuestions to ask:Opening:\n- \"What's top of mind for you?\"\n- \"What's going well? What's not?\"\n\nUnblocking:\n- \"What do you need from me?\"\n- \"What's slowing you down?\"\n\nGrowth:\n- \"What do you want to get better at?\"\n- \"What was the hardest decision you made this week?\"\n\nFeedback to give:Positive feedback (be specific):\n- \"The way you handled [situation] was great because [impact]\"\n\nConstructive feedback:\n- \"I noticed [specific behavior]. Here's the impact: [consequence]. Let's talk about how to approach it differently.\"\n\nThe key insight: Don't wait for performance reviews. Small feedback frequently beats big feedback rarely.\n\n### If Meeting With PEER/CROSS-FUNCTIONAL\n\nYour preparation:Alignment topics:\n- [ ] Where are we on shared project?\n- [ ] Any dependencies or handoffs?\n- [ ] Timeline changes affecting them?\n\nIssues to surface:\n- [ ] Misalignment on approach\n- [ ] Resource conflicts\n- [ ] Communication breakdown\n\nQuestions to ask:\n- \"How's [project] going from your side?\"\n- \"What do you need from me to be successful?\"\n- \"Where are we misaligned?\"\n\nThe key insight: Peer relationships are about mutual success. Help them win and they'll help you win.\n\n---\n\n## PART 4: DIFFICULT CONVERSATIONS PREP\n\n### If There's Tension\n\nName it, don't avoid it:\n\n\"I want to talk about [thing that's awkward]. I think we see it differently and I want to understand your perspective.\"\n\nPrepare:\n- [ ] The facts (not your interpretation)\n- [ ] Your perspective (own it as \"I\" statements)\n- [ ] What you're curious about (their view)\n- [ ] Desired outcome (what good looks like)\n\nExample:\n\n\"I noticed we disagreed in the meeting about the launch approach. I pushed for X because [reasoning]. I sensed you thought that was wrong. Can you help me understand your concern?\"\n\n### If You Need To Give Hard Feedback\n\nPrepare the feedback:Situation: [Specific behavior, with context]\nImpact: [Why it matters, who it affects]\nRequest: [What you want them to do differently]\n\nExample:\n\n\"In yesterday's review, you interrupted the designer three times while she was presenting. I think you didn't realize, but it made her feel like you weren't interested in her work. In future reviews, let's let people finish their thought before jumping in.\"\n\nThe key principle: Be direct, be kind, be specific.\n\n---\n\n## THE OUTPUT\n\n### Your 1:1 Agenda (Share Beforehand)\n\nMeeting: [Your Name] ↔ [Their Name]\nDate: [Date]\nTime: [Duration]\n\nTheir topics:\n- [Space for them to add]\n\nMy topics:\n1. [Topic 1 - one line description]\n2. [Topic 2 - one line description]\n3. [Topic 3 - one line description]\n\nStanding items:\n- Action items from last time\n- Looking ahead\n\n---\n\n### Your Prep Notes (For You Only)\n\nMy outcomes for this meeting:\n1. [Specific outcome]\n2. [Specific outcome]\n\nTopics I must cover:\n- [Topic with key points]\n\nQuestions to ask:\n- [Specific questions]\n\nFeedback to give:\n- [Prepared feedback]\n\nWhat I need from them:\n- [Specific asks]\n\nFollow-up actions I expect:\n- [Likely action items]\n\n---\n\n### Meeting Notes Template (Fill During/After)\n\nDate: [Date]\n\nDiscussed:\n- [Topic 1: Key points]\n- [Topic 2: Key points]\n\nDecisions:\n- [What was decided]\n\nAction items:\n- [ ] [Person]: [Action] by [date]\n- [ ] [Person]: [Action] by [date]\n\nNext time:\n- [Topics to follow up on]\n\n</prep_framework>\n\n<quality_check>\n\nIs your agenda focused?\n- [ ] 2-4 topics max (not 10 updates)\n- [ ] Topics that need discussion, not just FYI\n- [ ] Clear what you need from them\n\nAre you prepared to listen?\n- [ ] Asking questions, not just talking\n- [ ] Open to their topics\n- [ ] Ready to hear things you might not want to hear\n\nIs this actually valuable?\n- [ ] Better than an email or Slack?\n- [ ] Real-time discussion needed?\n- [ ] Building relationship, not just transacting\n\n</quality_check>\n\n<meta_wisdom>\n\nOn 1:1 frequency:\n\nWeekly is ideal for directs and manager.\nBiweekly works for stable relationships.\nMonthly is too infrequent—you lose continuity.\n\nThe biggest mistake:\n\nTreating 1:1s as status meetings.\n\nStatus can be async. Use 1:1s for:\n- Coaching and feedback\n- Strategic thinking\n- Career development\n- Building trust\n- Resolving tension\n\nOn preparation:\n\nThe person who prepares wins.\n\nIf you show up with clear topics, you drive the conversation.\nIf you show up unprepared, you waste 30 minutes of both your lives.\n\nThe hard truth:\n\nMost people avoid difficult conversations in 1:1s.\n\nThey stick to safe topics, avoid feedback, pretend tension doesn't exist.\n\nGreat 1:1s surface the hard stuff. That's where growth happens.\n\nRemember:\n\n1:1s are the highest-leverage time in your calendar.\n\n30 minutes with the right person, discussing the right thing, can unblock a week's worth of work.\n\nDon't waste them on status updates.\n\n</meta_wisdom>\n\n</one_on_one_prep>",
    "technique": "Relationship context modeling, issue prioritization, outcome-focused agendas",
    "tools": "Claude, ChatGPT Projects, NotebookLM",
    "useCase": "Have 1:1 meeting in an hour, need to prepare talking points and agenda"
  },
  {
    "name": "Write Status Updates",
    "category": "Productivity",
    "prompt": "<status_update>\n\n<update_inputs>\nWHAT YOU'RE UPDATING ON:\n1. Project/initiative name\n2. Who's the audience? (team, execs, stakeholders, company-wide)\n3. Update frequency (weekly, biweekly, monthly)\n4. What period are you covering? (this week, this sprint, this quarter)\n\nYOUR STATUS:\n5. Overall status: Green (on track), Yellow (at risk), Red (blocked)\n6. Key accomplishments this period\n7. What's next\n8. Blockers or risks\n9. What you need from audience\n\nOPTIONAL UPLOADS:\n- Previous status update\n- Project plan or roadmap\n- Metrics dashboard\n- Meeting notes\n</update_inputs>\n\n<update_framework>\n\nYou're a communications expert who knows that most status updates are unread. They're too long, too detailed, or bury the important stuff. Your job: Write updates that people actually read and act on.\n\nTHE REALITY:\n\nBad status update: Wall of text, every detail, no clear status\nGood status update: Scannable, highlights what matters, clear on what you need\n\nExecs skim. Busy people skim. Your update needs to work when skimmed.\n\n---\n\n## PART 1: PICK YOUR FORMAT\n\n### Format By Audience\n\nFOR YOUR TEAM (Weekly):\n- More detail, tactical\n- What got done, what's next\n- Blockers and help needed\n\nFOR STAKEHOLDERS (Biweekly/Monthly):\n- Higher level, outcomes not tasks\n- Progress toward goals\n- Risks and decisions needed\n\nFOR EXECS (Monthly or milestones):\n- Executive summary only\n- Health status, key metrics, asks\n- 3-5 bullet points max\n\nYour audience: [Team/Stakeholders/Execs]\nFormat to use: [Detailed/Medium/Executive]\n\n---\n\n## PART 2: THE STRUCTURE\n\n### The Universal Template\n\nOVERALL STATUS: 🟢 GREEN / 🟡 YELLOW / 🔴 RED\n\n[One-sentence summary of where things stand]\n\n✅ WINS / PROGRESS\n[What got done, what moved forward]\n\n⏭️ NEXT / UPCOMING\n[What's happening next period]\n\n⚠️ RISKS / BLOCKERS\n[What could go wrong, what's stuck]\n\n🙋 ASKS / NEEDS\n[What you need from recipients]\n\nThat's it. Everything important in 4 sections.\n\n---\n\n## PART 3: WRITE EACH SECTION\n\n### Overall Status Line\n\nOne sentence that captures current state.Green (on track):\n✅ \"Feature X on track for May 15 launch\"\n✅ \"Q1 goals 80% complete, no major blockers\"\n\nYellow (at risk):\n⚠️ \"Launch delayed 2 weeks due to API instability\"\n⚠️ \"On track but design resources stretched thin\"\n\nRed (blocked):\n🚨 \"Blocked on legal approval, launch date TBD\"\n🚨 \"Behind on Q1 goals, need to cut scope\"\n\nYour status line:\n[Status emoji + one sentence]\n\n### Wins / Progress\n\nWhat to include:\n- Completed milestones\n- Shipped features\n- Key decisions made\n- Metrics that improved\n- Customer wins\n\nHow to write:\n- Be specific (not \"made progress\" but \"shipped X\")\n- Include impact when possible\n- Celebrate your team\n\nExample:\n\n✅ Shipped SSO integration to 50 enterprise customers\n- 78% activated in first week\n- NPS +15 points vs. customers without SSO\n\n✅ Finalized Q2 roadmap with eng/design alignment\n- Cut 3 features to focus on core use case\n- Capacity plan confirmed\n\n✅ Closed 3 design-blocking decisions\n- Navigation will be sidebar (not top nav)\n- Mobile web first, native app v2\n- Free tier stays at 5 projects\n\nYour wins:\n[3-5 specific accomplishments]\n\n### Next / Upcoming\n\nWhat's happening in the next period.For team updates:\n- Specific tasks and owners\n- Sprint goals\n- Meetings or milestones\n\nFor stakeholder updates:\n- Key milestones\n- Expected completions\n- Decision points coming\n\nExample:\n\n⏭️ Next 2 weeks:\n- Design review with execs (May 5)\n- Begin engineering build (May 8)\n- Beta customer recruitment (May 10)\n\n⏭️ Upcoming decisions:\n- Pricing model for enterprise tier (need input by May 15)\n- Launch date: June 1 vs June 15 (will decide after eng sizing)\n\nYour upcoming:\n[3-5 next items with dates]\n\n### Risks / Blockers\n\nWhat could derail you.Include:\n- Technical risks\n- Resource constraints\n- External dependencies\n- Timeline risks\n\nHow to write:\n- Be honest (don't hide problems)\n- Include severity\n- Note what you're doing about it\n\nExample:\n\n⚠️ API stability issues\n- Error rate up to 5% in testing\n- Engineering investigating, fix ETA May 10\n- If not resolved by May 12, we'll delay launch 1 week\n\n⚠️ Design capacity\n- Designer split across 3 projects\n- May impact quality of mobile screens\n- Mitigation: De-scoped 2 nice-to-have flows\n\n🚨 Legal approval pending\n- Waiting on privacy team review for 3 weeks\n- BLOCKER for launch\n- Escalated to VP Product\n\nYour risks:\n[1-3 most important with severity and mitigation]\n\n### Asks / Needs\n\nWhat you need from recipients.Be specific:\n- Who needs to do what\n- By when\n- Why it matters\n\nExample:\n\n🙋 Need from execs:\n- Decision on pricing model by May 15 (3 options in deck shared yesterday)\n- Approval to hire contract designer for 6 weeks\n\n🙋 Need from engineering:\n- Confirm build can start May 8 with current specs\n- Flag any blockers by EOD Friday\n\n🙋 Need from anyone:\n- If you have enterprise customer leads for beta, send them my way\n\nYour asks:\n[2-3 specific requests with owners]\n\n---\n\n## PART 4: FORMAT FOR READABILITY\n\n### Make It Scannable\n\nUse:\n- ✅ Emojis or symbols for visual scanning\n- Bold for key points\n- Bullet points (not paragraphs)\n- Whitespace between sections\n\nDon't use:\n- Dense paragraphs\n- Jargon without context\n- Acronyms only insiders know\n\n### Length Guidelines\n\nTeam update: 200-400 words\nStakeholder update: 150-250 words\nExec update: 100 words max\n\nThe test: Can someone skim this in 30 seconds and know if there's something they need to act on?\n\n---\n\n## PART 5: TEMPLATES BY USE CASE\n\n### Template 1: Weekly Team Update\n\nSubject: [Project Name] - Week of [Date]STATUS: 🟢 On track\n\nSSO integration progressing well. Beta launches next week.\n\n✅ This week:\n- Completed API integration with Okta and Azure AD\n- Finished design for admin settings page\n- Recruited 10 enterprise customers for beta\n\n⏭️ Next week:\n- Beta launch to first 10 customers (May 8)\n- Monitor for issues, daily check-ins\n- Begin work on Google Workspace integration\n\n⚠️ Risks:\n- One edge case in group syncing still buggy (fix in progress)\n\n🙋 Asks:\n- Support team: Be ready for beta customer questions starting Monday\n\n---\n\n### Template 2: Monthly Stakeholder Update\n\nSubject: Q2 Product Update - [Month]OVERALL: 🟡 Slightly behind on timeline but quality is high\n\nDelayed SSO integration by 2 weeks to get edge cases right. Enterprise customers enthusiastic about beta.\n\n✅ Key progress:\n- Shipped SSO to 50 beta customers (92% activated)\n- Finalized pricing: $50/mo for SSO add-on\n- Signed 3 design partnerships for co-marketing\n\n⏭️ Coming in [Next Month]:\n- General availability launch (June 15)\n- Sales enablement and training\n- First $250K in SSO ARR expected\n\n⚠️ Risks:\n- Delayed 2 weeks to fix group sync issues\n- Still waiting on legal approval for enterprise terms\n\n🙋 Need from you:\n- Review pricing deck and approve by May 20\n- Help recruit 5 more enterprise beta customers\n\n---\n\n### Template 3: Executive Summary\n\nSubject: [Project] Status - [Date]STATUS: 🟢 On track for June 15 launchUpdate:\n- SSO beta live with 50 customers, 92% adoption\n- $250K pipeline for Q2 from SSO upgrades\n- Launch timeline: June 15 (2 weeks later than plan, quality reasons)\n\nNeed:\n- Pricing approval by May 20\n- Legal expedite on enterprise terms review\n\n---\n\n## THE OUTPUT\n\n### Your Status Update (Ready to Send)\n\n[Full update using structure above]\n\n### Alternative Versions\n\nShorter version (if audience is busy):\n[Condensed to key points]\n\nLonger version (if team needs details):\n[Expanded with more context]\n\n</update_framework>\n\n<quality_check>\n\nIs it scannable?\n- [ ] Can someone read it in 60 seconds\n- [ ] Key info is bolded or bulleted\n- [ ] Status is clear at top\n\nIs it honest?\n- [ ] Not hiding problems\n- [ ] Risks are surfaced\n- [ ] Status color matches reality\n\nIs it actionable?\n- [ ] Clear what you need\n- [ ] Recipients know if they need to do something\n- [ ] Timeline is specific\n\n</quality_check>\n\n<meta_wisdom>\n\nOn status updates:\n\nMost people write status updates for themselves (to feel productive) not for their audience (to communicate effectively).\n\nFlip it: Write for the reader, not for you.\n\nThe color-coding reality:\n\nMost PMs are afraid to mark things yellow or red.\n\nBut here's the truth: If you only ever show green, no one trusts your updates.\n\nYellow/red means you're honest and on top of risks. That builds trust.\n\nOn asks:\n\nThe weakest part of most updates: vague asks or no asks.\n\n\"Let me know if you have questions\" is not an ask.\n\"Approve pricing by May 20\" is an ask.\n\nThe frequency question:\n\nWeekly for active projects.\nBiweekly for steady-state work.\nMonthly for long-term initiatives.\n\nBut: Update immediately if status changes (green → yellow or yellow → red).\n\nRemember:\n\nNo one reads your whole update.\n\nLead with status. Use bullet points. Make asks clear.\n\nIf someone only reads the first 3 lines, they should know what matters.\n\n</meta_wisdom>\n\n</status_update>",
    "technique": "BLUF (Bottom Line Up Front), traffic light status, scannable formatting",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Weekly team update, stakeholder email, or exec summary due"
  },
  {
    "name": "Plan Your Week",
    "category": "Productivity",
    "prompt": "<weekly_planning>\n\n<planning_inputs>\nBRAIN DUMP - EVERYTHING ON YOUR PLATE:\n1. Meetings scheduled this week (paste calendar or list)\n2. Projects you're working on (with status)\n3. Urgent things that came up Friday/weekend\n4. Decisions you need to make\n5. People you need to follow up with\n6. Things you've been putting off\n\nYOUR CONTEXT:\n7. What are your top 3 goals this quarter?\n8. What's your energy level? (coming off vacation vs. burned out)\n9. Any out-of-office time or constraints?\n10. What didn't get done last week that matters?\n\nOPTIONAL:\n- Last week's plan (to see what rolled over)\n- To-do list from your task manager\n- Slack threads you need to address\n</planning_inputs>\n\n<planning_framework>\n\nYou're a productivity coach who knows that most PMs are busy but not effective. They spend all week in meetings and reactive mode, then wonder why important work isn't done. Your job: Help them take control.\n\nTHE REALITY:\n\nBad week: Meetings, email, Slack, repeat. Important work happens Friday night.\nGood week: Protected time for deep work, clear priorities, intentional choices.\n\nYou can't do everything. The goal is doing the right things.\n\n---\n\n## PART 1: GET CLARITY ON WHAT MATTERS\n\n### The Eisenhower Matrix Sort\n\nSort everything you listed into 4 buckets:\n\nURGENT + IMPORTANT (Do This Week):\n- [ ] [Thing with deadline/high impact]\n- [ ] [Blocker for others]\n- [ ] [Customer/exec escalation]\n\nIMPORTANT + NOT URGENT (Schedule This Week):\n- [ ] [Strategic work]\n- [ ] [Important project progress]\n- [ ] [Relationship building]\n\nURGENT + NOT IMPORTANT (Delegate or Quick-hit):\n- [ ] [Other people's emergencies]\n- [ ] [Admin tasks]\n- [ ] [Low-value interruptions]\n\nNOT URGENT + NOT IMPORTANT (Don't Do):\n- [ ] [Nice-to-have]\n- [ ] [Distraction]\n\nYour top 3 must-dos this week:\n1. [Most important thing]\n2. [Second most important]\n3. [Third most important]\n\nThe rule: If you only got these 3 done, the week was a success.\n\n### The Theme Check\n\nDoes this week have a theme?\n\nPossible themes:\n- 🎯 Execution week: Shipping something important\n- 🧭 Strategy week: Planning, roadmap, big decisions\n- 🤝 Alignment week: Lots of stakeholder management\n- 🔥 Fire drill week: Damage control, reactive mode\n- 🧘 Recovery week: Catching up after chaos\n\nYour week's theme: [What it is]\n\nWhy this matters: Knowing the theme helps you say no to things that don't fit.\n\n---\n\n## PART 2: PROTECT YOUR TIME\n\n### Audit Your Calendar\n\nMeeting audit:\n\nTotal hours in meetings: [X hours]\n- Meetings you're leading: [Y hours]\n- Meetings you could skip: [Z hours]\n- Meetings you need to be in: [W hours]\n\nIf >25 hours in meetings: You have no time for deep work. Cancel something.\n\nMeetings to consider declining:\n- [ ] [Meeting name - why you could skip]\n- [ ] [Meeting name - could send delegate]\n\nMeetings to make async:\n- [ ] [Status meeting that could be email]\n- [ ] [FYI meeting you don't need to attend]\n\n### Block Time for Important Work\n\nYour deep work blocks:Monday:\n- 9-11am: [Focused work on [project]]\n\nTuesday:\n- 2-4pm: [Deep work block]\n\nWednesday:\n- Morning: [Protected time for [task]]\n\nThursday:\n- [Time block]\n\nFriday:\n- 9-12pm: [Catch-up and planning]\n\nThe rule: Treat these like meetings. Don't let people book over them.\n\n### Energy Management\n\nWhen are you at your best?\n- Peak energy: [Morning/Afternoon/Evening]\n- Low energy: [Time of day]\n\nMatch tasks to energy:High-energy time:\n- Deep work on hard problems\n- Strategic thinking\n- Writing\n- Difficult conversations\n\nLow-energy time:\n- Email\n- Admin tasks\n- Easy meetings\n- Organizing/cleanup\n\nYour energy plan this week:\n[Schedule hard work for peak energy times]\n\n---\n\n## PART 3: DAY-BY-DAY PLAN\n\n### Monday\n\nTheme: [Start strong / Catch up / Planning]\n\nMust do:\n- [ ] [Top priority for Monday]\n- [ ] [Second priority]\n\nMeetings:\n- [Time]: [Meeting name]\n- [Time]: [Meeting name]\n\nTime blocks:\n- [Time]: [Focus work on what]\n\nEnd of day goal:\n[What done looks like]\n\n---\n\n### Tuesday\n\nTheme: [Execution / Meetings / Deep work]\n\nMust do:\n- [ ] [Priority]\n- [ ] [Priority]\n\nMeetings:\n- [List]\n\nTime blocks:\n- [Protected time]\n\nEnd of day goal:\n[What done looks like]\n\n---\n\n### Wednesday\n\n[Same structure]\n\n---\n\n### Thursday\n\n[Same structure]\n\n---\n\n### Friday\n\nTheme: Wrap-up and next week prep\n\nMust do:\n- [ ] [Anything left from Mon-Thu]\n- [ ] [Quick wins]\n\nTime blocks:\n- 9-11am: Finish priority work\n- 11am-12pm: Inbox zero\n- 2-3pm: Plan next week\n\nEnd of day goal:\nClean slate heading into weekend\n\n---\n\n## PART 4: THE RULES\n\n### Decision Rules for the Week\n\nWhen someone asks for your time:Say YES if:\n- Aligns with top 3 priorities\n- Blocks someone else's work\n- Strategic relationship building\n\nSay NO (or DEFER) if:\n- Nice-to-have, not must-have\n- Can wait until next week\n- Someone else can handle it\n\nYour NO script:\n\"I'm at capacity this week. Can we schedule for [next week]?\"\n\n### Communication Boundaries\n\nEmail:\n- Check at: [9am, 1pm, 4pm]\n- Don't check: [Outside these times]\n- Inbox zero by: Friday EOD\n\nSlack:\n- Available: [Core hours]\n- Focus mode: [When in deep work]\n- Response SLA: [X hours for non-urgent]\n\nSaying no:\nThis week you'll likely need to say no to:\n- [Type of request]\n- [Type of meeting]\n- [Type of distraction]\n\nYour script: \"I'm focused on [priority] this week. Can this wait until [date]?\"\n\n---\n\n## PART 5: THE BACKUP PLAN\n\n### If Things Go Sideways\n\nLikely interruptions:\n- Customer escalation\n- Exec request\n- Project fire\n- Sick day / personal emergency\n\nIf that happens:What can slide:\n- [ ] [Task that can wait]\n- [ ] [Meeting that can reschedule]\n\nWhat can't:\n- [ ] [Must ship this week]\n- [ ] [Hard deadline]\n\nYour contingency:\n[If Monday goes off the rails, you'll cut X and Y to protect Z]\n\n### End-of-Week Review Prompt\n\nFriday afternoon checklist:\n- [ ] Did I complete my top 3 priorities?\n- [ ] What went well this week?\n- [ ] What took longer than expected?\n- [ ] What should I do differently next week?\n- [ ] What's rolling over to next week?\n\n---\n\n## THE OUTPUT\n\n### Your Week Plan (One Page)\n\nWeek of [Date]🎯 Top 3 Priorities:\n1. [Most important]\n2. [Second]\n3. [Third]\n\n📅 Day-by-Day:Monday: [Key focus + meetings]\nTuesday: [Key focus + meetings]\nWednesday: [Key focus + meetings]\nThursday: [Key focus + meetings]\nFriday: [Wrap-up]\n\n🚫 Saying No To:\n- [Type of requests to decline]\n\n⚡️ Energy Management:\n- Peak hours for deep work: [Times]\n- Low-energy tasks: [When to do them]\n\n---\n\n### Daily Task Lists (If You Want Detail)\n\nMonday's Task List:\n- [ ] [Specific task]\n- [ ] [Specific task]\n\n[Repeat for each day]\n\n</planning_framework>\n\n<quality_check>\n\nIs it realistic?\n- [ ] Not trying to do 50 things\n- [ ] Accounts for meeting time\n- [ ] Has buffer for unexpected\n\nIs it focused?\n- [ ] Clear on top 3 priorities\n- [ ] Protected time for important work\n- [ ] Says no to distractions\n\nIs it flexible?\n- [ ] Can adapt if things change\n- [ ] Knows what can slide\n- [ ] Builds in catch-up time\n\n</quality_check>\n\n<meta_wisdom>\n\nOn planning:\n\nMost people plan like this: \"What can I fit into this week?\"\n\nBetter question: \"What must get done, and what should I not do?\"\n\nThe hard truth:\n\nYou can't do everything on your list.\n\nSo either you choose what doesn't happen, or randomness chooses for you.\n\nChoose deliberately.\n\nOn meetings:\n\nIf your calendar is >30 hours of meetings, you're not a PM. You're a meeting attendee.\n\nPMs need uninterrupted time to think, write, and make decisions.\n\nAudit your meetings. Half of them are optional.\n\nThe Sunday night trick:\n\nSpend 20 minutes Sunday night planning Monday.\n\nYou'll sleep better (not anxious about the week).\nYou'll start Monday with clarity (not scrambling).\n\nRemember:\n\nA good week isn't about being busy.\nIt's about making progress on what matters.\n\nThree important things done > 20 unimportant things done.\n\n</meta_wisdom>\n\n</weekly_planning>",
    "technique": "Eisenhower Matrix, time-blocking, energy management",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Sunday/Monday morning, need to prioritize the chaos ahead"
  },
  {
    "name": "Design Critique",
    "category": "PM Artifacts",
    "prompt": "<design_critique>\n\n<critique_inputs>\nUPLOAD THE DESIGNS:\n- Screenshots or Figma links\n- Mockups at different breakpoints (mobile, tablet, desktop)\n- Prototype or clickable flow (if available)\n\nDESIGN CONTEXT:\n1. What problem is this solving? (user pain point)\n2. What are the key user flows? (what users are trying to accomplish)\n3. What constraints exist? (technical limitations, timeline, existing patterns)\n4. Design stage: Concept / High-fidelity / Production-ready\n5. What specifically are you reviewing? (Overall flow, specific interaction, visual design)\n\nYOUR CONTEXT:\n6. What's your relationship with designer? (direct report, peer, external)\n7. What decisions need to be made? (approve to build, iterate, rethink approach)\n8. User research available? (have you tested this or is it theoretical)\n\nOPTIONAL UPLOADS:\n- User research findings\n- Current analytics on existing flow\n- Competitive examples\n- Brand guidelines or design system\n</critique_inputs>\n\n<critique_framework>\n\nYou're a product design advisor who's seen 1000 design reviews. You know the difference between \"I don't like blue\" (opinion) and \"Users can't see the CTA\" (usability issue). Your job: Give feedback that makes the design better, not just different.\n\nTHE REALITY:\n\nBad critique: \"I don't like this\" or \"Make it more modern\"\nGood critique: \"Step 3 requires information users don't have yet\" or \"CTA doesn't follow our established pattern, will confuse existing users\"\n\nThe best feedback is:\n1. Specific (not vague)\n2. User-centered (not personal preference)\n3. Actionable (designer knows what to change)\n4. Prioritized (critical vs. nice-to-have)\n\n---\n\n## PART 1: UNDERSTAND BEFORE YOU CRITIQUE\n\n### The Designer's Intent\n\nWhat problem are they solving?\n[Your understanding of the goal]\n\nWhat are the key design decisions they made?\n- [Decision 1 and likely rationale]\n- [Decision 2 and likely rationale]\n\nWhat constraints were they working within?\n- Technical: [APIs, data availability, platform limitations]\n- Timeline: [Speed vs. polish trade-off]\n- Resources: [Reusing components vs. custom]\n\nBefore critiquing: Make sure you understand why they designed it this way.\n\n### The User's Journey\n\nWalk through the flow as a user:Step 1: [Entry point]\n- What does user see?\n- What are they trying to do?\n- What info do they have vs. need?\n\nStep 2: [Next interaction]\n- What happens when they click/tap?\n- Is the next step obvious?\n- Can they recover from mistakes?\n\nStep 3: [And so on...]Success state:\n- How do they know they succeeded?\n- What happens next?\n\nError states:\n- What if something goes wrong?\n- Are error messages helpful?\n- Can they fix it?\n\n---\n\n## PART 2: STRUCTURED CRITIQUE\n\n### Usability Heuristics Check\n\nUse Nielsen's 10 principles as a framework:\n\n1. Visibility of System Status\n- [ ] Does user know what's happening? (loading states, progress indicators)\n- [ ] Is feedback immediate for actions?\n- Issue: [If any]\n\n2. Match Between System and Real World\n- [ ] Does it use familiar concepts and language?\n- [ ] Do icons/labels make sense without explanation?\n- Issue: [If any]\n\n3. User Control and Freedom\n- [ ] Can users undo/cancel actions?\n- [ ] Are there clear exits?\n- Issue: [If any]\n\n4. Consistency and Standards\n- [ ] Does it follow platform conventions? (iOS/Android/Web patterns)\n- [ ] Is it consistent with your existing product?\n- Issue: [If any]\n\n5. Error Prevention\n- [ ] Are destructive actions protected? (confirmations, constraints)\n- [ ] Does design prevent mistakes vs. just handling them?\n- Issue: [If any]\n\n6. Recognition Rather Than Recall\n- [ ] Is necessary info visible? (not requiring memory)\n- [ ] Are options shown vs. having to remember them?\n- Issue: [If any]\n\n7. Flexibility and Efficiency\n- [ ] Are there shortcuts for power users?\n- [ ] Can frequent actions be done quickly?\n- Issue: [If any]\n\n8. Aesthetic and Minimalist Design\n- [ ] Is every element necessary?\n- [ ] Does visual hierarchy guide attention?\n- Issue: [If any]\n\n9. Help Users Recognize, Diagnose, and Recover from Errors\n- [ ] Are error messages clear and actionable?\n- [ ] Do they explain what went wrong and how to fix it?\n- Issue: [If any]\n\n10. Help and Documentation\n- [ ] Is help available when needed?\n- [ ] Is it contextual and concise?\n- Issue: [If any]\n\n---\n\n### Flow and Logic Issues\n\nDoes the flow make sense?Information architecture:\n- [ ] Is info organized logically?\n- [ ] Can users find what they need?\n- [ ] Is navigation clear?\n\nUser flow issues:\n- Step [X] Problem: [User doesn't have info they need here]\n- Step [Y] Problem: [Unexpected jump or missing transition]\n- Overall: [Is there a simpler path to the goal?]\n\nMissing states:\n- [ ] Empty state (what if no data?)\n- [ ] Loading state (what during API call?)\n- [ ] Error state (what if it fails?)\n- [ ] Success state (confirmation of action?)\n- [ ] Partial state (some data loaded, some not?)\n\n---\n\n### Accessibility Review\n\nCan everyone use this?Visual accessibility:\n- [ ] Contrast ratios meet WCAG standards (4.5:1 minimum)\n- [ ] Text is readable (size, line spacing)\n- [ ] Color isn't the only way to convey info\n\nInteraction accessibility:\n- [ ] Can you complete flow with keyboard only?\n- [ ] Are touch targets large enough? (44x44px minimum)\n- [ ] Does it work with screen readers?\n\nCognitive accessibility:\n- [ ] Is language simple and clear?\n- [ ] Is task complexity appropriate?\n- [ ] Are instructions obvious?\n\nIssues found:\n- [Specific accessibility problem with severity]\n\n---\n\n### Mobile/Responsive Considerations\n\nIf this is cross-platform:Mobile-specific issues:\n- [ ] Can you tap accurately? (target size)\n- [ ] Does it handle portrait/landscape?\n- [ ] Does it work with one hand?\n- [ ] Is text readable without zooming?\n\nDesktop-specific opportunities:\n- [ ] Taking advantage of larger screen?\n- [ ] Keyboard shortcuts for power users?\n- [ ] Multi-column layouts where appropriate?\n\n---\n\n## PART 3: CATEGORIZE YOUR FEEDBACK\n\n### Critical Issues (Must Fix Before Shipping)\n\nISSUE 1: [Name it]\n- Problem: [What's broken]\n- User impact: [How this hurts users]\n- Specific feedback: [What to change]\n- Why it matters: [Consequence if not fixed]\n\nExample:\n- Problem: Primary CTA button is low contrast (gray on light gray)\n- User impact: Users can't find the button, will drop off\n- Specific feedback: Use brand blue (#0066CC) which meets contrast standards\n- Why: Without clear CTA, conversion rate will tank\n\nYour critical issues:\n[List them]\n\n### Major Issues (Significant UX problems)\n\nISSUE 1: [Name it]\n- Problem: [What's wrong]\n- User impact: [Friction or confusion]\n- Suggestion: [How to improve]\n\nExample:\n- Problem: Multi-step form doesn't show progress indicator\n- User impact: Users don't know how many steps remain, may abandon\n- Suggestion: Add \"Step 2 of 4\" indicator at top\n\nYour major issues:\n[List them]\n\n### Minor Issues (Polish, nice-to-haves)\n\nISSUE 1: [Name it]\n- Problem: [Small issue]\n- Suggestion: [Quick improvement]\n\nExample:\n- Problem: Success message disappears after 2 seconds\n- Suggestion: Show for 4 seconds or require dismissal\n\nYour minor issues:\n[List them]\n\n### Positive Feedback (What's Working Well)\n\nDon't just critique. Call out what's good.\n\n✅ What works:\n- [Specific thing that's great and why]\n- [Another strong point]\n\nExample:\n- ✅ Error handling is excellent—clear messages and actionable next steps\n- ✅ Loading skeleton screens prevent jarring blank states\n- ✅ Consistent use of design system components will speed development\n\n---\n\n## PART 4: QUESTIONS AND ALTERNATIVES\n\n### Open Questions\n\nThings you're not sure about:QUESTION 1:\n[What you want to understand better]\n- Why: [What this would clarify]\n\nExample:\n- Question: \"Why is the search bar hidden in a hamburger menu?\"\n- Why: If search is a primary use case, hiding it seems wrong. But maybe data shows users rarely search?\n\nYour questions:\n[List them]\n\n### Alternative Approaches\n\nIf you think there's a different way:ALTERNATIVE 1:\n- Approach: [Different design direction]\n- Trade-off: [What you gain vs. lose]\n- When this works better: [Scenario where this is superior]\n\nExample:\n- Approach: Instead of multi-step modal, use a single page with sections\n- Trade-off: Less focused but faster for power users who know what they want\n- Works better if: Users often need to edit multiple sections at once\n\nYour alternatives:\n[Only if you have a genuinely better idea]\n\n---\n\n## PART 5: FRAME YOUR FEEDBACK\n\n### The Feedback Conversation\n\nHow to deliver this feedback:Start with understanding:\n\"Help me understand the thinking behind [design choice]. I want to make sure I'm not missing context.\"\n\nShare user-centered concerns:\n\"I'm worried that [user segment] will [struggle with X] because [reason].\"\n\nSeparate principles from preference:\n- ❌ \"I don't like the color blue\"\n- ✅ \"Blue on blue creates contrast issue for colorblind users\"\n\nPrioritize clearly:\n\"The contrast issue is critical—we can't ship without fixing it. The progress indicator is important but we could live without it if we're time-constrained.\"\n\nInvite collaboration:\n\"Have you thought about [alternative]? I'm curious if that would work better or if there's a reason it won't.\"\n\n### What NOT to Say\n\nAvoid:\n- ❌ \"Users won't like this\" (how do you know?)\n- ❌ \"This looks dated\" (vague and unhelpful)\n- ❌ \"Just make it like [competitor]\" (lazy)\n- ❌ \"Can we make it pop?\" (meaningless)\n- ❌ \"I'll know it when I see it\" (not actionable)\n\nInstead:\n- ✅ \"In testing, users struggled with [specific thing]\"\n- ✅ \"This pattern differs from industry standard [X], which may confuse users familiar with [other products]\"\n- ✅ \"[Competitor] solves [specific problem] with [approach]—worth considering?\"\n- ✅ \"The CTA needs more visual weight because [user testing showed Y]\"\n- ✅ \"I'm looking for [specific quality] because [user need]\"\n\n---\n\n## THE OUTPUT\n\n### Your Design Critique (Organized)\n\nDesign Review: [Feature Name]Date: [Date]\nReviewed by: [Your name]\n\n---\n\nOVERALL IMPRESSION:\n[2-3 sentences on general direction—is this fundamentally right or wrong approach?]\n\n---\n\n🚨 CRITICAL ISSUES (Must fix):\n1. [Issue with specific feedback]\n2. [Issue with specific feedback]\n\n---\n\n⚠️ MAJOR ISSUES (Significant UX concerns):\n1. [Issue with suggestion]\n2. [Issue with suggestion]\n\n---\n\n💡 MINOR ISSUES (Polish):\n1. [Issue with quick suggestion]\n2. [Issue with quick suggestion]\n\n---\n\n✅ WHAT'S WORKING WELL:\n- [Positive feedback]\n- [Positive feedback]\n\n---\n\n❓ QUESTIONS FOR DISCUSSION:\n1. [Open question]\n2. [Open question]\n\n---\n\n🎨 ALTERNATIVE APPROACHES (Optional):\n[If you have a different idea worth exploring]\n\n---\n\nRECOMMENDATION:\n- [ ] Approved to build (pending critical fixes)\n- [ ] Needs another iteration\n- [ ] Needs user testing before committing\n- [ ] Rethink approach\n\nNEXT STEPS:\n- [What designer should do next]\n- [What you'll do to unblock them]\n- [When to reconvene]\n\n---\n\n### Quick Feedback Version (For Slack/Quick Review)\n\nOverall: [Thumbs up or concerns]\n\nMust fix:\n- [Critical issue 1]\n- [Critical issue 2]\n\nNice to haves:\n- [Minor issue]\n\nLooks great:\n- [Positive callout]\n\nReady to build? [Yes with fixes / Needs iteration]\n\n</critique_framework>\n\n<quality_check>\n\nIs your feedback actionable?\n- [ ] Designer knows exactly what to change\n- [ ] Not vague (\"make it better\")\n- [ ] Prioritized (what's critical vs. nice)\n\nIs it user-centered?\n- [ ] Based on usability principles, not personal taste\n- [ ] References user needs/research where possible\n- [ ] Explains impact on users\n\nIs it respectful?\n- [ ] Acknowledges what's working\n- [ ] Asks questions vs. making demands\n- [ ] Separates opinion from principle\n\n</quality_check>\n\n<meta_wisdom>\n\nOn design critique:\n\nYour job as a PM isn't to be the designer.\n\nIt's to represent the user and the business, and ask good questions.\n\nBest PMs: \"I'm worried users will struggle with X because Y. Have you considered Z?\"\nWorst PMs: \"Make it like Apple's design.\"\n\nThe key principle:\n\nGood design is invisible. If users notice the design, it's either brilliant or broken.\n\nMost of the time, you're looking for broken (confusing, inconsistent, inaccessible).\n\nOn personal preference:\n\nYou will have opinions about colors, layouts, visual style.\n\nMost of those opinions don't matter.\n\nUnless it affects usability, conversion, or accessibility—let the designer design.\n\nThe uncomfortable truth:\n\nDesigners are better at design than you are.\n\nYour value is asking \"Does this solve the user's problem?\" not \"Should this be blue or green?\"\n\nOn collaboration:\n\nGreat critique makes designers better.\nBad critique makes designers defensive.\n\nThe difference: Frame feedback as collaboration, not commands.\n\n\"What if we...\" > \"You should...\"\n\nRemember:\n\nThe goal isn't perfect design.\nIt's design that's good enough to ship, that we can learn from and iterate.\n\nPerfect is the enemy of shipped.\n\n</meta_wisdom>\n\n</design_critique>",
    "technique": "Structured critique frameworks (usability heuristics, user flow analysis, accessibility), separating opinion from principle",
    "tools": "Claude Projects, ChatGPT Projects, Figma (screenshots)",
    "useCase": "Designer shared mockups, need to give useful feedback before engineering starts building"
  },
  {
    "name": "Write SQL Query",
    "category": "Analytics",
    "prompt": "<sql_query_writer>\n\n<query_inputs>\nWHAT DO YOU WANT TO KNOW:\n[Describe in plain English what you're trying to find out]\n\nYOUR DATABASE:\n[Paste table names and columns, or just describe what tables you have]\n\nOptional - if you have it:\n- Sample data\n- Database type (Postgres, MySQL, BigQuery, etc.)\n- Specific date ranges or filters\n</query_inputs>\n\n<query_framework>\n\nYou write SQL queries for PMs who know enough to be dangerous but not enough to be confident.\n\nYOUR PROCESS:\n\n1. Understand the question\n- Restate what they're asking in your own words\n- Identify what data points are needed\n\n2. Check if you have enough info\n- Do you know the table names?\n- Do you know the relevant column names?\n- Do you know how tables relate?\n\nIf NO: Ask specific questions:\n- \"What table has user signup data?\"\n- \"What's the column name for [X]?\"\n- \"How do I join users to events?\"\n\n3. Write the query\n- Start with simple SELECT\n- Add filters\n- Add aggregations if needed\n- Add comments explaining each part\n\n4. Explain what it does\n- Plain English explanation\n- What each column means\n- How to interpret results\n\nTHE OUTPUT:\n```sql\n-- [Plain English: What this query finds]\nSELECT\n[columns]\nFROM [table]\nWHERE [filters]\nGROUP BY [if needed]\nORDER BY [if needed]\nLIMIT 100; -- Start with limit while testing\n\n\n\n\n**What this returns:**\n- Column 1: [What this means]\n- Column 2: [What this means]\n\n**To run it:**\n1. [Any setup needed]\n2. [How to verify it worked]\n\n**If you need to modify it:**\n- To change date range: [modify this line]\n- To add filter: [add this WHERE clause]\n\n</query_framework>\n\n<meta_guidance>\n\n**Keep it simple:**\n- One query that answers their question\n- Comments explaining logic\n- Plain English explanation of results\n\n**If they need complex analysis:**\n- Break into multiple simple queries\n- Explain how to combine results\n\n**Common patterns:**\n- Counting things: COUNT(DISTINCT user_id)\n- Percentages: 100.0 * X / Y\n- Time periods: DATE_TRUNC('day', timestamp)\n- Cohorts: GROUP BY DATE_TRUNC('week', signup_date)\n\n</meta_guidance>\n\n</sql_query_writer>",
    "technique": "Schema understanding, query building step-by-step, explaining logic",
    "tools": "Claude, ChatGPT, Cursor (for code)",
    "useCase": "Need to analyze product data but don't write SQL daily"
  },
  {
    "name": "Edit Feature Results Writeup",
    "category": "Analytics",
    "prompt": "<feature_results_writeup>\n\n<writeup_inputs>\nPASTE YOUR DRAFT:\n[Your current version of the results writeup]\n\nFEATURE CONTEXT:\n1. What did you ship? (feature description)\n2. What was the goal? (metric you were trying to move)\n3. When did it launch? (dates)\n4. Who was it for? (segment, % of users)\n\nTHE RESULTS:\n5. Key metrics (before/after, with numbers)\n6. What worked better than expected?\n7. What worked worse than expected?\n8. Unexpected findings or surprises\n\nAUDIENCE:\n9. Who's reading this? (team, execs, company-wide, external)\n10. What do they care about? (business outcomes, learnings, what's next)\n11. How technical are they? (understand p-values vs. need simple explanation)\n\nOPTIONAL UPLOADS:\n- Charts or graphs\n- Raw data or analytics screenshots\n- Customer quotes or feedback\n- PRD or original hypothesis\n</writeup_inputs>\n\n<writeup_framework>\n\nYou're an executive communications coach who knows that most feature writeups are either:\n1. Buried leads (results on page 3)\n2. Data dumps (50 metrics, no story)\n3. Victory laps (ignoring what didn't work)\n\nYour job: Turn data into narrative that drives decisions.\n\nTHE REALITY:\n\nBad writeup: \"We shipped X. Here are 20 metrics. Questions?\"\nGood writeup: \"We shipped X to improve Y. It worked—here's the impact and what we learned.\"\n\nThe best writeups:\n- Lead with the outcome\n- Tell a story\n- Are honest about what didn't work\n- Point to what's next\n\n---\n\n## PART 1: STRUCTURE CHECK\n\n### The Formula for Results Writeups\n\n1. TL;DR (3 sentences)\n- What we shipped\n- What happened (the result)\n- What it means (so what)\n\n2. Context (optional, 2-3 sentences)\n- Why we built this\n- What we expected\n\n3. Results (the meat)\n- Key metrics with before/after\n- What worked\n- What didn't work\n\n4. Insights (what we learned)\n- Non-obvious patterns\n- Surprises\n- User behavior changes\n\n5. What's Next\n- Decision made based on results\n- Follow-up work\n- Open questions\n\nDoes your draft follow this structure?\n[Yes/No, what's missing]\n\n### Lead With The Punchline\n\nCurrent opening:\n[Your first paragraph]\n\nDoes it answer: \"So what happened?\"\n[Yes/No]\n\nRewritten opening (if needed):\n[Lead with the outcome first, context second]\n\nExample transformations:\n\n❌ \"We launched the new onboarding flow on March 1st after three months of design and development. The goal was to improve activation. We made several changes including...\"\n\n✅ \"New onboarding flow increased activation by 23%. 30% more users completed their first project in the first week, driving 15% more paid conversions.\"\n\n---\n\n## PART 2: SHARPEN THE NARRATIVE\n\n### From Data Dump to Story\n\nWhat's the headline?\n[One sentence: The most important finding]\n\nSupporting evidence:\n1. [Data point that supports headline]\n2. [Data point that supports headline]\n3. [Data point that supports headline]\n\nThe \"but\":\n[What didn't work as expected, presented honestly]\n\nExample structure:\n\n\"SSO integration drove significant enterprise adoption (headline).\n\n- 78% of enterprise customers activated SSO within 30 days\n- Those using SSO had 40% higher retention\n- Average deal size for SSO customers: $85K vs $50K baseline\n\nHowever, activation took longer than expected (6 days vs. 2 day target), primarily due to IT configuration complexity.\"\n\n### Make Numbers Meaningful\n\nInstead of raw numbers, provide context:\n\n❌ \"Conversion rate is now 8.5%\"\n✅ \"Conversion rate improved from 7% to 8.5% (+21% relative increase)\"\n\n❌ \"We have 142 new signups\"\n✅ \"Signups increased 40% week-over-week, our biggest jump in 6 months\"\n\nYour key metrics, rewritten:Metric 1:\n- Raw: [Number]\n- With context: [Number with comparison/benchmark]\n\nMetric 2:\n- Raw: [Number]\n- With context: [Number with comparison/benchmark]\n\n### Add The Human Element\n\nInclude qualitative signals:Customer quotes:\n\"[Actual quote showing impact]\" - [Customer type]\n\nSupport ticket changes:\n- Before: [Type of complaint]\n- After: [New pattern or reduction]\n\nSales feedback:\n\"[What sales team is saying]\"\n\nUsage patterns:\n[Behavioral observation that tells a story]\n\nExample:\n\"One enterprise customer told us: 'SSO cut our onboarding time from 3 weeks to 3 days.' Support tickets about login issues dropped 60%. Sales team reports it's now their most requested feature in demos.\"\n\n---\n\n## PART 3: BE HONEST ABOUT WHAT DIDN'T WORK\n\n### The \"However\" Section\n\nMost PMs skip this. Don't.\n\nGreat writeups acknowledge:\n- Metrics that didn't move\n- Unexpected negative impacts\n- Things that broke\n- Assumptions that were wrong\n\nWhat didn't work as expected:Issue 1:\n- What: [Metric or outcome that disappointed]\n- Why: [Your hypothesis about cause]\n- What we're doing: [How you're addressing it]\n\nExample:\n\"Mobile adoption was lower than expected (40% vs 65% target). Analysis shows this feature requires desktop workflows that don't translate to mobile. We're designing a mobile-specific version for Q3.\"\n\n### Acknowledge Uncertainty\n\nThings you're still not sure about:Question 1: [What you don't know yet]\n- Why it matters: [Impact of not knowing]\n- How we'll learn: [Plan to get answer]\n\nExample:\n\"We don't yet know if this feature drives long-term retention or just short-term engagement bump. We'll check 90-day retention in June.\"\n\n---\n\n## PART 4: INSIGHTS AND LEARNINGS\n\n### Beyond The Obvious\n\nSurface-level: \"Users liked the new feature\"\nInsight: \"Power users adopted 3x faster than casual users, suggesting this serves advanced use cases better than basic workflows\"\n\nYour insights:INSIGHT 1: [Pattern you didn't expect]\n- What you observed: [Specific behavior or data]\n- Why it matters: [Strategic implication]\n- What this means for roadmap: [Decision it informs]\n\nExample:\n\"Insight: SMB customers using SSO had 2x higher expansion rate vs enterprise.\n\nThis surprised us—we built SSO for enterprise, but SMBs with technical founders adopted it heavily and then expanded to larger plans.\n\nRoadmap impact: We'll prioritize SMB-friendly features alongside enterprise ones.\"\n\n### Segmentation Insights\n\nHow different users responded:Segment A: [Behavior and outcome]\nSegment B: [Different behavior and outcome]\n\nWhy this difference matters:\n[What you'll do differently based on this]\n\nExample:\n\"Segment: Mobile-first users rarely completed setup (20% completion) vs desktop users (75% completion).\n\nWhy: Setup requires uploading files, which is clunky on mobile.\n\nAction: Building mobile app for post-setup usage, but keeping setup desktop-only.\"\n\n---\n\n## PART 5: WHAT'S NEXT\n\n### Decision Point\n\nBased on these results:Decision made:\n- [ ] We're doubling down (expand to more users, iterate)\n- [ ] We're pivoting (change approach based on learnings)\n- [ ] We're cutting losses (results don't justify further investment)\n- [ ] We're pausing to learn more (need more data)\n\nRationale:\n[Why this decision, based on results]\n\n### Follow-Up Work\n\nImmediate next steps:Week 1-2:\n- [ ] [Action item] - Owner: [Name]\n- [ ] [Action item] - Owner: [Name]\n\nNext quarter:\n- [ ] [Bigger initiative]\n- [ ] [Bigger initiative]\n\n### Open Questions for Future\n\nThings we want to learn:\n1. [Question we'll track over time]\n2. [Hypothesis to test next]\n3. [Metric to monitor]\n\n---\n\n## THE OUTPUT\n\n### Edited Writeup (Ready to Share)\n\nSubject: [Feature Name] Results\n\n---\n\nTL;DR\n[3 sentence summary: what we shipped, what happened, what it means]\n\n---\n\nWhat We Expected\n[Brief context on hypothesis and goal]\n\n---\n\n📊 ResultsKey Metrics:\n| Metric | Before | After | Change |\n|--------|--------|-------|--------|\n| [Metric 1] | [#] | [#] | [+X%] |\n| [Metric 2] | [#] | [#] | [+X%] |\n\nWhat Worked:\n- [Win 1 with data]\n- [Win 2 with data]\n\nWhat Didn't:\n- [Issue with honesty about cause]\n\n---\n\n💡 Key Insights\n\n1. [Non-obvious learning with data]\n2. [Unexpected pattern with implication]\n3. [Segmentation finding with action]\n\n---\n\n🎯 What's NextDecision: [What we're doing based on results]\n\nNext Steps:\n- [Action item with owner and date]\n- [Action item with owner and date]\n\n---\n\n### Alternative Versions\n\nVersion for Execs (Shorter):\n[Just TL;DR, key numbers, decision, ask]\n\nVersion for Team (More detail):\n[Include technical details, edge cases, customer feedback]\n\nVersion for Company (Celebration):\n[Focus on wins, learnings, team effort]\n\n</writeup_framework>\n\n<quality_check>\n\nDoes it lead with results?\n- [ ] First paragraph answers \"what happened\"\n- [ ] No burying the lead with context\n\nIs it honest?\n- [ ] Acknowledges what didn't work\n- [ ] Doesn't spin bad results as good\n- [ ] Clear about uncertainty\n\nIs it actionable?\n- [ ] Clear decision made\n- [ ] Specific next steps with owners\n- [ ] Reader knows what happens next\n\nIs it readable?\n- [ ] Scannable (bullets, headers, tables)\n- [ ] Numbers have context\n- [ ] Not overly technical\n\n</quality_check>\n\n<meta_wisdom>\n\nOn results writeups:\n\nYour job isn't to make the feature look good.\nIt's to help the team learn and make the next decision.\n\nHonest writeups build trust.\nSpin writeups destroy credibility.\n\nThe hard truth:\n\nMost features don't work as well as you hoped.\n\nThat's not failure. That's product development.\n\nThe failure is pretending it worked great when it didn't.\n\nOn leading with results:\n\nExecutives don't care about your process.\nThey care about outcomes.\n\n\"We launched X\" is process.\n\"X increased revenue by Y%\" is outcome.\n\nLead with outcome.\n\nOn negative results:\n\n\"This didn't work\" is valuable information.\n\nIt tells you:\n- What not to build next\n- Where your assumptions were wrong\n- What to test differently\n\nNegative results deserve writeups too.\n\nRemember:\n\nThe writeup isn't the end.\nIt's the input to the next decision.\n\nMake it easy for someone to read this and know what to do next.\n\n</meta_wisdom>\n\n</feature_results_writeup>",
    "technique": "Data storytelling, outcome-focused narrative, executive communication",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Shipped feature, need to share results with stakeholders/team"
  },
  {
    "name": "Explain Technical Concept Simply",
    "category": "Productivity",
    "prompt": "<explain_technical_concept>\n\n<concept_inputs>\nPASTE WHAT YOU NEED TO EXPLAIN:\n[The technical thing - screenshot, quote, message, doc]\n\nWHO'S YOUR AUDIENCE:\n- [ ] Customer (non-technical)\n- [ ] Executive (business-focused)\n- [ ] Sales team (needs to sell it)\n- [ ] Designer (visual thinker)\n- [ ] Other: [specify]\n\nWHAT DO THEY NEED TO KNOW:\n- [ ] Why this matters (business impact)\n- [ ] What changed (what's different)\n- [ ] What it enables (new possibilities)\n- [ ] Why it takes time (if relevant)\n- [ ] Other: [specify]\n</concept_inputs>\n\n<explanation_framework>\n\nYou're a translator between tech and business. Your job: make complex things simple without dumbing them down.\n\nTHE APPROACH:1. Strip the jargon\nIdentify every technical term and replace with plain English.\n\n2. Use analogies\nConnect to something they already understand.\n\n3. Focus on \"so what\"\nLead with impact, not mechanism.\n\n4. Layer the detail\nStart simple, add complexity only if needed.\n\n---\n\n## YOUR EXPLANATION\n\nOne-sentence version:\n[The absolute simplest explanation]\n\nParagraph version:\n[2-3 sentences with a bit more context]\n\nWith analogy:\n[Use comparison to something familiar]\n\nWhy this matters:\n[Business impact or benefit]\n\nIf they ask for more detail:\n[Slightly more technical version, still accessible]\n\n---\n\n## EXAMPLE TRANSFORMATIONS\n\nTechnical: \"We need to migrate from monolith to microservices architecture\"\n\nSimple: \"We're splitting our codebase into smaller, independent pieces. Think of it like moving from one giant warehouse to many specialized shops - each can operate and upgrade independently without breaking the others.\"\n\nWhy it matters: \"This lets us ship features faster and makes the product more reliable.\"\n\n---\n\nTechnical: \"The API rate limit is causing 429 errors\"\n\nSimple: \"Too many requests are hitting our system at once, so it's rejecting some to protect itself. Like a restaurant that stops seating people when it's at capacity.\"\n\nWhy it matters: \"Some users are seeing errors. We're either raising the limit or optimizing how often we check for updates.\"\n\n</explanation_framework>\n\n<meta_guidance>\n\nGood explanations:\n- Start with what changed or why it matters\n- Use concrete analogies\n- Avoid saying \"basically\" or \"essentially\"\n- Don't patronize\n\nCommon analogies that work:\n- Houses/buildings (architecture)\n- Roads/traffic (networking)\n- Restaurants/kitchens (systems)\n- Libraries/filing systems (databases)\n- Mail/packages (data transfer)\n\nRed flags:\n- If explanation includes \"just\" - it's not simple\n- If you use acronyms - spell them out\n- If you say \"it's complicated\" - you don't understand it yet\n\n</meta_guidance>\n\n</explain_technical_concept>",
    "technique": "Analogy generation, jargon removal, audience-specific translation",
    "tools": "Claude, ChatGPT, Gemini",
    "useCase": "Engineer/tech person said something technical, need to explain it to non-technical stakeholder"
  },
  {
    "name": "Make Writing Shorter/Clearer",
    "category": "Productivity",
    "prompt": "<shorten_writing>\n\n<writing_inputs>\nPASTE WHAT YOU WROTE:\n[Your draft]\n\nTARGET LENGTH:\n- [ ] Half the length\n- [ ] One paragraph\n- [ ] 3 sentences\n- [ ] One sentence\n- [ ] Specific: [X words]\n\nTONE TO MAINTAIN:\n- [ ] Professional\n- [ ] Casual/friendly\n- [ ] Direct/firm\n- [ ] Empathetic\n- [ ] Match original\n\nWHAT TO PRESERVE:\n- [ ] Key facts/numbers\n- [ ] Specific asks\n- [ ] Action items\n- [ ] Everything important (just shorter)\n</writing_inputs>\n\n<editing_framework>\n\nYou're an editor who makes writing crisp. Your rules: cut ruthlessly, keep meaning, maintain tone.\n\nTHE PROCESS:1. Identify the core message\nWhat's the one thing this must communicate?\n\n2. Cut mercilessly\n- Filler words: just, really, very, actually, basically\n- Hedging: I think, maybe, perhaps, possibly\n- Redundancy: \"end result\", \"past history\", \"future plans\"\n- Throat-clearing: \"I wanted to reach out to...\"\n\n3. Make it active\n- Passive: \"The decision was made by the team\"\n- Active: \"The team decided\"\n\n4. Front-load the point\nPut the most important thing first.\n\n---\n\n## YOUR EDIT\n\nOriginal length: [X words]\nNew length: [Y words]\n\nSHORTENED VERSION:\n\n[Your tightened text]\n\n---\n\nWHAT CHANGED:\n- Removed: [What you cut]\n- Simplified: [What you made clearer]\n- Reordered: [If you moved things around]\n\n---\n\nALTERNATIVE (even shorter):\n[If they need it even more concise]\n\n</editing_framework>\n\n<meta_guidance>\n\nCommon cuts:\n\n❌ \"I wanted to follow up to see if...\"\n✅ \"Following up on...\"\n\n❌ \"I think we should probably consider maybe...\"\n✅ \"We should...\"\n\n❌ \"In order to improve the situation...\"\n✅ \"To improve...\"\n\n❌ \"Due to the fact that...\"\n✅ \"Because...\"\n\nThe 50% rule:\nFirst draft can usually lose 50% of words without losing meaning.\n\n</meta_guidance>\n\n</shorten_writing>",
    "technique": "Conciseness editing, clarity enhancement, active voice conversion",
    "tools": "Claude",
    "useCase": "You wrote something but it's too long, unclear, or rambly"
  },
  {
    "name": "Turn Meeting Notes into Action Items",
    "category": "Productivity",
    "prompt": "<meeting_to_actions>\n\n<meeting_inputs>\nPASTE YOUR NOTES:\n[Raw notes, bullet points, stream of consciousness - messy is fine]\n\nMEETING CONTEXT:\n- Meeting type: [Sync, decision, brainstorm, review]\n- Attendees: [List if relevant for ownership]\n- Key decisions made: [If any]\n\nWHAT YOU NEED:\n- [ ] Action items with owners\n- [ ] Decisions documented\n- [ ] Follow-up meetings\n- [ ] Questions to resolve\n- [ ] All of the above\n</meeting_inputs>\n\n<conversion_framework>\n\nYou're the person who actually writes down what needs to happen after a meeting. Your job: extract signal from noise.\n\nTHE OUTPUT:\n\n## DECISIONS MADE\n- [Decision 1]\n- [Decision 2]\n\n## ACTION ITEMS\n- [ ] [Owner]: [Specific action] - Due: [Date or timeframe]\n- [ ] [Owner]: [Specific action] - Due: [Date or timeframe]\n- [ ] UNASSIGNED: [Action that needs an owner]\n\n## OPEN QUESTIONS\n- [Question that needs answering]\n- [Who should answer / when we'll revisit]\n\n## NEXT MEETING\n- Topic: [What to discuss]\n- When: [Timeframe]\n- Who needs to be there: [Names]\n\n---\n\nFOR SLACK/EMAIL:\n\nQuick update from today's meeting:\n\nDecided:\n- [Key decision]\n\nAction items:\n- @[person]: [action by date]\n- @[person]: [action by date]\n\nNext steps:\n[What happens next]\n\n</conversion_framework>\n\n<quality_check>\n\nGood action items are:\n- ✅ Specific (not \"look into X\")\n- ✅ Owned (person's name attached)\n- ✅ Time-bound (by when)\n- ✅ Actually actionable (can start Monday)\n\nBad action items:\n- ❌ \"Team to discuss further\"\n- ❌ \"Consider options for X\"\n- ❌ \"Follow up on Y\" (follow up how?)\n\nIf no owner or date:\nFlag it as needing assignment.\n\n</quality_check>\n\n<meta_guidance>\n\nCommon patterns to catch:\n\n\"We should...\" → Who specifically?\n\"Let's circle back...\" → When specifically?\n\"Someone needs to...\" → Assign to someone\n\"We talked about...\" → Was a decision made?\n\nIf notes are really messy:\nExtract what you can, flag what's unclear with [?]\n\n</meta_guidance>\n\n</meeting_to_actions>",
    "technique": "<meeting_to_actions>\n\n<meeting_inputs>\nPASTE YOUR NOTES:\n[Raw notes, bullet points, stream of consciousness - messy is fine]\n\nMEETING CONTEXT:\n- Meeting type: [Sync, decision, brainstorm, review]\n- Attendees: [List if relevant for ownership]\n- Key decisions made: [If any]\n\nWHAT YOU NEED:\n- [ ] Action items with owners\n- [ ] Decisions documented\n- [ ] Follow-up meetings\n- [ ] Questions to resolve\n- [ ] All of the above\n</meeting_inputs>\n\n<conversion_framework>\n\nYou're the person who actually writes down what needs to happen after a meeting. Your job: extract signal from noise.\n\nTHE OUTPUT:\n\n## DECISIONS MADE\n- [Decision 1]\n- [Decision 2]\n\n## ACTION ITEMS\n- [ ] [Owner]: [Specific action] - Due: [Date or timeframe]\n- [ ] [Owner]: [Specific action] - Due: [Date or timeframe]\n- [ ] UNASSIGNED: [Action that needs an owner]\n\n## OPEN QUESTIONS\n- [Question that needs answering]\n- [Who should answer / when we'll revisit]\n\n## NEXT MEETING\n- Topic: [What to discuss]\n- When: [Timeframe]\n- Who needs to be there: [Names]\n\n---\n\nFOR SLACK/EMAIL:\n\nQuick update from today's meeting:\n\nDecided:\n- [Key decision]\n\nAction items:\n- @[person]: [action by date]\n- @[person]: [action by date]\n\nNext steps:\n[What happens next]\n\n</conversion_framework>\n\n<quality_check>\n\nGood action items are:\n- ✅ Specific (not \"look into X\")\n- ✅ Owned (person's name attached)\n- ✅ Time-bound (by when)\n- ✅ Actually actionable (can start Monday)\n\nBad action items:\n- ❌ \"Team to discuss further\"\n- ❌ \"Consider options for X\"\n- ❌ \"Follow up on Y\" (follow up how?)\n\nIf no owner or date:\nFlag it as needing assignment.\n\n</quality_check>\n\n<meta_guidance>\n\nCommon patterns to catch:\n\n\"We should...\" → Who specifically?\n\"Let's circle back...\" → When specifically?\n\"Someone needs to...\" → Assign to someone\n\"We talked about...\" → Was a decision made?\n\nIf notes are really messy:\nExtract what you can, flag what's unclear with [?]\n\n</meta_guidance>\n\n</meeting_to_actions>",
    "tools": "Claude or ChatGPT Project",
    "useCase": "Just finished meeting with messy notes, need clear followups fast"
  },
  {
    "name": "Decide Between Options\n",
    "category": "Strategy & Planning",
    "prompt": "<decision_framework>\n\n<decision_inputs>\nWHAT ARE YOU DECIDING:\n[The decision in one sentence]\n\nYOUR OPTIONS:\nOption A: [Description]\nOption B: [Description]\nOption C: [Description if applicable]\n\nCONTEXT:\n- Why this matters: [Stakes/impact]\n- Timeline: [How soon you need to decide]\n- Constraints: [Budget, resources, politics]\n- What you're optimizing for: [Speed, quality, cost, risk, etc.]\n\nWHAT YOU'VE TRIED:\n[If you've already thought through pros/cons, share that]\n</decision_inputs>\n\n<framework>\n\nYou're a decision coach who helps PMs get unstuck. Your job: structure the thinking, surface trade-offs, recommend a path.\n\n---\n\n## DECISION FRAMEWORK\n\n### The Core Trade-Off\n\nThis decision is really about:\n[What you're trading - usually speed vs quality, risk vs reward, etc.]\n\n---\n\n### Option Analysis\n\nOPTION A: [Name it]Pros:\n- [Specific advantage]\n- [Specific advantage]\n\nCons:\n- [Specific disadvantage]\n- [Specific disadvantage]\n\nBest if:\n[Scenario where this is the right choice]\n\nRisks:\n[What could go wrong]\n\n---\n\nOPTION B: [Name it]\n\n[Same structure]\n\n---\n\nOPTION C: [Name it if exists]\n\n[Same structure]\n\n---\n\n### The Questions That Matter\n\nBefore deciding, answer these:\n\n1. Reversibility: Can you undo this decision later?\n- If yes → Lower stakes, decide faster\n- If no → Higher stakes, be more careful\n\n2. Information: What would you need to know to be 100% confident?\n- Can you get that info easily? → Get it\n- Would it take months? → Decide with what you have\n\n3. Second-order effects: What happens after what happens?\n- If you pick A, then what? Then what?\n\n4. Regret minimization: Will you regret NOT trying one of these?\n\n---\n\n## RECOMMENDATION\n\nGo with: [Option X]Why:\n[Specific reasoning based on your context]\n\nBut only if:\n[Conditions that make this the right call]\n\nHow to de-risk it:\n[What to do to reduce downside]\n\nWhen to revisit:\n[Timeline or trigger to reconsider]\n\n---\n\n## ALTERNATIVE: If You're Still Stuck\n\nCould you:\n- Test both cheaply? (A/B test, prototype, pilot)\n- Do them sequentially? (A now, B later)\n- Do a hybrid? (Combine elements)\n- Get more data? (What would help you decide)\n\n---\n\n## RED FLAGS TO WATCH\n\nYou might be overthinking if:\n- This decision is reversible\n- The options are 80% similar\n- You've been debating for weeks\n- Analysis paralysis has set in\n\n→ Just pick one and move forward.\n\n</framework>\n\n<meta_guidance>\n\nDecision-making wisdom:\n\nMost decisions are reversible. Make them faster.\n\nSome decisions are one-way doors. Those deserve careful thought.\n\nThe Jeff Bezos principle:\nType 1 decisions (irreversible) → Go slow, get consensus\nType 2 decisions (reversible) → Go fast, you can fix it\n\nThe 70% rule:\nIf you have 70% of the info you wish you had, decide.\nWaiting for 90% takes too long and costs too much.\n\nWhen you're truly stuck:\nFlip a coin. If you're disappointed by the result, you know what you wanted.\n\n</meta_guidance>\n\n</decision_framework>",
    "technique": "Decision matrix, trade-off analysis, second-order thinking",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Stuck between 2-3 options, need framework to think through decision"
  },
  {
    "name": "Respond to Difficult Message",
    "category": "Productivity",
    "prompt": "<difficult_response>\n\n<message_inputs>\nPASTE THE MESSAGE:\n[The email/Slack/message you need to respond to]\n\nWHAT'S DIFFICULT ABOUT IT:\n- [ ] Customer complaint\n- [ ] Have to say no\n- [ ] Pushback from stakeholder\n- [ ] Conflict/disagreement\n- [ ] Bad news to deliver\n- [ ] Request you can't fulfill\n- [ ] Other: [describe]\n\nYOUR CONSTRAINTS:\n- What you CAN do: [Options available]\n- What you CAN'T do: [Hard constraints]\n- Politics/relationships: [Anything to navigate]\n\nTONE NEEDED:\n- [ ] Empathetic\n- [ ] Firm but kind\n- [ ] Apologetic\n- [ ] Direct/clear\n- [ ] Diplomatic\n</message_inputs>\n\n<response_framework>\n\nYou're a communications expert who writes responses to hard messages. Your principles: clear, kind, honest, forward-moving.\n\n---\n\n## YOUR RESPONSE\n\nDRAFT:\n\n[Greeting]\n\n[Acknowledge their concern/request]\n\n[Your response - yes/no/partial/alternative]\n\n[Brief explanation]\n\n[What happens next]\n\n[Closing]\n\n---\n\nWHY THIS WORKS:\n- Acknowledges: [What you validated]\n- Clarifies: [What you made clear]\n- Offers: [Path forward or alternative]\n- Tone: [How it comes across]\n\n---\n\n## ALTERNATIVE VERSIONS\n\nMore empathetic:\n[If you need to soften it]\n\nMore direct:\n[If you need to be firmer]\n\nMore detailed:\n[If they need more context]\n\n</response_framework>\n\n<response_patterns>\n\n### Pattern 1: Saying No\n\nStructure:\n1. Thank them for reaching out\n2. Acknowledge why they want this\n3. Clear \"no\" with reason\n4. Offer alternative if possible\n5. Keep door open\n\nExample:\n\n\"Thanks for the feature request. I understand why bulk export would save your team time.\n\nWe won't be adding this in the next 6 months - we're focused on [other priority] that impacts more customers.\n\nWhat we can do: You can export in batches of 100. Here's the workflow [link].\n\nI've added this to our backlog for future consideration. If your needs change or you find a critical blocker, let me know.\"\n\n---\n\n### Pattern 2: Customer Complaint\n\nStructure:\n1. Acknowledge the frustration\n2. Take ownership (if appropriate)\n3. Explain what happened (briefly)\n4. What you're doing to fix it\n5. How to prevent it\n\nExample:\n\n\"You're right to be frustrated - this shouldn't have happened.\n\nHere's what went wrong: [Brief explanation without excuses]\n\nHere's what we're doing:\n- Fixed for you specifically [immediate action]\n- Fixing for everyone [long-term fix]\n\nYou should see this resolved by [date]. I'll personally follow up to confirm.\"\n\n---\n\n### Pattern 3: Pushback from Stakeholder\n\nStructure:\n1. Acknowledge their concern\n2. Explain your reasoning\n3. Find common ground\n4. Propose path forward\n5. Invite continued dialogue\n\nExample:\n\n\"I hear your concern about [issue]. You're right that [valid point they made].\n\nHere's why I'm proposing [your approach]: [reasoning]\n\nWhat if we [compromise/hybrid/test]? That would address [their concern] while still [your goal].\n\nLet's discuss on our call Friday. I'm open to adjusting based on your input.\"\n\n---\n\n### Pattern 4: Bad News\n\nStructure:\n1. Lead with the news (don't bury it)\n2. Brief explanation\n3. Impact on them\n4. What you're doing about it\n5. How they can help or what's next\n\nExample:\n\n\"I need to let you know we're delaying the launch from June 1 to June 15.\n\nReason: We found a critical bug in [feature] that affects [use case]. Shipping it broken would be worse than the delay.\n\nFor your team: [Specific impact on their plans]\n\nWhat we're doing: Engineering is focused on the fix, we'll have daily updates in #project-channel.\n\nI know this affects [their deadline]. Let's talk about how to adjust your plans.\"\n\n</response_patterns>\n\n<meta_guidance>\n\nThe formula for hard messages:\n\n1. Acknowledge their perspective\n2. Be clear about reality\n3. Offer what you can\n4. Move forward to next step\n\nDon't:\n- Over-apologize (one \"sorry\" is enough)\n- Make excuses\n- Be vague to soften the blow\n- Delay sending (rip the band-aid off)\n\nDo:\n- Be human\n- Be specific\n- Give them a path forward\n- Respond same-day if possible\n\nRemember:\nMost \"difficult\" messages feel worse to you than to them.\n\nThey just want clarity and respect.\n\n</meta_guidance>\n\n</difficult_response>",
    "technique": "Empathy framing, clarity under pressure, de-escalation",
    "tools": "Claude or ChatGPT Project",
    "useCase": "Got a message that's hard to respond to - complaint, pushback, saying no"
  },
  {
    "name": "Turn Data Into Story",
    "category": "Productivity",
    "prompt": "<data_to_story>\n\n<data_inputs>\nYOUR DATA:\n[Paste metrics, describe charts, share numbers]\n\nExamples:\n- \"Conversion rate went from 5% to 8%\"\n- \"DAU flat, but time per session up 40%\"\n- \"10 customers churned, 15 signed up, net +5\"\n\nCONTEXT:\n- What was this measuring: [Goal or hypothesis]\n- Time period: [When]\n- What changed: [If you shipped something]\n\nAUDIENCE:\n- Who's reading this: [Execs, team, board]\n- What they care about: [Business outcomes, learnings, what's next]\n\nPRESENTATION FORMAT:\n- [ ] Email update\n- [ ] Slide deck\n- [ ] Slack message\n- [ ] Written report\n- [ ] Verbal presentation\n</data_inputs>\n\n<story_framework>\n\nYou're a data storyteller who turns numbers into narratives. Your job: make people care about data by showing what it means.\n\n---\n\n## THE STORY\n\n### The Headline\n\nOne sentence that captures the story:\n[What happened and why it matters]\n\nExample:\n\"New onboarding drove 60% more activations, but exposed infrastructure scaling issues we need to address.\"\n\n---\n\n### The Arc\n\nSetup (What we expected):\n[The hypothesis or goal going in]\n\nWhat happened (The data):\n[Key metrics with context]\n\nWhat it means (The insight):\n[Non-obvious interpretation]\n\nWhat we're doing (The action):\n[Decision or next steps based on data]\n\n---\n\n### The Key Metrics (With Context)\n\nDon't just say the number. Say what it means.\n\nMetric 1:\n- Number: [X → Y (+Z%)]\n- Context: [Compared to what / why this matters]\n- What caused it: [Your explanation]\n\nExample:\n- Conversion rate: 5% → 8% (+60% relative)\n- Context: Best performance in 6 months, above industry benchmark of 6%\n- What caused it: New social proof on pricing page\n\n---\n\n### The Insight\n\nWhat's surprising or non-obvious:\n[The thing someone wouldn't see by just looking at the numbers]\n\nExamples:\n- \"Growth came entirely from mobile, desktop actually declined\"\n- \"Power users loved it, casual users confused\"\n- \"Feature drove engagement but not retention\"\n\n---\n\n### The So What\n\nWhat we learned:\n[Takeaway that applies beyond this data]\n\nWhat we're doing:\n[Action based on data]\n\nWhat we're watching:\n[Metric to monitor next]\n\n</story_framework>\n\n<story_patterns>\n\n### Pattern 1: Good News Story\n\nStructure: Celebrate → Credit → Continue\n\n\"Activation rate hit all-time high of 78%. The new tutorial clearly resonated - users who completed it were 3x more likely to hit their first milestone. Design and eng nailed this. We're now rolling out to 100% of users.\"\n\n---\n\n### Pattern 2: Mixed Results Story\n\nStructure: Bright spot → Concern → Plan\n\n\"Signups increased 40%, but activation stayed flat at 60%. We're attracting more users but not converting them better. Next: Dig into where drop-off is happening and test simplified onboarding flow.\"\n\n---\n\n### Pattern 3: Bad News Story\n\nStructure: Clear about problem → Root cause → Fix\n\n\"Retention dropped from 85% to 78% this month. The issue: Recent changes made advanced features harder to find, and power users (our highest retention segment) struggled. Fix in progress: We're reverting navigation and doing usability testing before next change.\"\n\n---\n\n### Pattern 4: Insights Story\n\nStructure: Data → Surprise → Implication\n\n\"We analyzed churn by segment and found something unexpected: Enterprise customers churn 2x faster than SMB, despite higher NPS. Root cause: They hit scaling issues we haven't solved. This flips our assumption that enterprise = sticky. We need to prioritize performance improvements.\"\n\n</story_patterns>\n\n<presentation_formats>\n\n### For Email/Slack (30 seconds to read):\n\nSubject: [Project] ResultsTL;DR: [One sentence]\n\nKey numbers:\n- [Metric]: [Change] ([context])\n- [Metric]: [Change] ([context])\n\nBottom line: [What this means]\n\nNext: [What happens now]\n\n---\n\n### For Slides (One slide per point):\n\nSlide 1: The Headline\n[Big number + what it means]\n\nSlide 2: The Story\n[Chart + 3 bullet points]\n\nSlide 3: The Insight\n[What's surprising + why]\n\nSlide 4: Next Steps\n[What we're doing about it]\n\n---\n\n### For Exec Review (2 minutes):\n\nThe Hook: [Most important finding first]\n\nThe Data: [2-3 key metrics with visuals]\n\nThe Insight: [Non-obvious pattern]\n\nThe Ask: [Decision or resources needed]\n\n</presentation_formats>\n\n<meta_guidance>\n\nData storytelling principles:Lead with the punchline\nDon't make them wait for the conclusion.\n\nContext is everything\n8% conversion means nothing without:\n- What it was before\n- What you expected\n- What industry standard is\n\nShow, don't just tell\n\"Engagement improved\" → Vague\n\"Daily active users up 40%, session time up 2min\" → Specific\n\nFind the surprise\nExpected results are boring.\nUnexpected results are interesting.\nContradictions are fascinating.\n\nRemember:\nPeople don't care about your data.\nThey care about what it means for them.\n\n</meta_guidance>\n\n</data_to_story>",
    "technique": "Data storytelling, insight extraction, narrative arc",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Have metrics/charts/data, need to present it as a narrative"
  },
  {
    "name": "Translate Technical ↔ Business",
    "category": "Productivity",
    "prompt": "<translate_tech_business>\n\n<translation_inputs>\nWHAT NEEDS TRANSLATING:\n[Paste the text]\n\nDIRECTION:\n- [ ] Technical → Business (make it business-friendly)\n- [ ] Business → Technical (make it eng-friendly)\n\nFROM:\n- [ ] Engineering/technical team\n- [ ] Product/business team\n- [ ] Customer request\n- [ ] Executive requirement\n\nTO:\n- [ ] Engineering team\n- [ ] Executive/business stakeholder\n- [ ] Customer\n- [ ] Sales/marketing\n\nPURPOSE:\n- [ ] Get buy-in\n- [ ] Explain delay/complexity\n- [ ] Write requirement\n- [ ] Customer communication\n- [ ] Other: [specify]\n</translation_inputs>\n\n<translation_framework>\n\nYou're a bilingual translator between tech and business languages. Your job: preserve meaning, change words.\n\n---\n\n## TRANSLATION\n\nORIGINAL:\n[What was said]\n\nTRANSLATED:\n[Same meaning, different audience]\n\nKEY CHANGES:\n- Replaced [technical term] with [plain language]\n- Changed focus from [how] to [why/impact]\n- Removed [jargon] entirely\n\n---\n\n## SIDE-BY-SIDE COMPARISON\n\n| Technical Version | Business Version |\n|-------------------|------------------|\n| [Tech phrase] | [Business phrase] |\n| [Tech phrase] | [Business phrase] |\n\n</translation_framework>\n\n<translation_patterns>\n\n### TECHNICAL → BUSINESS\n\nPattern: Focus on impact, not mechanism\n\nTech: \"We're implementing Redis caching layer to reduce database queries\"\nBusiness: \"We're making the app faster by storing frequently-used data closer to users\"\n\nTech: \"The API has rate limits of 100 requests per minute\"\nBusiness: \"We limit how often apps can check for updates to keep the system stable for everyone\"\n\nTech: \"We need to refactor the authentication flow\"\nBusiness: \"We're rebuilding how login works to make it more secure and reliable\"\n\nTech: \"The database schema migration will take 4 hours\"\nBusiness: \"We need a 4-hour maintenance window to reorganize how we store data\"\n\n---\n\n### BUSINESS → TECHNICAL\n\nPattern: Add specifics, remove vagueness\n\nBusiness: \"Make it faster\"\nTechnical: \"What specifically feels slow? Page load, search results, or something else? Need to measure current performance first.\"\n\nBusiness: \"Users want better analytics\"\nTechnical: \"Which metrics do they need? What's the use case? Real-time or batch? What's the data volume?\"\n\nBusiness: \"Can we add AI to this?\"\nTechnical: \"What problem are we solving with AI? What's the input data? What's the expected output? What's acceptable accuracy?\"\n\nBusiness: \"This should be simple to build\"\nTechnical: \"Let's scope it: Does this need to integrate with existing auth? Handle edge cases? Scale to all users? Each adds complexity.\"\n\n---\n\n### CUSTOMER COMMUNICATION\n\nTech speak → Customer-friendly:\n\nTech: \"503 Service Unavailable error due to upstream dependency failure\"\nCustomer: \"Our service is temporarily down because a partner system we rely on is having issues. We're working with them to resolve it.\"\n\nTech: \"We're deprecating the v1 API in favor of v2\"\nCustomer: \"We're upgrading our integration to a newer, more reliable version. Here's how to upgrade: [link]. Old version stops working June 1.\"\n\nTech: \"You're hitting rate limits\"\nCustomer: \"You're making requests faster than our system can handle. Here's how to stay within limits: [link]\"\n\n---\n\n### REQUIREMENT TRANSLATION\n\nBusiness requirement → Technical spec:\n\nBusiness: \"Users should be able to export their data\"\nTechnical:\n- Which data? (All entities or specific types?)\n- What format? (CSV, JSON, PDF?)\n- All time or filtered by date?\n- Async job or real-time download?\n- File size limits?\n- Frequency limits?\n\nBusiness: \"Make it secure\"\nTechnical:\n- Authentication required? (OAuth, SSO?)\n- Encryption in transit? (HTTPS)\n- Encryption at rest? (Database encryption)\n- Audit logging?\n- RBAC (role-based access)?\n- Compliance requirements? (SOC2, GDPR)\n\n</translation_patterns>\n\n<meta_guidance>\n\nThe key skill:\n\nTechnical people think in \"how\"\nBusiness people think in \"why\"\n\nYour job: Bridge the gap.\n\nCommon mistakes:\n\n❌ Just removing jargon (still too technical)\n❌ Oversimplifying to point of inaccuracy\n❌ Not preserving the important details\n\n✅ Change vocabulary, keep meaning\n✅ Add context for non-technical\n✅ Add specifics for technical\n\nWhen translating TO engineers:\n- Be specific about requirements\n- Include edge cases\n- Don't tell them how to build it\n- Focus on the problem and constraints\n\nWhen translating TO business:\n- Lead with impact\n- Use analogies\n- Avoid \"it's complicated\" (explain simply instead)\n- Connect to outcomes they care about\n\n</meta_guidance>\n\n</translate_tech_business>",
    "technique": "Bidirectional translation, audience adaptation, jargon mapping",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to convert technical explanation to business language or vice versa"
  },
  {
    "name": "Generate Ideas/Alternatives",
    "category": "Strategy & Planning",
    "prompt": "<generate_ideas>\n\n<problem_inputs>\nWHAT'S THE PROBLEM:\n[Describe what you're trying to solve]\n\nWHAT YOU'VE TRIED:\n[Ideas you already considered]\n\nCONSTRAINTS:\n- Budget: [Tight / Flexible / Unlimited]\n- Timeline: [Urgent / Normal / No rush]\n- Resources: [What you have / don't have]\n- Technical: [Any limitations]\n\nWHAT WOULD MAKE A GOOD IDEA:\n- [ ] Novel/creative\n- [ ] Practical/achievable\n- [ ] Quick to test\n- [ ] Cheap to try\n- [ ] Bold/ambitious\n- [ ] Safe/low-risk\n</problem_inputs>\n\n<ideation_framework>\n\nYou're a creative problem-solver who generates alternatives. Your job: break people out of their default thinking.\n\n---\n\n## FRESH PERSPECTIVES\n\n### Reframe The Problem\n\nYou described it as:\n[Original problem statement]\n\nAlternative framings:\n1. [Different angle on same problem]\n2. [More specific version]\n3. [Broader version]\n4. [Opposite framing]\n\nWhich framing opens new solutions?\n\n---\n\n### Idea Generation\n\nAPPROACH 1: [Name it]\n- The idea: [Specific approach]\n- Why it could work: [Logic]\n- Quick test: [How to validate cheaply]\n- Risk: [What could go wrong]\n\nAPPROACH 2: [Name it]\n[Same structure]\n\nAPPROACH 3: [Name it]\n[Same structure]\n\nAPPROACH 4: [Wildcard - ambitious/creative]\n[Same structure]\n\nAPPROACH 5: [Minimal - simplest possible]\n[Same structure]\n\n---\n\n### Constraint Removal\n\nIf you had unlimited budget:\n[What would you do?]\n\nIf you had unlimited time:\n[What would you try?]\n\nIf you had 10x the team:\n[What becomes possible?]\n\nIf you had to solve it in 1 week:\n[What's the scrappy version?]\n\nIf you couldn't use technology:\n[How would you solve it manually?]\n\n---\n\n### Analogies From Other Domains\n\nHow does [other industry] solve similar problems?\n\nExamples:\n- How does retail handle [your problem]?\n- How does gaming handle [your problem]?\n- How does healthcare handle [your problem]?\n\nWhat can you borrow?\n\n---\n\n### Inversion\n\nInstead of solving the problem, how would you make it worse?\n[List ways to make it worse]\n\nNow, do the opposite:\n[Turn each into a potential solution]\n\n</ideation_framework>\n\n<idea_categories>\n\n### Quick Wins (Do This Week)\n\nIdea:\n[Low-effort, immediate impact]\n\nWhy now:\n[Why this works short-term]\n\n---\n\n### Experiments (Test in 2-4 Weeks)\n\nIdea:\n[Testable hypothesis]\n\nHow to test:\n[Minimal viable experiment]\n\n---\n\n### Big Bets (Multi-Month)\n\nIdea:\n[Ambitious, requires investment]\n\nWhy it could work:\n[Strategic rationale]\n\n---\n\n### Crazy Ideas (Probably Won't Do But Fun)\n\nIdea:\n[Out there, but creative]\n\nWhy it's interesting:\n[Kernel of insight even if not practical]\n\n</idea_categories>\n\n<meta_guidance>\n\nIdeation principles:Quantity before quality\nGenerate 20 ideas, pick the best 3.\n\nSuspend judgment\nDon't shoot down ideas during generation.\nEvaluate later.\n\nCombine ideas\nThe best solution is often 2-3 ideas merged.\n\nStart with extremes\nMost obvious idea + craziest idea = interesting middle ground.\n\nWhen stuck:\n- Work backwards from ideal outcome\n- Steal ideas from adjacent industries\n- Ask \"What would [person] do?\"\n- Remove one constraint and see what opens up\n\nRemember:\nThe first idea is usually the obvious one.\nThe good ideas come after that.\n\n</meta_guidance>\n\n</generate_ideas>",
    "technique": "Lateral thinking, constraint removal, perspective shifting",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Stuck on a problem, need fresh perspectives or alternatives"
  },
  {
    "name": "Create Meeting Agenda",
    "category": "Productivity",
    "prompt": "<meeting_agenda>\n\n<meeting_inputs>\nMEETING PURPOSE:\n[What this meeting is for]\n\nATTENDEES:\n[Who's coming - relevant if roles matter]\n\nDURATION:\n- [ ] 30 minutes\n- [ ] 1 hour\n- [ ] Other: [specify]\n\nMEETING TYPE:\n- [ ] Decision meeting (need to decide X)\n- [ ] Review meeting (evaluate X)\n- [ ] Brainstorm (generate ideas)\n- [ ] Sync (align on status)\n- [ ] Problem-solving (unstick X)\n- [ ] Planning (map out X)\n\nDESIRED OUTCOME:\n[What success looks like for this meeting]\n\nBACKGROUND:\n[Any context or pre-reads]\n</meeting_inputs>\n\n<agenda_framework>\n\nYou're a meeting facilitator who creates agendas that lead to outcomes. Your job: structure time so meetings end with clarity.\n\n---\n\n## MEETING AGENDA\n\nMeeting: [Title]\nDate/Time: [When]\nDuration: [Length]\nAttendees: [Who]\n\nGoal: [One sentence outcome]\n\n---\n\n### Agenda\n\n[0-5 min] Context Setting\n- Quick reminder of why we're here\n- Any updates since last meeting\n\n[5-X min] [Topic 1]\n- [Specific discussion point]\n- [What we need to decide/discuss]\n\n[X-Y min] [Topic 2]\n[Continue pattern]\n\n[Last 5 min] Wrap-up\n- Summarize decisions\n- Clarify action items\n- Schedule follow-up if needed\n\n---\n\n### Pre-Read (Optional)\n\n[Link to doc or brief context people should review before meeting]\n\n---\n\n### Decision/Discussion Topics\n\nTopic 1: [Name]\n- Question: [What we're deciding]\n- Options: [A, B, C]\n- Time: [X minutes]\n\nTopic 2: [Name]\n[Same structure]\n\n---\n\n### Success Looks Like\n\nBy end of meeting, we will have:\n- [ ] [Specific outcome 1]\n- [ ] [Specific outcome 2]\n- [ ] [Specific outcome 3]\n\n</agenda_framework>\n\n<agenda_templates>\n\n### DECISION MEETING (30-60 min)\n\n[5 min] Frame the decision\n- What are we deciding\n- Why now\n- What happens if we don't decide\n\n[15 min] Present options\n- Option A, B, C\n- Pros/cons of each\n- Recommendation if any\n\n[20 min] Discussion\n- Questions\n- Concerns\n- Build alignment\n\n[10 min] Decide + Document\n- Make the call\n- Document rationale\n- Clarify next steps\n\n---\n\n### REVIEW MEETING (30-60 min)\n\n[5 min] Set context\n- What we're reviewing\n- Goals we set\n\n[10 min] Present results\n- Key metrics\n- What worked/didn't\n- Insights\n\n[20 min] Discussion\n- Questions\n- Implications\n- What we learned\n\n[10 min] Next steps\n- Decisions based on review\n- Action items\n\n---\n\n### BRAINSTORM MEETING (60 min)\n\n[10 min] Frame the problem\n- What we're solving\n- Constraints\n- Success criteria\n\n[30 min] Generate ideas\n- Rapid ideation\n- No judgment yet\n- Quantity over quality\n\n[15 min] Cluster & prioritize\n- Group similar ideas\n- Vote on top 3-5\n- Identify quick wins\n\n[5 min] Next steps\n- Who tests what\n- When we reconvene\n\n---\n\n### PROBLEM-SOLVING MEETING (60 min)\n\n[10 min] Define the problem\n- What's broken\n- Impact/urgency\n- Root cause hypothesis\n\n[20 min] Explore solutions\n- Brainstorm approaches\n- Pros/cons\n- Quick wins vs long-term\n\n[20 min] Build plan\n- What we'll do\n- Who owns what\n- Timeline\n\n[10 min] Confirm & close\n- Review action items\n- Escalations needed?\n- Follow-up timing\n\n</agenda_templates>\n\n<meta_guidance>\n\nGood agendas:\n\n✅ Start and end on time\n✅ Have clear outcomes\n✅ Allocate time to topics\n✅ Sent before meeting\n✅ Leave time for decisions/wrap\n\nBad agendas:\n\n❌ \"Discuss X, Y, Z\" (too vague)\n❌ No time allocation (run over)\n❌ All sharing, no deciding\n❌ Made up during meeting\n\nTime allocation rules:\n\n- Give important topics more time\n- Always save 5-10 min for wrap-up\n- Build in buffer (things take longer than planned)\n- If running over, defer topics vs going long\n\nThe forcing function:\n\nWriting agenda forces you to clarify:\n- Why are we meeting?\n- What's the outcome?\n- Is a meeting the right format?\n\nSometimes writing the agenda reveals you don't need the meeting.\n\nRemember:\n\nIf you can't write a clear agenda, the meeting will be chaos.\n\n5 minutes planning saves 30 minutes of confused discussion.\n\n</meta_guidance>\n\n</meeting_agenda>",
    "technique": "Meeting structure templates, time allocation, outcome focus",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Important meeting in an hour, need structured agenda fast"
  },
  {
    "name": "Debug Low Adoption/Metrics\n",
    "category": "Analytics",
    "prompt": "<debug_metrics>\n\n<diagnostic_inputs>\nWHAT'S THE PROBLEM:\n- [ ] Feature launched but low adoption\n- [ ] Metric isn't moving (which one: [specify])\n- [ ] Usage dropped (what changed)\n- [ ] Conversion rate down\n- [ ] Other: [describe]\n\nTHE DATA:\n- Baseline: [What was normal]\n- Current: [What it is now]\n- When it changed: [Timeline]\n- Who it affects: [All users, segment, platform]\n\nWHAT YOU SHIPPED:\n[If this is about a feature - what was it, when did it launch]\n\nCONTEXT:\n- What was the goal: [What you expected to happen]\n- Current theories: [Your hunches about what's wrong]\n</diagnostic_inputs>\n\n<debugging_framework>\n\nYou're a product diagnostician who debugs why features/metrics aren't working. Your job: systematic investigation, not guessing.\n\n---\n\n## DIAGNOSTIC FRAMEWORK\n\n### Step 1: Clarify the Problem\n\nThe symptom:\n[What you observed]\n\nIs this a real problem?\n- Sample size sufficient? (Not just 10 users)\n- Time window sufficient? (Not just 1 day)\n- Measurement correct? (Tracking actually works)\n\nSeverity:\n- How far from goal? [X% gap]\n- Impact on business? [Revenue, churn, growth]\n- Getting worse or stable? [Trend]\n\n---\n\n### Step 2: The Five Why's\n\nWhy is adoption low / metric not moving?Why #1:\n[First-level answer]\n\nWhy #2:\n[Go deeper - why that?]\n\nWhy #3:\n[Go deeper - why that?]\n\nWhy #4:\n[Go deeper - why that?]\n\nWhy #5:\n[Root cause]\n\nExample:\n\nWhy is feature adoption low?\n→ Users aren't discovering it\n\nWhy aren't they discovering it?\n→ It's buried in settings menu\n\nWhy is it in settings?\n→ We thought it was an \"advanced\" feature\n\nWhy did we think that?\n→ We assumed only power users would want it\n\nRoot cause: Wrong assumption about who needs this.\n\n---\n\n### Step 3: Hypothesis Generation\n\nPossible causes:AWARENESS: Do users know it exists?\n- [ ] Feature isn't visible in UI\n- [ ] No announcement/education\n- [ ] Hidden behind other steps\n- [ ] Poor naming/labeling\n\nUNDERSTANDING: Do users know what it does?\n- [ ] Unclear value proposition\n- [ ] Confusing description\n- [ ] No examples/use cases shown\n- [ ] Assumes too much knowledge\n\nMOTIVATION: Do users care?\n- [ ] Solving wrong problem\n- [ ] Not painful enough to act on\n- [ ] Timing is wrong\n- [ ] Wrong user segment\n\nABILITY: Can users actually use it?\n- [ ] Too complicated\n- [ ] Technical barriers (bugs, performance)\n- [ ] Missing prerequisites\n- [ ] Broken on certain platforms\n\nWhich hypothesis is most likely:\n[Your strongest hunch based on data]\n\n---\n\n### Step 4: Investigation Plan\n\nTo test [hypothesis], check:Data to pull:\n- [ ] [Specific metric or funnel]\n- [ ] [Segmentation to look at]\n- [ ] [Comparison to check]\n\nUsers to talk to:\n- [ ] Users who tried it (why'd they stop?)\n- [ ] Users who never tried it (why not?)\n- [ ] Power users (what's their workaround?)\n\nQuick tests:\n- [ ] [Small experiment to validate theory]\n\n---\n\n### Step 5: Funnel Analysis\n\nBreak down the user journey:Step 1: Awareness\n- How many users see/hear about feature?\n- [X%] of total users\n\n→ Drop-off: [Y%] never proceed\n\nStep 2: Interest\n- How many click/explore?\n- [Z%] of aware users\n\n→ Drop-off: [Y%] bounce\n\nStep 3: Trial\n- How many try to use it?\n- [W%] of interested users\n\n→ Drop-off: [Y%] give up\n\nStep 4: Adoption\n- How many successfully use it?\n- [V%] of trial users\n\nBiggest drop-off is at:\n[Which step loses most users]\n\nThis tells us:\n[What that drop-off reveals about the problem]\n\n---\n\n### Step 6: Diagnosis\n\nRoot cause:\n[What you believe is actually wrong]\n\nEvidence:\n- [Data point supporting this]\n- [User feedback supporting this]\n- [Observation supporting this]\n\nConfidence level:\n- [ ] High (70%+ sure)\n- [ ] Medium (40-70% sure)\n- [ ] Low (<40% sure) - need more data\n\n---\n\n### Step 7: Recommended Actions\n\nImmediate fixes (this week):\n- [ ] [Quick win that might help]\n- [ ] [Another quick win]\n\nShort-term (this month):\n- [ ] [Bigger fix or test]\n- [ ] [Another approach]\n\nIf that doesn't work:\n- [ ] [Backup plan]\n\nWhat we're measuring:\n[Metric that will tell us if fix worked]\n\n</debugging_framework>\n\n<common_issues>\n\n### Issue: \"Users aren't discovering the feature\"\n\nDiagnostics:\n- Where is it in UI? (Primary nav, settings, hidden submenu?)\n- Did we announce it? (Email, in-app notification, changelog?)\n- What % of users scroll past it without noticing?\n\nCommon root causes:\n- Buried in UI hierarchy\n- Looks like existing feature (not differentiated)\n- No onboarding tooltip or callout\n- Launched quietly\n\nFixes:\n- Surface it more prominently\n- Add onboarding highlight\n- In-app announcement\n- Email to users who'd benefit\n\n---\n\n### Issue: \"Users try it once and never return\"\n\nDiagnostics:\n- What happens on first use? (Success rate)\n- What do they do after? (Bounce, continue elsewhere?)\n- Error rates or performance issues?\n\nCommon root causes:\n- First experience is bad (buggy, slow, confusing)\n- Value isn't immediate (takes too long to see benefit)\n- No habit formation (not integrated into workflow)\n\nFixes:\n- Improve first-time experience\n- Show value faster\n- Add reminders or triggers to return\n- Make it part of existing workflow\n\n---\n\n### Issue: \"Only certain segments adopt\"\n\nDiagnostics:\n- Which segments DO adopt?\n- What's different about them?\n- Why doesn't it work for others?\n\nCommon root causes:\n- Built for wrong segment\n- Requires prerequisites other segments lack\n- Solves problem only one segment has\n\nFixes:\n- Explicitly target the segment that cares\n- Build version for other segments\n- Accept narrow adoption if right segment\n\n---\n\n### Issue: \"Metric moved in wrong direction\"\n\nDiagnostics:\n- What are second-order effects?\n- Did we break something else?\n- Did we make something worse to improve this?\n\nCommon root causes:\n- Optimization for wrong metric\n- Trade-off we didn't anticipate\n- Changed user behavior unexpectedly\n\nFixes:\n- Look at related metrics\n- Talk to users about what changed\n- Consider rollback if harm > good\n\n</common_issues>\n\n<meta_guidance>\n\nDebugging principles:Data + Intuition\nDon't just look at data.\nDon't just trust gut.\nCombine both.\n\nTalk to users\nThe fastest way to debug:\nFind 5 users who didn't adopt and ask why.\n\nCheck the obvious first\n- Is tracking broken?\n- Is feature actually live?\n- Did announcement go out?\n\nMost \"mysterious\" problems have simple explanations.\n\nRemember:\n\nLow adoption ≠ bad feature\n\nSometimes:\n- You built for wrong segment\n- You haven't marketed it yet\n- Users need time to discover\n\nDon't panic and redesign until you understand WHY.\n\n</meta_guidance>\n\n</debug_metrics>",
    "technique": "Diagnostic frameworks, hypothesis generation, funnel analysis",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Feature shipped but no one's using it, or key metric isn't moving"
  },
  {
    "name": "Write Release Notes\n",
    "category": "PM Artifacts",
    "prompt": "<release_notes>\n\n<release_inputs>\nWHAT YOU SHIPPED:\n[List features, fixes, improvements]\n\nWHO'S THE AUDIENCE:\n- [ ] All users (in-app notification)\n- [ ] Technical users (API changelog)\n- [ ] Specific segment (enterprise, free tier, etc.)\n- [ ] Internal team\n- [ ] Public blog post\n\nTONE:\n- [ ] Excited/celebratory\n- [ ] Professional/matter-of-fact\n- [ ] Technical/detailed\n- [ ] Casual/friendly\n\nCONTEXT:\n- Why this matters: [User impact]\n- What changed behind the scenes: [Technical details if relevant]\n- Breaking changes: [Anything users need to know]\n</release_inputs>\n\n<notes_framework>\n\nYou write release notes that make users care. Your job: turn feature lists into benefits, not just \"we added X.\"\n\n---\n\n## RELEASE NOTES\n\n[Date] - [Version if applicable]\n\n### 🎉 New Features\n\n[Feature Name]\n[One sentence: what users can now do]\n\nWhy this matters: [User benefit, not technical description]\n\nExample:\n❌ \"Added SSO integration\"\n✅ \"Single Sign-On - Your team can now log in with your company credentials. No more remembering another password.\"\n\n---\n\n### ✨ Improvements\n\n[Improvement]\n[What got better]\n\nExample:\n\"Faster search - Results now load 3x faster, especially for large datasets\"\n\n---\n\n### 🐛 Bug Fixes\n\n- Fixed: [Issue that was broken]\n- Fixed: [Another issue]\n\nExample:\n- Fixed: Date picker now works correctly on Safari\n- Fixed: Export no longer times out for large files\n\n---\n\n### 📚 Learn More\n\n[Link to docs, video, or help article if available]\n\n</notes_framework>\n\n<notes_templates>\n\n### Template 1: In-App Notification (Short)\n\nWhat's New ✨[Feature]: [One sentence benefit]\n\n[Improvement]: [What got better]\n\n[Try it now button]\n\n---\n\n### Template 2: Email/Blog (Medium)\n\nSubject: [Feature] is here\n\nWe just shipped [feature] to help you [benefit].\n\nWhat's new:[Feature name]\n[2-3 sentences explaining what it does and why it matters]\n\nHow to use it:\n1. [Step]\n2. [Step]\n\nWhat else:\n- [Improvement]\n- [Bug fix]\n\n[Screenshot or gif if available]\n\n---\n\n### Template 3: Technical Changelog (Detailed)\n\nv2.1.0 - [Date]Breaking Changes:\n- [What changed that might break integrations]\n- Migration path: [How to update]\n\nNew:\n- [Feature] - [Technical description]\n- [API endpoint] - [What it does]\n\nFixed:\n- [Bug with technical details]\n\nDeprecated:\n- [What's going away and when]\n\n---\n\n### Template 4: Internal Release (For Team)\n\nShip Summary - [Date]What we shipped:\n- [Feature with impact]\n- [Fix with context]\n\nMetrics we're watching:\n- [Metric to track adoption]\n\nKnown issues:\n- [Issue we're monitoring]\n\nSupport FYI:\n- [What support team needs to know]\n\n</notes_templates>\n\n<meta_guidance>\n\nRelease notes principles:Lead with benefit, not feature\nUsers don't care you \"implemented X\"\nThey care they can now \"do Y faster\"\n\nKeep it scannable\nEmojis, bullets, short paragraphs\nMost people skim\n\nShow, don't just tell\nScreenshot > description\nGIF > screenshot\nVideo > GIF\n\nBe honest about fixes\nDon't hide that something was broken\n\"Fixed\" is better than pretending it was never an issue\n\nKnow your audience\n- Customers: Benefits and \"how to\"\n- Developers: Technical details and breaking changes\n- Team: Impact and metrics\n\nRemember:\nRelease notes are marketing, not just documentation.\nMake users excited, not just informed.\n\n</meta_guidance>\n\n</release_notes>",
    "technique": "User-benefit framing, changelog formatting, audience layering",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Just shipped feature, need to tell users what's new"
  },
  {
    "name": "Create FAQ\n",
    "category": "PM Artifacts",
    "prompt": "<faq_creation>\n\n<faq_inputs>\nWHAT'S THE FAQ FOR:\n- [ ] New feature launch\n- [ ] Product in general\n- [ ] Pricing/billing\n- [ ] Technical integration\n- [ ] Sales enablement\n- [ ] Support documentation\n\nWHO'S ASKING:\n- [ ] Customers (current users)\n- [ ] Prospects (considering buying)\n- [ ] Sales team (need answers to close deals)\n- [ ] Support team (need to answer tickets)\n\nWHAT YOU KNOW:\n- Common questions you've heard: [List them]\n- Pain points or confusion: [What trips people up]\n- Objections you hear: [Why people hesitate]\n\nCONTEXT:\n[Brief description of what this is about]\n</faq_inputs>\n\n<faq_framework>\n\nYou write FAQs that actually answer questions, not marketing fluff. Your job: anticipate what users ask, give straight answers.\n\n---\n\n## FAQ STRUCTURE\n\n[Category if multiple topics]\n\n### [Question in user's words]\n\n[Short, direct answer]\n\n[Optional: More detail if needed]\n\n[Optional: Link to longer doc/video]\n\n---\n\nEXAMPLE:\n\n### How much does it cost?\n\nPlans start at $29/month for up to 10 users. Enterprise pricing is custom based on your team size and needs.\n\nhttps://www.notion.so/link\n\n---\n\n### Can I try it for free?\n\nYes, 14-day free trial. No credit card required. You get full access to all features.\n\nhttps://www.notion.so/link\n\n</faq_framework>\n\n<faq_categories>\n\n### COMMON FAQ CATEGORIES\n\nGetting Started\n- How do I sign up?\n- What do I need to get started?\n- How long does setup take?\n\nFeatures & Capabilities\n- What can I do with [feature]?\n- Does it integrate with [tool]?\n- What's the difference between [X] and [Y]?\n\nPricing & Billing\n- How much does it cost?\n- Can I change plans?\n- What happens if I cancel?\n\nTechnical\n- What are the system requirements?\n- Is my data secure?\n- What's your uptime SLA?\n\nSupport\n- How do I get help?\n- What's included in support?\n- Can I talk to a human?\n\n---\n\n## YOUR FAQ\n\n[Generate questions and answers based on context]\n\n</faq_categories>\n\n<faq_patterns>\n\n### Pattern 1: Objection Handling\n\nQuestion: \"Is this hard to set up?\"\n\nBad answer: \"No, it's easy!\"\n\nGood answer: \"Most teams are up and running in under 30 minutes. We'll help you import your data and our setup wizard walks you through each step. If you get stuck, chat support is available.\"\n\n---\n\n### Pattern 2: Comparison Questions\n\nQuestion: \"How is this different from [competitor]?\"\n\nBad answer: \"We're better!\"\n\nGood answer: \"We focus on [your differentiation], while [competitor] focuses on [their strength]. Best for: You if [use case], them if [different use case].\"\n\n---\n\n### Pattern 3: Concern Questions\n\nQuestion: \"What happens to my data if I cancel?\"\n\nBad answer: \"You can export it.\"\n\nGood answer: \"You can export all your data anytime (even after canceling). We keep your data for 30 days after cancellation, then permanently delete it. No lock-in.\"\n\n---\n\n### Pattern 4: \"Will this work for me?\" Questions\n\nQuestion: \"Does this work for [specific use case]?\"\n\nBad answer: \"Yes, it works for everyone!\"\n\nGood answer: \"Yes, [specific capability that addresses use case]. For example, [customer name] uses it for [similar use case]. [Link to case study].\"\n\n</faq_patterns>\n\n<meta_guidance>\n\nFAQ principles:Write questions as users ask them\nNot: \"What are the features?\"\nBut: \"Can I do [specific thing I care about]?\"\n\nAnswer the actual question\nDon't dodge or market-speak\nIf answer is \"no\" or \"not yet,\" say so\n\nAnticipate follow-ups\nAfter answering, think: \"What would they ask next?\"\n\nKeep answers short\n2-3 sentences ideal\nLink to more detail if needed\n\nUse real language\nNot: \"Our platform facilitates...\"\nBut: \"You can...\"\n\nUpdate regularly\nFAQ is living document\nAdd new questions as they come up\n\nRemember:\nGood FAQ reduces support load.\nBad FAQ creates more questions.\n\n</meta_guidance>\n\n</faq_creation>",
    "technique": "Question anticipation, objection handling, progressive disclosure",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need FAQ for new feature, product, or common questions"
  },
  {
    "name": "Generate Test Plan",
    "category": "PM Artifacts",
    "prompt": "<test_plan>\n\n<test_inputs>\nWHAT YOU'RE TESTING:\n[Feature or change description]\n\nUSER FLOWS:\n[Main paths users will take]\n\nKNOWN EDGE CASES:\n[Unusual scenarios you're aware of]\n\nPLATFORMS:\n- [ ] Web (desktop)\n- [ ] Web (mobile)\n- [ ] iOS app\n- [ ] Android app\n- [ ] API\n\nINTEGRATION POINTS:\n[What this touches - other features, systems, APIs]\n\nRISK AREAS:\n[What you're most worried could break]\n</test_inputs>\n\n<plan_framework>\n\nYou create test plans that find bugs before users do. Your job: think of everything that could go wrong.\n\n---\n\n## TEST PLAN\n\n### Test Coverage\n\nHappy Path (Must test):\n- [ ] [Main user flow works end-to-end]\n- [ ] [Success states display correctly]\n- [ ] [Expected outcome happens]\n\nEdge Cases (Must test):\n- [ ] [Boundary condition]\n- [ ] [Unusual input]\n- [ ] [Maximum/minimum values]\n\nError Cases (Must test):\n- [ ] [What happens when it fails]\n- [ ] [Error messages are clear]\n- [ ] [User can recover]\n\nIntegration (Must test):\n- [ ] [Works with feature X]\n- [ ] [Doesn't break feature Y]\n- [ ] [API calls succeed]\n\nCross-Platform (Must test):\n- [ ] [Works on mobile]\n- [ ] [Works on different browsers]\n- [ ] [Responsive design works]\n\nPerformance (Should test):\n- [ ] [Loads fast enough]\n- [ ] [Handles expected volume]\n\nSecurity (If applicable):\n- [ ] [Permissions work correctly]\n- [ ] [Can't access others' data]\n\n---\n\n### Test Scenarios\n\nSCENARIO 1: [Happy path name]Setup:\n- User state: [New user, logged in, etc.]\n- Prerequisites: [What needs to be true]\n\nSteps:\n1. [Action]\n2. [Action]\n3. [Action]\n\nExpected Result:\n[What should happen]\n\nActual Result:\n[Leave blank for tester to fill]\n\n---\n\nSCENARIO 2: [Edge case name]\n\n[Same structure]\n\n---\n\n### Acceptance Criteria Checklist\n\nFrom the PRD/spec, these must all pass:\n\n- [ ] [Acceptance criteria 1]\n- [ ] [Acceptance criteria 2]\n- [ ] [Acceptance criteria 3]\n\n---\n\n### Things That Should NOT Break\n\n- [ ] [Existing feature that this could affect]\n- [ ] [Another feature]\n- [ ] [Core workflow]\n\n---\n\n### Test Data Needed\n\n- [ ] Test account with [specific setup]\n- [ ] Sample data for [use case]\n- [ ] Edge case data (empty, max, special chars)\n\n---\n\n### Known Issues / Won't Fix\n\n- [Issue we're shipping with (with rationale)]\n- [Edge case we're not handling yet]\n\n</plan_framework>\n\n<test_scenarios>\n\n### Common Scenarios to Test\n\nEmpty States:\n- What if user has no data yet?\n- What if search returns nothing?\n- What if list is empty?\n\nMaximum/Minimum:\n- What if user has 10,000 items?\n- What if user has 1 item?\n- What if field is at character limit?\n\nSpecial Characters:\n- What if name has apostrophe? (O'Brien)\n- What if email has + in it?\n- What if description has emoji?\n\nTiming:\n- What if two actions happen simultaneously?\n- What if user clicks button multiple times?\n- What if session expires mid-flow?\n\nPermissions:\n- What if user is admin vs regular user?\n- What if user tries to access others' data?\n- What if user is read-only?\n\nNetwork:\n- What if API call fails?\n- What if request times out?\n- What if user is offline?\n\nBrowser/Device:\n- Does it work in Safari, Chrome, Firefox?\n- Does it work on iPhone, Android, tablet?\n- Does it work with keyboard only?\n\n</test_scenarios>\n\n<meta_guidance>\n\nTest plan principles:Think like an attacker\nHow would you break this?\nWhat's the weirdest thing a user could do?\n\nTest the unhappy paths\nHappy path usually works\nIt's the errors that surprise you\n\nDon't just test features\nTest that you didn't break other things\n\nWrite it before building\nTest plan reveals gaps in spec\nBetter to find them before coding\n\nNot everything needs testing\nFocus on:\n- High-risk changes\n- User-facing features\n- Integration points\n\nRemember:\nGood test plan finds bugs in QA.\nBad test plan finds bugs in production.\n\n</meta_guidance>\n\n</test_plan>",
    "technique": "Edge case identification, scenario coverage, acceptance criteria expansion",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "About to launch, need to document what to test before shipping"
  },
  {
    "name": "Write Product Brief/One-Pager",
    "category": "PM Artifacts",
    "prompt": "<product_brief>\n\n<brief_inputs>\nWHAT'S THE BRIEF FOR:\n- [ ] New feature\n- [ ] New product\n- [ ] Initiative/project\n- [ ] Strategic direction\n\nWHO'S READING IT:\n- [ ] Execs (need to approve)\n- [ ] Team (need to build)\n- [ ] Stakeholders (need to align)\n- [ ] Cross-functional partners\n\nCURRENT STATUS:\n- [ ] Idea/proposal (seeking buy-in)\n- [ ] Approved (documenting decision)\n- [ ] In progress (status update)\n\nCONTEXT:\n- What you're proposing: [Description]\n- Why it matters: [Business case]\n- Key trade-offs: [What you're saying no to]\n</brief_inputs>\n\n<brief_framework>\n\nYou write product briefs that get to the point. Your job: fit everything decision-makers need on one page.\n\n---\n\n## PRODUCT BRIEF\n\nProject: [Name]\nOwner: [Your name]\nDate: [Date]\nStatus: [Proposal / Approved / In Progress]\n\n---\n\n### TL;DR (3 sentences)\n\n[What it is, why it matters, what you need]\n\n---\n\n### Problem\n\nUser pain: [Specific problem users have]\n\nBusiness impact: [Cost of not solving this]\n- [Metric 1]: [Current state and impact]\n- [Metric 2]: [Current state and impact]\n\nWhy now: [Urgency or opportunity]\n\n---\n\n### Solution\n\nWhat we're building: [High-level approach]\n\nHow it works: [Key user flow in 3-4 bullets]\n1. [Step]\n2. [Step]\n3. [Step]\n\nWhy this approach: [Rationale for this vs alternatives]\n\n---\n\n### Success Metrics\n\nPrimary: [The one metric that matters]\n- Baseline: [Current]\n- Target: [Goal]\n- Timeline: [When]\n\nSecondary:\n- [Supporting metric]\n- [Supporting metric]\n\n---\n\n### Scope\n\nIn scope:\n- [What we're building]\n- [What's included]\n\nOut of scope:\n- [What we're explicitly NOT doing]\n- [Future phase considerations]\n\n---\n\n### Investment\n\nResources needed:\n- Engineering: [X weeks]\n- Design: [Y weeks]\n- Other: [Specify]\n\nTimeline:\n- Kickoff: [Date]\n- Launch: [Date]\n\n---\n\n### Risks & Mitigations\n\nRisk 1: [What could go wrong]\n- Mitigation: [How we'll address it]\n\nRisk 2: [Another risk]\n- Mitigation: [How we'll address it]\n\n---\n\n### Decision Needed\n\nAsk: [What you need from stakeholders]\n- [ ] Approval to proceed\n- [ ] Resource allocation\n- [ ] Prioritization decision\n- [ ] Other: [Specify]\n\nBy when: [Decision deadline]\n\n</brief_framework>\n\n<brief_templates>\n\n### Template 1: Feature Proposal\n\nWHY THIS MATTERS:\n[Problem and business case in 2-3 sentences]\n\nWHAT WE'RE BUILDING:\n[Solution in 2-3 sentences]\n\nEXPECTED IMPACT:\n[Metrics and estimates]\n\nINVESTMENT:\n[Time and resources]\n\nALTERNATIVES CONSIDERED:\n[What else we looked at and why we didn't choose it]\n\nNEXT STEPS:\n[What happens if approved]\n\n---\n\n### Template 2: Initiative Brief\n\nSTRATEGIC CONTEXT:\n[How this fits company goals]\n\nTHE OPPORTUNITY:\n[Market, user, or business opportunity]\n\nOUR APPROACH:\n[High-level strategy]\n\nSUCCESS LOOKS LIKE:\n[Clear outcomes, not outputs]\n\nWHAT WE NEED:\n[Resources, decisions, support]\n\n---\n\n### Template 3: Technical Architecture Brief\n\nCURRENT STATE:\n[What exists today and limitations]\n\nPROPOSED CHANGE:\n[What we want to build/change]\n\nTECHNICAL APPROACH:\n[Architecture, stack, key decisions]\n\nTRADE-OFFS:\n[What we're optimizing for vs not]\n\nMIGRATION PATH:\n[How we get from current to future]\n\n</brief_templates>\n\n<meta_guidance>\n\nProduct brief principles:One page max\nIf you can't fit it on one page, it's not clear enough\nException: Appendix for detailed specs\n\nLead with why\nDon't bury the business case\nStart with problem and impact\n\nMake the ask clear\nWhat decision do you need?\nBy when?\n\nShow trade-offs\nWhat are you NOT doing?\nWhat's the downside of this approach?\n\nInclude alternatives\nShows you thought it through\nExplains why this is the best path\n\nUse visuals\nDiagram > paragraphs\nScreenshot > description\n\nRemember:\nBrief is sales tool AND alignment doc.\nGet buy-in AND document decision.\n\n</meta_guidance>\n\n</product_brief>",
    "technique": "Pyramid principle, executive summary format, decision-focused structure",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need concise doc to align stakeholders on what you're building and why"
  },
  {
    "name": "Quarterly Planning\n",
    "category": "Strategy & Planning",
    "prompt": "<quarterly_planning>\n\n<planning_inputs>\nPASTE YOUR INPUTS:\n- Company/team goals for quarter\n- Backlog of ideas/requests\n- In-flight projects\n- Team capacity\n- Known constraints or deadlines\n\nCONTEXT:\n- What shipped last quarter: [Key outcomes]\n- What's carrying over: [Unfinished work]\n- Strategic priorities: [Top company goals]\n- Team size: [How many people]\n\nPLANNING FOR:\n- Quarter: [Q1/Q2/Q3/Q4 Year]\n- Team: [Your team name]\n</planning_inputs>\n\n<planning_framework>\n\nYou help PMs plan quarters that are ambitious but achievable. Your job: balance strategic bets with team capacity reality.\n\n---\n\n## QUARTERLY PLAN\n\nQ[X] [Year] - [Team Name]\n\n---\n\n### Theme\n\nThis quarter is about:\n[One sentence that captures the focus]\n\nWhy this focus:\n[Strategic rationale]\n\n---\n\n### Goals (OKRs)\n\nObjective 1: [Ambitious, qualitative goal]\n\nKey Results:\n- KR1: [Measurable outcome] - Owner: [Name]\n- KR2: [Measurable outcome] - Owner: [Name]\n- KR3: [Measurable outcome] - Owner: [Name]\n\nObjective 2: [If applicable]\n\n[Same structure]\n\n---\n\n### Roadmap\n\nMONTH 1:Focus: [What we're shipping/achieving]\n\nProjects:\n- [Project name] - [Brief description] - Owner: [Name]\n- [Project name] - [Brief description] - Owner: [Name]\n\nMilestones:\n- [Key milestone and date]\n\n---\n\nMONTH 2:\n\n[Same structure]\n\n---\n\nMONTH 3:\n\n[Same structure]\n\n---\n\n### Capacity Planning\n\nTeam capacity: [X person-weeks available]\n\nAllocated:\n- Project A: [Y weeks]\n- Project B: [Z weeks]\n- Bug fixes/maintenance: [W weeks, usually 20%]\n- Buffer: [V weeks, usually 20%]\n\nTotal: [Should equal capacity]\n\n---\n\n### What We're NOT Doing\n\n[Explicitly list things you're deferring]\n\nDeferred to Q[X+1]:\n- [Feature/project]\n- [Feature/project]\n\nNot doing (saying no):\n- [Request/idea]\n- [Request/idea]\n\n---\n\n### Risks & Dependencies\n\nRisk 1: [What could derail plan]\n- Mitigation: [How you'll handle it]\n- Owner: [Who's monitoring]\n\nDependency 1: [External dependency]\n- Impact if delayed: [Consequence]\n- Backup plan: [Alternative]\n\n---\n\n### Success Criteria\n\nAt end of quarter, we'll know we succeeded if:\n- [ ] [Specific outcome]\n- [ ] [Specific outcome]\n- [ ] [Specific outcome]\n\n</planning_framework>\n\n<planning_process>\n\n### Step 1: Gather Inputs\n\nFrom leadership:\n- Company OKRs for quarter\n- Strategic priorities\n- Revenue/growth targets\n\nFrom customers/data:\n- Top feature requests\n- Churn reasons\n- Usage patterns\n- NPS feedback\n\nFrom team:\n- Technical debt priorities\n- Infrastructure needs\n- Velocity concerns\n\nFrom stakeholders:\n- Sales asks\n- Marketing needs\n- Support pain points\n\n---\n\n### Step 2: Categorize Work\n\nStrategic Bets (30-40% of capacity)\n[New initiatives that drive growth/differentiation]\n\nCustomer Asks (30-40% of capacity)\n[High-impact feature requests]\n\nTechnical Excellence (20-30% of capacity)\n[Tech debt, infrastructure, reliability]\n\nBusiness as Usual (10-20% of capacity)\n[Bug fixes, maintenance, support]\n\n---\n\n### Step 3: Prioritization\n\nMust do (non-negotiable):\n- [ ] [Hard deadline or dependency]\n- [ ] [Critical to company goals]\n\nShould do (high value):\n- [ ] [Big impact, aligned to OKRs]\n- [ ] [Customer commitment]\n\nCould do (if capacity):\n- [ ] [Nice to have]\n- [ ] [Low-hanging fruit]\n\nWon't do (explicit no):\n- [ ] [Not aligned to goals]\n- [ ] [Can wait until later]\n\n---\n\n### Step 4: Capacity Reality Check\n\nAvailable capacity:\n- [X] people × 13 weeks × 40 hrs = [Y] hours\n- Minus holidays/PTO: [Z] hours\n- Minus meetings/overhead (30%): [W] hours\n- Net capacity: [V] hours or [P] person-weeks\n\nRule of thumb:\n- Big project: 4-8 weeks\n- Medium project: 2-4 weeks\n- Small project: <2 weeks\n\nTypical quarter:\n- 2-3 big projects\n- 3-4 medium projects\n- Ongoing small improvements\n\nIf your plan exceeds capacity:\nCut scope, not quality.\n\n---\n\n### Step 5: Timeline & Sequencing\n\nDependencies:\nWhat must happen before what?\n\nParallelization:\nWhat can different people work on simultaneously?\n\nRisk management:\nStart risky/uncertain work earlier.\n\nBuffer:\nAlways keep 20% buffer for unknowns.\n\n</planning_process>\n\n<meta_guidance>\n\nQuarterly planning principles:Be ambitious but realistic\nPlan for 70% capacity, not 100%\nThings take longer than you think\n\nChoose a theme\nDon't do 10 unrelated things\nPick a focus and commit\n\nSay no explicitly\nDocument what you're not doing\nPrevents scope creep\n\nBuild in flexibility\nPlans change\nDon't over-commit to rigid timeline\n\nGet buy-in early\nShare draft with team and stakeholders\nAdjust before committing\n\nReview monthly\nQuarterly plans aren't set-it-and-forget-it\nCheck in and adjust as needed\n\nRemember:\nThe goal isn't to plan perfectly.\nIt's to align team on what matters.\n\nDone quarterly plan > perfect quarterly plan.\n\n</meta_guidance>\n\n</quarterly_planning>",
    "technique": "OKR framework, capacity planning, strategic prioritization",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to plan next quarter's roadmap and priorities"
  },
  {
    "name": "Create User Persona\n",
    "category": "Discovery",
    "prompt": "<user_persona>\n\n<persona_inputs>\nYOUR RESEARCH:\n- User interviews: [Paste notes or summaries]\n- Usage data: [Key behavioral patterns]\n- Customer segments: [How you currently segment]\n- Feedback/quotes: [What users say]\n\nWHAT YOU KNOW:\n- Who uses your product: [Demographics, roles, company types]\n- How they use it: [Workflows, frequency, features]\n- Why they use it: [Problems they're solving]\n\nPURPOSE:\n- [ ] Team alignment on target user\n- [ ] Design guidance\n- [ ] Marketing messaging\n- [ ] Prioritization framework\n</persona_inputs>\n\n<persona_framework>\n\nYou create user personas that are useful, not decorative. Your job: extract real patterns from research, not make up composite characters.\n\n---\n\n## USER PERSONA\n\n[Persona Name] - [Role/Type]\n\n\"[Quote that captures their mindset]\"\n\n---\n\n### Overview\n\nWho they are:\n[1-2 sentences describing this user type]\n\n% of user base:\n[Roughly how common this persona is]\n\nExample companies:\n[Real examples help make it concrete]\n\n---\n\n### Demographics & Context\n\nRole: [Job title(s)]\nIndustry: [Sectors where common]\nCompany size: [Small/Medium/Enterprise]\nTech savviness: [Low/Medium/High]\nTeam structure: [Who they work with]\n\n---\n\n### Goals & Jobs-to-be-Done\n\nPrimary goal:\n[What they're ultimately trying to achieve]\n\nKey jobs:\n1. [Job-to-be-done #1]\n2. [Job-to-be-done #2]\n3. [Job-to-be-done #3]\n\nSuccess looks like:\n[How they measure if they're winning]\n\n---\n\n### Behaviors & Usage Patterns\n\nHow they use the product:\n- Frequency: [Daily/Weekly/Monthly]\n- Session length: [Typical time spent]\n- Top features: [What they use most]\n- Workflow: [How it fits their day]\n\nTypical user journey:\n1. [Entry point]\n2. [Common action]\n3. [Outcome]\n\n---\n\n### Pain Points\n\nBiggest frustrations:\n1. [Pain point with evidence]\n2. [Pain point with evidence]\n3. [Pain point with evidence]\n\nCurrent workarounds:\n[How they solve these problems today]\n\n---\n\n### Motivations & Barriers\n\nWhat drives them:\n- [Motivation]\n- [Motivation]\n\nWhat holds them back:\n- [Barrier]\n- [Barrier]\n\nDecision criteria:\n[What makes them choose/stay/leave]\n\n---\n\n### Quotes (Real user feedback)\n\n\"[Quote about what they need]\"\n\n\"[Quote about frustration]\"\n\n\"[Quote about what works]\"\n\n---\n\n### Design Implications\n\nFor product:\n- [What this means for features]\n- [What this means for UI/UX]\n\nFor messaging:\n- [How to talk to this persona]\n- [What resonates with them]\n\n---\n\n### How to Serve This Persona\n\nDo:\n- [Specific approach that works]\n- [Another effective approach]\n\nDon't:\n- [What frustrates this persona]\n- [What doesn't resonate]\n\n</persona_framework>\n\n<persona_patterns>\n\n### Pattern 1: Power User\n\nCharacteristics:\n- High frequency usage\n- Uses advanced features\n- Wants efficiency and shortcuts\n- Vocal about product gaps\n- Willing to learn complexity\n\nDesign implications:\n- Keyboard shortcuts matter\n- Customization is valued\n- Don't hide advanced features\n- Power features worth building\n\nQuote: \"I live in this product 8 hours a day. Every second you save me adds up.\"\n\n---\n\n### Pattern 2: Occasional User\n\nCharacteristics:\n- Infrequent usage (weekly/monthly)\n- Uses basic features\n- Forgets how things work between uses\n- Needs simple, intuitive UI\n- Low tolerance for complexity\n\nDesign implications:\n- Discoverability crucial\n- Inline help important\n- Simplicity > power\n- Onboarding matters every session\n\nQuote: \"I use this once a month. If it's not obvious, I give up.\"\n\n---\n\n### Pattern 3: Admin/Buyer\n\nCharacteristics:\n- Sets up for others\n- Cares about control and visibility\n- Not the daily end user\n- Makes purchase decisions\n- Needs reporting and governance\n\nDesign implications:\n- Admin features matter for sales\n- Reporting and analytics needed\n- Permissions and controls important\n- Support for rollout crucial\n\nQuote: \"I need to know my team is using this, and I need to prove ROI.\"\n\n---\n\n### Pattern 4: Evaluator\n\nCharacteristics:\n- Trying to decide if to buy\n- Comparing alternatives\n- Time-constrained evaluation\n- Skeptical, needs proof\n- Looking for deal-breakers\n\nDesign implications:\n- Fast time-to-value crucial\n- Clear differentiation needed\n- Demo/trial experience critical\n- Comparison content helpful\n\nQuote: \"I'm evaluating 3 tools this week. Show me quickly why yours is best.\"\n\n</persona_patterns>\n\n<meta_guidance>\n\nPersona principles:Base on real research\nNot made-up demographics\nReal patterns from real users\n\nFocus on behavior, not demographics\n\"Uses product daily for X\" > \"Male, 35, lives in city\"\n\nKeep it actionable\n\"So what?\" test: Does this help make decisions?\nIf not, it's decoration.\n\nDon't over-personify\nYou don't need their favorite color\nYou need their pain points and goals\n\nLimit to 2-4 personas\nMore than 4 = you're not focused\n1-2 is often enough\n\nUpdate based on data\nPersonas evolve\nRevisit quarterly\n\nRemember:\nPersonas are tools for alignment.\nIf team doesn't reference them in decisions, they're useless.\n\nMake them real, specific, and useful.\n\n</meta_guidance>\n\n</user_persona>",
    "technique": "Pattern synthesis from research, behavioral clustering, jobs-to-be-done integration",
    "tools": "Claude Projects, NotebookLM, ChatGPT Projects",
    "useCase": "Need to document who your users are for team alignment"
  },
  {
    "name": "Pricing Analysis",
    "category": "Strategy & Planning",
    "prompt": "<pricing_analysis>\n\n<pricing_inputs>\nCURRENT STATE:\n- Current pricing: [What you charge now, or \"not set yet\"]\n- Revenue model: [Per user, per usage, flat fee, freemium]\n- Competitors' pricing: [What others charge]\n\nYOUR PRODUCT:\n- What it does: [Core value prop]\n- Target customers: [Who buys this]\n- Cost to serve: [Rough COGS or operational cost per customer]\n\nTHE QUESTION:\n- [ ] Set initial pricing (new product)\n- [ ] Change pricing (increase/decrease)\n- [ ] Add new tier\n- [ ] Move to different model\n\nRESEARCH/DATA:\n- Customer feedback on pricing: [What you've heard]\n- Willingness to pay signals: [Any data on what customers would pay]\n- Churn related to pricing: [If relevant]\n</pricing_inputs>\n\n<pricing_framework>\n\nYou help PMs think through pricing strategy. Your job: connect pricing to value, not just costs.\n\n---\n\n## PRICING ANALYSIS\n\n### Value-Based Pricing Foundation\n\nWhat value does your product create?Quantifiable value:\n- Saves time: [X hours/week]\n- Saves money: [$Y/month in costs eliminated]\n- Generates revenue: [$Z in new revenue enabled]\n- Reduces risk: [$ value of problems prevented]\n\nYour product's value: $[Calculated total]\n\nPricing rule of thumb:\nCharge 1/10th to 1/3rd of value created.\n\nIf product saves customer $10K/month, you can charge $1-3K/month.\n\n---\n\n### Competitive Positioning\n\nCompetitive pricing landscape:\n\n| Competitor | Price | Positioning |\n|------------|-------|-------------|\n| [Name] | [$X/mo] | [Premium/Mid/Budget] |\n| [Name] | [$Y/mo] | [Premium/Mid/Budget] |\n| Your target | [?] | [Where you want to be] |\n\nWhere do you want to be?\n- [ ] Premium (20-30% above market)\n- [ ] Market rate (within 10% of competition)\n- [ ] Value option (20-30% below market)\n\nWhy that positioning:\n[Strategic rationale]\n\n---\n\n### Pricing Model Options\n\nOPTION 1: Per-User Pricing\n- Structure: $[X]/user/month\n- Good if: Value scales with team size\n- Bad if: Encourages limiting users (adoption blocker)\n\nOPTION 2: Usage-Based Pricing\n- Structure: $[X] per [action/event/GB]\n- Good if: Costs scale with usage, perceived as fair\n- Bad if: Unpredictable bills scare customers\n\nOPTION 3: Flat-Fee Pricing\n- Structure: $[X]/month for unlimited\n- Good if: Simple, predictable, encourages adoption\n- Bad if: Heavy users cost you more than they pay\n\nOPTION 4: Tiered Pricing\n- Structure: Starter/Pro/Enterprise\n- Good if: Different segments want different things\n- Bad if: Complexity can confuse\n\nOPTION 5: Freemium\n- Structure: Free tier + paid upgrades\n- Good if: PLG motion, network effects, low marginal cost\n- Bad if: Free users never convert, support burden\n\nRecommendation: [Which model and why]\n\n---\n\n### Tiered Pricing Structure (If Applicable)\n\nFREE/TRIAL TIERPrice: $0\nTarget: Individuals, evaluators\nIncludes:\n- [Limited feature set]\n- [Usage limits]\n\nConversion path: [How they upgrade]\n\n---\n\nSTARTER/BASIC TIERPrice: $[X]/month\nTarget: [Segment]\nIncludes:\n- [Core features]\n- [Support level]\n\nWhy this price: [Rationale]\n\n---\n\nPROFESSIONAL TIERPrice: $[Y]/month (X.X% increase)\nTarget: [Segment]\nIncludes:\n- Everything in Starter\n- [Additional features that matter]\n- [Better support]\n\nValue gap: [Why someone upgrades]\n\n---\n\nENTERPRISE TIERPrice: Custom (typically $[Z]+ range)\nTarget: [Large companies]\nIncludes:\n- Everything in Pro\n- [Enterprise features: SSO, SCIM, etc.]\n- [Dedicated support]\n- [SLAs]\n\nSold via: [Sales team vs self-serve]\n\n---\n\n### Pricing Principles\n\nGood pricing:\n- Simple to understand\n- Aligns with value received\n- Has natural upgrade path\n- Predictable for customer\n\nAvoid:\n- Too many tiers (confusing)\n- Pricing on features (wrong incentive)\n- Nickel-and-diming (annoying)\n\n---\n\n### Testing Strategy\n\nBefore committing:Test 1: Customer interviews\n- Ask: \"At what price does this become expensive but still worth it?\"\n- Ask: \"At what price does this become too expensive?\"\n- Gap between = your pricing range\n\nTest 2: A/B test pricing page\n- Show different prices to different visitors\n- Measure conversion rate × revenue per customer\n- Optimize for revenue, not conversions\n\nTest 3: Grandfather existing customers\n- New pricing for new customers only\n- Measure uptake before forcing migration\n\n---\n\n### Migration Plan (If Changing Pricing)\n\nExisting customers:\n- [ ] Grandfather at current price (indefinite)\n- [ ] Grandfather for X months, then migrate\n- [ ] Force migration (with advance notice)\n\nCommunication:\n- Announce: [X weeks ahead]\n- Message: [Focus on value added]\n- Support: [How to help customers adjust]\n\nRisk mitigation:\n- Watch churn closely\n- Have win-back offer ready\n- Support team prepared for objections\n\n</pricing_framework>\n\n<meta_guidance>\n\nPricing strategy principles:Price on value, not cost\nYour costs don't matter to customers\nWhat matters: value you create for them\n\nSimple > complex\nIf your pricing needs a calculator, it's too complex\n\nRoom to grow\nHave upgrade path built in\nEasy to move from $10→$50→$200/mo\n\nRaise prices regularly\nMany startups charge too little\nEasier to lower than raise\n\nTest and iterate\nFirst pricing is always wrong\nBe willing to change\n\nRemember:\nPricing is positioning.\nPremium price = premium product (in customers' minds)\nCheap price = cheaper product\n\nMost PMs undercharge.\n\n</meta_guidance>\n\n</pricing_analysis>",
    "technique": "Value-based pricing, competitive positioning, willingness-to-pay analysis",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to set or change pricing, or analyze pricing strategy"
  },
  {
    "name": "Root Cause Analysis (5 Whys)\n",
    "category": "Operations",
    "prompt": "<root_cause_analysis>\n\n<rca_inputs>\nWHAT HAPPENED:\n[Describe the incident, bug, failure, or problem]\n\nWHEN:\n[Timeline of events]\n\nIMPACT:\n- Users affected: [Number or percentage]\n- Business impact: [Revenue, churn, reputation]\n- Duration: [How long]\n\nIMMEDIATE FIX:\n[What you did to stop the bleeding]\n\nCONTEXT:\n[Any relevant background or recent changes]\n</rca_inputs>\n\n<rca_framework>\n\nYou help teams get to root causes, not symptoms. Your job: ask \"why\" until you find the systemic issue, not just the proximate cause.\n\n---\n\n## ROOT CAUSE ANALYSIS\n\nIncident: [Brief description]\nDate: [When it happened]\nSeverity: [Critical/High/Medium/Low]\n\n---\n\n### The Problem\n\nWhat happened: [Observable symptom]\n\nImpact:\n- [Specific impact metric]\n- [Customer effect]\n- [Business consequence]\n\n---\n\n### The 5 Whys\n\nProblem Statement:\n[Starting problem]\n\nWhy #1: Why did this happen?\n→ [First-level cause]\n\nWhy #2: Why did that happen?\n→ [Dig deeper]\n\nWhy #3: Why did that happen?\n→ [Keep going]\n\nWhy #4: Why did that happen?\n→ [Almost there]\n\nWhy #5: Why did that happen?\n→ [Root cause]\n\nRoot Cause:\n[The systemic issue that, if fixed, prevents recurrence]\n\n---\n\nEXAMPLE:Problem: Users couldn't log in for 2 hours\n\nWhy #1: Why couldn't users log in?\n→ Authentication service was down\n\nWhy #2: Why was auth service down?\n→ Database ran out of connections\n\nWhy #3: Why did database run out of connections?\n→ Connection pool was too small for peak traffic\n\nWhy #4: Why was connection pool too small?\n→ We never load-tested this service\n\nWhy #5: Why didn't we load-test?\n→ No testing process for new services before production\n\nRoot cause: Missing load testing in deployment process\n\n---\n\n### Contributing Factors\n\nBeyond the primary root cause, what else contributed?Technical factors:\n- [Infrastructure, code, architecture]\n\nProcess factors:\n- [Missing reviews, unclear ownership]\n\nHuman factors:\n- [Communication gaps, knowledge gaps]\n\nOrganizational factors:\n- [Incentives, priorities, resources]\n\n---\n\n### Corrective Actions\n\nImmediate (This week):\n- [ ] [Fix to prevent immediate recurrence] - Owner: [Name]\n- [ ] [Another immediate action] - Owner: [Name]\n\nShort-term (This month):\n- [ ] [Process improvement] - Owner: [Name]\n- [ ] [System improvement] - Owner: [Name]\n\nLong-term (This quarter):\n- [ ] [Systemic fix] - Owner: [Name]\n\nHow we'll know it's fixed:\n[Metric or signal that shows problem solved]\n\n---\n\n### Lessons Learned\n\nWhat worked well:\n- [Response that went right]\n- [System that caught issue]\n\nWhat we'll do differently:\n- [Change to process]\n- [Change to system]\n- [Change to team practice]\n\n---\n\n### Follow-Up\n\nReview date: [When to check if fixes worked]\nOwner: [Who's responsible for follow-through]\n\n</rca_framework>\n\n<rca_patterns>\n\n### Common Root Causes\n\n\"We didn't know\"\n→ Missing monitoring/alerting\n→ Fix: Add visibility\n\n\"We didn't have time\"\n→ Technical debt prioritization\n→ Fix: Allocate capacity for maintenance\n\n\"We didn't think it would happen\"\n→ Missing risk assessment\n→ Fix: Pre-mortems before launches\n\n\"We didn't communicate\"\n→ Unclear ownership or handoffs\n→ Fix: RACI matrix, better documentation\n\n\"We didn't test\"\n→ Missing testing process\n→ Fix: Add required tests before deploy\n\n\"We made a trade-off\"\n→ Conscious decision that backfired\n→ Fix: Document trade-offs, revisit regularly\n\n</rca_patterns>\n\n<meta_guidance>\n\nRoot cause analysis principles:Blame the system, not the person\n\"Sarah deployed bad code\" is not root cause\n\"No code review process\" is root cause\n\nDon't stop at first answer\nFirst \"why\" is usually proximate cause\nKeep going until you hit systemic issue\n\nMultiple root causes\nComplex failures often have multiple contributing factors\nFix all of them\n\nFocus on prevention\nRCA isn't about blame\nIt's about making sure it doesn't happen again\n\nDocument and share\nRCAs are learning opportunities\nShare widely so others learn too\n\nRemember:\nIf your root cause is \"human error,\" you haven't dug deep enough.\n\nSystems should prevent human error, not rely on humans being perfect.\n\n</meta_guidance>\n\n</root_cause_analysis>",
    "technique": "5 Whys method, fishbone analysis, systemic thinking",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Something broke or failed, need to understand why and fix root cause"
  },
  {
    "name": "Create Sales Battlecard\n",
    "category": "GTM",
    "prompt": "<sales_battlecard>\n\n<battlecard_inputs>\nYOUR PRODUCT:\n[What you sell and core value prop]\n\nCOMPETITOR:\n[Which competitor this is for]\n\nYOUR RESEARCH:\n- Competitor's strengths: [What they do well]\n- Competitor's weaknesses: [Where they fall short]\n- How customers compare you: [What you hear in sales calls]\n- Win/loss data: [When you win vs lose against them]\n\nCOMMON OBJECTIONS:\n[What prospects say when considering competitor]\n\nYOUR DIFFERENTIATORS:\n[What makes you better/different]\n</battlecard_inputs>\n\n<battlecard_framework>\n\nYou create battlecards that help sales win competitive deals. Your job: arm sales with positioning, not just feature comparisons.\n\n---\n\n## SALES BATTLECARD\n\nCompetitor: [Name]\nLast updated: [Date]\n\n---\n\n### Quick Reference\n\nWhen to use this:\n[Buying scenario where this competitor comes up]\n\nTheir position: [How they market themselves]\nYour position: [How you differentiate]\n\nWin themes:\n[Top 3 reasons you beat them]\n1. [Theme with proof point]\n2. [Theme with proof point]\n3. [Theme with proof point]\n\n---\n\n### Competitive Overview\n\nWho they are:\n[Brief description]\n\nTheir ideal customer:\n[Who they're best for]\n\nWhat they're good at:\n[Honest assessment of strengths]\n\nWhere they struggle:\n[Weaknesses you can exploit]\n\n---\n\n### Head-to-Head Comparison\n\n| Factor | Us | Them | Talk Track |\n|--------|----|----|------------|\n| [Key differentiator] | ✅ [Our strength] | ❌ [Their gap] | [What to say] |\n| [Another factor] | [Status] | [Status] | [What to say] |\n\n---\n\n### Messaging & Positioning\n\nIf they say: \"We're considering [Competitor]\"\n\nYou say: \"Great, they're a solid choice for [use case]. The main difference customers tell us: we're [differentiation], while they're [positioning]. For [your ICP], that means [benefit].\"\n\nExample:\n\"Great, Competitor X is solid for basic analytics. The difference: we're built for real-time decision-making, while they're batch processing. For ops teams that need to react in minutes not hours, that's critical.\"\n\n---\n\n### Objection Handling\n\nOBJECTION: \"Competitor is cheaper\"\n\nResponse:\n\"You're right, they're less expensive upfront. What we've found: customers save more long-term because [reason]. [Customer name] switched from them and saved $X by [specific benefit].\"\n\nEvidence: [ROI data or case study]\n\n---\n\nOBJECTION: \"Competitor has feature X\"\n\nResponse:\n\"They do have that feature. The trade-off customers mention: [downside of their approach]. We built [your approach] instead because [reason]. For [your ICP], that means [benefit].\"\n\n---\n\nOBJECTION: \"Competitor has been around longer\"\n\nResponse:\n\"True, they've got tenure. What we hear from customers who switched: they're built on older technology. We started fresh with [modern approach], which means [specific advantage].\"\n\n---\n\n### Trap Questions (Ask These)\n\nQuestions that expose their weaknesses:\n\nQuestion 1: \"How does [their product] handle [edge case they struggle with]?\"\n- Why this works: [Exposes gap]\n- Follow-up: [How you handle it better]\n\nQuestion 2: \"What happens when [scenario where they fail]?\"\n- Why this works: [Reveals limitation]\n- Follow-up: [Your advantage]\n\nQuestion 3: \"Can you [thing they can't do]?\"\n- Why this works: [Shows missing capability]\n- Follow-up: [How you solve it]\n\n---\n\n### Proof Points\n\nCustomer wins:\n- \"[Customer name] switched from them and [result]\"\n- \"[Customer name] chose us over them for [reason]\"\n\nData points:\n- [X]% faster than them at [task]\n- [Y]% of their customers mention [pain point]\n- [Z]% of evaluators choose us when [criteria]\n\nReviews/validation:\n- G2 rating: [Yours vs theirs]\n- Analyst positioning: [How you compare]\n- Customer quotes: \"[Quote comparing you]\"\n\n---\n\n### When You'll Lose to Them\n\nBe honest about when they're better choice:They win if customer:\n- [Scenario where they're better fit]\n- [Another scenario]\n\nIf that's the case:\n- [How to reframe] or\n- [Gracefully walk away - not every deal is worth chasing]\n\n---\n\n### Discovery Questions\n\nAsk these to position yourself:\n\n1. \"What's most important: [your strength] or [their strength]?\"\n- If your strength: You're well-positioned\n- If their strength: You'll need different angle\n\n2. \"How quickly do you need [thing they're slow at]?\"\n- Reveals if your speed advantage matters\n\n3. \"Who on your team will use this daily?\"\n- Helps identify if your UX advantage matters\n\n---\n\n### Resources\n\nCase studies: [Link to customers who switched from them]\nProduct comparison: [Link to detailed comparison]\nDemo script: [Link to demo that highlights differentiation]\nROI calculator: [Link to tool showing cost comparison]\n\n---\n\n### Red Flags\n\nSigns you might lose:\n\n- [ ] They've already deployed competitor (switching cost high)\n- [ ] Buying criteria favor their strengths\n- [ ] Budget-driven decision (not value-driven)\n- [ ] Existing relationship with their parent company\n\nIf you see these:\n[Strategy to overcome or decision to walk]\n\n</battlecard_framework>\n\n<meta_guidance>\n\nBattlecard principles:Be honest\nDon't trash competitors\nAcknowledge what they do well\nMakes you credible\n\nFocus on fit, not features\n\"Better for X, worse for Y\"\nNot \"we're better at everything\"\n\nArm sales, don't overwhelm\nOne page ideal\nQuick reference, not encyclopedia\n\nUpdate regularly\nCompetitors change\nMonthly review minimum\n\nInclude proof\nDon't just claim superiority\nShow evidence (customers, data, reviews)\n\nPractice objection handling\nSales should rehearse responses\nNot read them for first time in meeting\n\nRemember:\nBest battlecard helps sales disqualify bad-fit prospects\nNot every deal is winnable\nKnowing when to walk is valuable\n\n</meta_guidance>\n\n</sales_battlecard>",
    "technique": "Competitive differentiation, objection pre-emption, value messaging",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Sales needs competitive positioning and objection handling"
  },
  {
    "name": "Cold Outreach Email",
    "category": "GTM",
    "prompt": "<cold_outreach>\n\n<outreach_inputs>\nWHO YOU'RE REACHING OUT TO:\n- Type: [ ] Potential customer [ ] Research participant [ ] Partner [ ] Influencer [ ] Other\n- What you know about them: [Role, company, relevant context]\n- How you found them: [Referral, LinkedIn, product usage, etc.]\n\nYOUR ASK:\n- [ ] Schedule a call\n- [ ] Get feedback\n- [ ] Demo product\n- [ ] Partnership discussion\n- [ ] Join beta/pilot\n- [ ] Other: [specify]\n\nYOUR CONTEXT:\n- Why them specifically: [What makes them relevant]\n- What you offer in return: [Value for them]\n- Timeline: [How urgent]\n</outreach_inputs>\n\n<outreach_framework>\n\nYou write cold emails that get responses. Your job: be brief, relevant, and valuable—not salesy.\n\nTHE FORMULA:Subject Line (Make them open)\n- Personal + Specific\n- Curiosity without clickbait\n- Under 50 characters\n\nLine 1 (Why you're reaching out to THEM specifically)\n- Show you did homework\n- Relevant observation\n- Genuine compliment (not generic)\n\nLines 2-3 (What's in it for them)\n- Value you provide\n- What they get, not what you want\n- Make it concrete\n\nLine 4 (The ask)\n- Single, clear CTA\n- Low friction\n- Specific time/format\n\nLine 5 (Make it easy to say yes)\n- Remove barriers\n- Offer optionality\n- Graceful exit\n\n---\n\n## YOUR EMAIL\n\nSubject: [Compelling, specific, personal]\n\nHi [Name],\n\n[Opening line showing you know who they are and why you're reaching out to them specifically]\n\n[What's in it for them - value proposition in 1-2 sentences]\n\n[The ask - clear and low-friction]\n\n[Make it easy - remove objections, offer flexibility]\n\n[Your name]\n[Title/Context]\n\n---\n\n## ALTERNATIVE VERSIONS\n\nShorter (3 sentences):\n[Ultra-brief version]\n\nWith social proof:\n[Version mentioning relevant customer/mutual connection]\n\nResearch angle:\n[Version positioning as learning, not selling]\n\n</outreach_framework>\n\n<outreach_patterns>\n\n### Pattern 1: Customer Research\n\nSubject: Quick question about [their workflow/pain point]\n\nHi [Name],\n\nSaw you're leading [team] at [company]. We're talking to [role] leaders about how they handle [specific challenge].\n\nWould you have 20 minutes to share what's working (or not) for your team? Not selling anything—genuinely trying to understand the problem before we build.\n\n[Calendar link] or just reply with a better time.\n\nThanks,\n[You]\n\n---\n\n### Pattern 2: Beta/Pilot Invitation\n\nSubject: Early access to [product] for [their company]\n\nHi [Name],\n\n[Mutual connection] mentioned you're dealing with [pain point]. We just built [solution] and looking for 10 companies to pilot it.\n\nWhat you get: Free access for 3 months + we'll customize it for your workflow. We get feedback.\n\nInterested? Here's a 2-min demo: [link]\n\nNo pressure—happy to explain more if timing's not right.\n\n[You]\n\n---\n\n### Pattern 3: Partnership Discussion\n\nSubject: [Your company] + [Their company]?\n\nHi [Name],\n\nWe both serve [audience]. You're great at [their strength], we're focused on [your strength].\n\nQuick idea: [Specific partnership concept, e.g., integration, co-marketing, referral]. Seems like a win-win for [shared customer].\n\nWorth 15 minutes to explore? [Calendar link]\n\n[You]\n\n---\n\n### Pattern 4: Product Demo (Warm Lead)\n\nSubject: [Feature] for [Their Company]\n\nHi [Name],\n\nNoticed you've been [signal of interest—e.g., checking our pricing page, reading blog].\n\n[Customer in their industry] is using [product] for [use case]. Seems similar to what [their company] does.\n\nWant to see it in action? 15-min demo: [calendar link]\n\nOr I can send you a sandbox to try yourself—just let me know.\n\n[You]\n\n---\n\n### Pattern 5: Re-engage Dormant User\n\nSubject: Miss you!\n\nHi [Name],\n\nYou signed up [X months] ago but haven't been active lately.\n\nCurious: Was it [potential reason 1] or [potential reason 2]? We've shipped [new feature] that might change things.\n\nWorth another look? Or should I stop bothering you? (Totally fine either way!)\n\n[You]\n\n</outreach_patterns>\n\n<meta_guidance>\n\nCold email principles:Shorter is better\n5 sentences max\nIf you can't fit value prop in subject line + 3 sentences, it's not clear\n\nPersonalization beats templates\nOne detail about them > generic spray-and-pray\n\"I saw you posted about X\" works\n\nLead with value\nNot \"I want to tell you about...\"\nBut \"You might want to know about...\"\n\nSingle CTA\nDon't give 5 options\nOne clear next step\n\nRemove friction\nCalendar link > \"let me know times\"\nSpecific ask > vague \"let's chat\"\n\nGive them an out\n\"No worries if not\" makes them more likely to respond\n\nFollow up (but not annoyingly)\nDay 1: Send\nDay 4: Bump (if no response)\nDay 8: Final follow-up\nThen stop\n\nRemember:\nYour email is interrupting their day.\nMake it worth their time.\n\n80% of cold emails are terrible because they're about YOU.\nGood cold emails are about THEM.\n\n</meta_guidance>\n\n</cold_outreach>",
    "technique": "Value-first messaging, personalization at scale, clear CTA",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Need to reach out to potential customer, partner, or user for research/feedback"
  },
  {
    "name": "Write Blog Post",
    "category": "GTM",
    "prompt": "<blog_post_writer>\n\n<post_inputs>\nTOPIC:\n[What you're writing about]\n\nPURPOSE:\n- [ ] Announce product/feature\n- [ ] Thought leadership/opinion\n- [ ] Educational/how-to\n- [ ] Company update\n- [ ] Customer story\n\nAUDIENCE:\n- Who's reading: [Customers, prospects, industry, general]\n- What they care about: [Their interests/pain points]\n- Technical level: [Non-technical, somewhat technical, very technical]\n\nYOUR ANGLE:\n- What's your unique take: [Your POV or insight]\n- Why should they care: [Hook/relevance]\n\nTARGET LENGTH:\n- [ ] Short (500-800 words)\n- [ ] Medium (800-1500 words)\n- [ ] Long (1500-3000 words)\n\nSEO KEYWORDS (optional):\n[Terms you want to rank for]\n</post_inputs>\n\n<post_framework>\n\nYou write blog posts that people actually read. Your job: hook them, teach them something, leave them satisfied.\n\nTHE STRUCTURE:HEADLINE (Make them click)\n- Promise specific benefit\n- Use numbers or \"how to\"\n- Under 60 characters for SEO\n\nINTRO (Hook in 3 sentences)\n- Sentence 1: Relatable problem or surprising fact\n- Sentence 2: Why this matters now\n- Sentence 3: What they'll learn\n\nBODY (Deliver on promise)\n- Break into scannable sections\n- Use subheadings every 2-3 paragraphs\n- Examples, not just theory\n- Actionable takeaways\n\nCONCLUSION (Stick the landing)\n- Recap key points\n- Clear next step\n- Call to action (if appropriate)\n\n---\n\n## YOUR BLOG POST\n\nHeadline: [Compelling title with benefit]\n\nSubheading (optional): [Clarify or expand on headline]\n\n---\n\n### [Opening hook]\n\n[3-4 sentences that grab attention and set up the problem/topic]\n\n### [First Main Point]\n\n[Content with examples]\n\nKey takeaway: [Actionable insight]\n\n### [Second Main Point]\n\n[Content with examples]\n\nKey takeaway: [Actionable insight]\n\n### [Third Main Point if needed]\n\n[Content]\n\n### Conclusion\n\n[Tie it together, give them the \"so what\"]\n\nWhat to do next:\n[Clear CTA or next step]\n\n---\n\n## SEO OPTIMIZATION\n\nMeta description (155 characters):\n[Summary with keyword]\n\nSuggested images:\n- [Hero image concept]\n- [Diagram/screenshot for section X]\n\nInternal links:\n- [Related post 1]\n- [Related post 2]\n\n</post_framework>\n\n<post_templates>\n\n### Template 1: Product Launch Post\n\nHeadline: Introducing [Feature]: [Benefit in 3-5 words]\n\nIntro:\nWe heard you. [Pain point] has been your #1 request for [time period]. Today we're launching [feature] to make [outcome] possible.\n\nBody:\n- What it does: [Simple explanation]\n- Why we built it: [Problem it solves]\n- How it works: [Step-by-step with screenshots]\n- Who it's for: [Use cases]\n- What's next: [Future plans]\n\nConclusion:\nTry it today. [Link]. We'd love your feedback.\n\n---\n\n### Template 2: How-To Post\n\nHeadline: How to [Achieve Outcome] in [Timeframe]\n\nIntro:\n[Pain point]. We've helped [X customers] do [outcome]. Here's exactly how.\n\nBody:Step 1: [Action]\n[What to do and why]\n\nStep 2: [Action]\n[What to do and why]\n\nStep 3: [Action]\n[What to do and why]\n\nCommon mistakes:\n- [Mistake and how to avoid]\n\nConclusion:\n[Summary checklist]\nStart with [first step]. You'll see [result] within [timeframe].\n\n---\n\n### Template 3: Thought Leadership\n\nHeadline: [Contrarian Take] About [Topic]\n\nIntro:\nEveryone says [conventional wisdom]. They're wrong. Here's why.\n\nBody:The problem with conventional thinking:\n[What's broken about current approach]\n\nWhat we learned:\n[Your experience/data]\n\nA better way:\n[Your alternative approach with evidence]\n\nWhat this means for you:\n[Practical application]\n\nConclusion:\n[Call to rethink + invitation to discuss]\n\n---\n\n### Template 4: Customer Story\n\nHeadline: How [Customer] [Achieved Result] with [Product]\n\nIntro:\n[Customer] had a problem: [specific challenge]. Here's how they solved it.\n\nBody:The challenge:\n[What wasn't working]\n\nWhat they tried:\n[Previous failed attempts]\n\nThe solution:\n[How they used your product]\n\nThe results:\n[Specific metrics and outcomes]\n\nWhat they learned:\n[Customer quote and advice]\n\nConclusion:\nWant similar results? [CTA]\n\n</post_templates>\n\n<meta_guidance>\n\nBlog post principles:Write like you talk\nConversational > formal\nShort sentences > long ones\nActive voice > passive\n\nShow, don't tell\nExamples > abstractions\nScreenshots > descriptions\nData > claims\n\nMake it scannable\n- Subheadings every 2-3 paragraphs\n- Bullet points for lists\n- Bold key points\n- White space is your friend\n\nHook early\nFirst 3 sentences determine if they read more\nDon't bury the lead\n\nEnd strong\nRecap + next step\nDon't just trail off\n\nSEO basics:\n- Keyword in headline\n- Keyword in first paragraph\n- Use keyword naturally 3-5 times\n- Alt text on images\n- Meta description\n\nRemember:\nPeople skim before reading.\nMake skimming satisfying.\n\nIf they only read subheadings, they should get the gist.\n\n</meta_guidance>\n\n</blog_post_writer>",
    "technique": "SEO-aware structure, narrative arc, audience engagement",
    "tools": " Claude Projects, ChatGPT Projects",
    "useCase": "Need to write thought leadership, product launch, or educational content"
  },
  {
    "name": "Create Social Media Posts",
    "category": "GTM",
    "prompt": "<social_media_posts>\n\n<social_inputs>\nWHAT YOU'RE POSTING ABOUT:\n[Feature launch, blog post, milestone, thought, engagement]\n\nPLATFORMS:\n- [ ] LinkedIn (professional)\n- [ ] Twitter/X (brief, punchy)\n- [ ] Product Hunt (launch-focused)\n- [ ] Company blog promo\n- [ ] Other: [specify]\n\nTONE:\n- [ ] Professional/thoughtful\n- [ ] Excited/celebratory\n- [ ] Educational/helpful\n- [ ] Casual/conversational\n\nYOUR GOAL:\n- [ ] Drive clicks/traffic\n- [ ] Engagement/discussion\n- [ ] Awareness/reach\n- [ ] Conversions/signups\n\nCONTEXT:\n[Brief description of what you're promoting/discussing]\n</social_inputs>\n\n<social_framework>\n\nYou write social posts that stop the scroll. Your job: hook in first line, deliver value, make sharing easy.\n\nPLATFORM RULES:LinkedIn:\n- First 2 lines matter (preview)\n- Longer form OK (1-3 paragraphs)\n- Professional tone, but human\n- Questions drive engagement\n\nTwitter/X:\n- First 8 words are the hook\n- Thread for depth (1 idea per tweet)\n- Visual > text only\n- Hashtags sparingly (1-2 max)\n\nProduct Hunt:\n- Clear value prop first line\n- Feature bullets\n- Social proof\n- Direct CTA\n\n---\n\n## YOUR POSTS\n\n### LinkedIn Post\n\n[First line hook - shows up in preview]\n[Rest of content - 2-3 short paragraphs]\n\n[CTA or question for engagement]\n\n---\n\n### Twitter/X Post\n\n[Tweet 1 - Hook]\n\n[Tweet 2 - Expand if needed]\n\n[Tweet 3 - Payoff/CTA]\n\n---\n\n### Short Version (All Platforms)\n\n[One sentence hook + link]\n\n</social_framework>\n\n<social_patterns>\n\n### Pattern 1: Feature Announcement\n\nLinkedIn:\nWe just shipped something our customers have been asking for.\n\n[Feature name] is now live. Here's what it does:\n→ [Benefit 1]\n→ [Benefit 2]\n→ [Benefit 3]\n\nBuilt because [customer story or pain point].\n\nTry it: [link]\n\n---\n\nTwitter/X:\nNew: [Feature name] 🎉\n\nNow you can [concrete action] in [timeframe/way].\n\n[Customer/team] has been asking for this for months. It's here.\n\n[Link]\n\n---\n\n### Pattern 2: Thought Leadership\n\nLinkedIn:\nHot take: [Contrarian opinion]\n\nEveryone's focused on [common approach]. But [your insight about what actually matters].\n\nHere's what I've learned from [experience/data]:\n\n[3 bullet points]\n\nWhat am I missing? [Genuine question to drive comments]\n\n---\n\nTwitter/X:\nUnpopular opinion: [Statement]\n\n[Thread explaining why]\n\n1/ [Setup the problem]\n\n2/ [Your experience]\n\n3/ [The insight]\n\n4/ [What this means]\n\nDisagree? Tell me why.\n\n---\n\n### Pattern 3: Educational/How-To\n\nLinkedIn:\n[Persona] often struggle with [problem].\n\nHere's a simple framework I use:\n\n1. [Step] - [Why it matters]\n2. [Step] - [Why it matters]\n3. [Step] - [Why it matters]\n\nLearned this from [source/experience].\n\nFull guide: [link]\n\n---\n\nTwitter/X:\nA thread on [topic] everyone should know:\n\n[1-2 sentence hook explaining why this matters]\n\n👇\n\n[Then thread with one insight per tweet]\n\n---\n\n### Pattern 4: Milestone/Celebration\n\nLinkedIn:\nSmall milestone: [achievement] 🎉\n\nWhat makes this meaningful: [why it matters beyond the number]\n\nThanks to [team/customers] who made this possible.\n\nWhat's next: [future focus]\n\n---\n\nTwitter/X:\nWe hit [milestone]!\n\nThank you to everyone who [action - used, supported, built, etc.]\n\nNext up: [what's coming]\n\n[GIF or image]\n\n---\n\n### Pattern 5: Content Promotion\n\nLinkedIn:\nWrote about [topic] after [recent experience/observation].\n\nKey points:\n- [Takeaway 1]\n- [Takeaway 2]\n- [Takeaway 3]\n\nFull post: [link]\n\nWorth your time if you [target reader situation].\n\n---\n\nTwitter/X:\n📝 New post: [Title]\n\nTL;DR:\n- [Point]\n- [Point]\n- [Point]\n\n[Link]\n\n</social_patterns>\n\n<meta_guidance>\n\nSocial media principles:First line = everything\nHook in 8 words or lose them\nDon't waste it on preamble\n\nVisual > text\nImage, GIF, or video outperforms text-only\nScreenshots with annotations work well\n\nShorter > longer\nException: LinkedIn stories\nBut even there, break into short paragraphs\n\nOne idea per post\nDon't cram 5 things into one post\nMultiple posts > one overwhelming post\n\nCTA or question\nEnd with clear action or invitation to engage\n\"What do you think?\" drives comments\n\nTiming matters\nLinkedIn: Weekday mornings\nTwitter: Throughout day\nTest what works for your audience\n\nAuthenticity > polish\nTypo-free important\nBut overly corporate = ignored\n\nRemember:\n\nSocial is conversation, not broadcast.\nWrite like you're talking to one person, not a crowd.\n\nTest, learn, iterate. Every audience is different.\n\n</meta_guidance>\n\n</social_media_posts>",
    "technique": "Platform-specific formatting, hook optimization, CTA clarity",
    "tools": " Claude Projects, ChatGPT Projects",
    "useCase": "Need to announce feature, share content, or engage audience on social"
  },
  {
    "name": "Landing Page Copy\n",
    "category": "GTM",
    "prompt": "<landing_page_copy>\n\n<page_inputs>\nWHAT'S THE PAGE FOR:\n- [ ] Product homepage\n- [ ] Feature page\n- [ ] Pricing page\n- [ ] Landing page (ads, campaign)\n- [ ] Signup/conversion page\n\nTARGET AUDIENCE:\n[Who's landing here and what they care about]\n\nYOUR OFFER:\n[What you're selling/promoting]\n\nUNIQUE VALUE:\n[Why choose you vs alternatives]\n\nGOAL:\n- [ ] Signups\n- [ ] Demo requests\n- [ ] Sales calls\n- [ ] Downloads\n- [ ] Awareness/education\n\nOBJECTIONS TO HANDLE:\n[Common hesitations or concerns]\n</page_inputs>\n\n<copy_framework>\n\nYou write landing page copy that converts. Your job: hook → explain → prove → convert.\n\nTHE STRUCTURE:Above the fold:\n- Headline (value prop in 5-10 words)\n- Subheadline (expand on benefit)\n- Visual (product screenshot or hero image)\n- Primary CTA (clear, action-oriented)\n\nThe Problem (Build tension):\n- What pain exists today\n- Cost of not solving\n\nThe Solution (Your product):\n- How it works (simply)\n- Key features as benefits\n- Why it's different\n\nProof (Build trust):\n- Social proof (logos, testimonials, stats)\n- Case studies or results\n- Trust signals (security, certifications)\n\nHow It Works (Reduce friction):\n- 3-step process\n- Show don't tell\n\nPricing (Be clear):\n- Transparent pricing if possible\n- Show value, not just cost\n\nFinal CTA (Convert):\n- Repeat primary CTA\n- Handle last objections\n- Low-risk trial or guarantee\n\n---\n\n## YOUR LANDING PAGE COPY\n\n### Hero Section\n\nHeadline:\n[Value proposition - what outcome do you deliver?]\n\nSubheadline:\n[Expand - who is this for, what makes it unique?]\n\nCTA Button: [Action-oriented text]\n\n---\n\n### The Problem\n\n[Pain point headline]\n\n[2-3 sentences about the current painful state]\n\nSound familiar?\n\n---\n\n### The Solution\n\n[Product name]: [Benefit-driven description]\n\n[How it works in simple terms]\n\nKey Benefits:\n\n✓ [Benefit 1]: [Specific outcome]\n✓ [Benefit 2]: [Specific outcome]\n✓ [Benefit 3]: [Specific outcome]\n\n---\n\n### Social Proof\n\nTrusted by [number] [type of companies]\n\n[Customer logos]\n\n> \"[Testimonial quote about specific result]\"\n> — [Name, Title, Company]\n\n---\n\n### How It Works\n\nGet started in 3 steps:1. [Action]\n[Brief description - make it sound easy]\n\n2. [Action]\n[Brief description]\n\n3. [Action]\n[Brief description - emphasize speed to value]\n\n---\n\n### Features → Benefits\n\n[Feature name]\n[What it is and why it matters to user]\n\n[Feature name]\n[Benefit-focused description]\n\n---\n\n### Objection Handling\n\n\"Is it secure?\"\n[How you handle security with specifics]\n\n\"Will it work for my team?\"\n[Who it's built for, flexibility]\n\n\"What if it doesn't work out?\"\n[Trial, guarantee, easy cancellation]\n\n---\n\n### Final CTA\n\nReady to [outcome]?\n\n[CTA Button: Action text]\n\nNo credit card required • 14-day free trial • Cancel anytime\n\n</copy_framework>\n\n<copy_patterns>\n\n### Pattern 1: Problem-Agitate-Solve\n\nHeadline: Stop [painful activity]. Start [desired outcome].\n\nProblem:\nYou're spending [X hours/week] on [manual task]. Your team is frustrated. Results are inconsistent.\n\nAgitate:\nMeanwhile, your competitors are [moving faster/winning deals/scaling]. Every day you wait costs you [specific cost].\n\nSolve:\n[Product] automates [task], so you can [benefit]. [Social proof]. Get started in 5 minutes.\n\n---\n\n### Pattern 2: Before/After\n\nHeadline: From [Current State] to [Desired State] in [Timeframe]\n\nBefore: [Paint picture of current pain]\n- [Pain point]\n- [Pain point]\n- [Pain point]\n\nAfter: [Show transformed state]\n- [Benefit]\n- [Benefit]\n- [Benefit]\n\nHow: [Product name] gives you [key capabilities]\n\n---\n\n### Pattern 3: The Promise\n\nHeadline: [Specific Outcome] Without [Common Objection]\n\nExample: \"Scale your sales without hiring 10 reps\"\n\nThe Promise:\nWe help [target audience] achieve [outcome] without [typical cost/hassle].\n\nHow we deliver:\n[3 key differentiators]\n\nProof:\n[Customer results]\n\n---\n\n### Pattern 4: The Comparison\n\nHeadline: Like [Familiar Thing] but [Key Difference]\n\nExample: \"Like Salesforce but actually easy to use\"\n\nWhat you know:\n[Familiar solution] works but [pain points]\n\nWhat's different:\nWe took the best parts ([strengths]) and fixed what's broken ([innovations]).\n\nResult:\n[Customer testimonial showing the difference]\n\n</copy_patterns>\n\n<meta_guidance>\n\nLanding page copy principles:Headline is 80% of success\n5 seconds to hook them\nClear benefit > clever wordplay\n\nFeatures tell, benefits sell\nNot: \"Real-time dashboard\"\nBut: \"Know what's working before your morning coffee\"\n\nShow don't tell\nScreenshots > descriptions\nVideos > screenshots\nDemos > videos\n\nSocial proof everywhere\nLogos above the fold\nTestimonials throughout\nStats that prove value\n\nRemove friction\n\"No credit card required\"\n\"14-day free trial\"\n\"Cancel anytime\"\n\nOne clear CTA\nDon't give 5 options\nOne primary action\nRepeat it 2-3 times\n\nMobile-first\nMost traffic is mobile\nHeadline must work on small screen\n\nRemember:\n\nPeople skim landing pages.\nMake skimming get them 80% of the message.\n\nThen the full read converts them.\n\n</meta_guidance>\n\n</landing_page_copy>",
    "technique": "Benefit-driven copywriting, objection handling, conversion optimization",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Need copy for product page, feature page, or landing page"
  },
  {
    "name": "Email Campaign",
    "category": "GTM",
    "prompt": "<email_campaign>\n\n<campaign_inputs>\nCAMPAIGN TYPE:\n- [ ] Onboarding (new users)\n- [ ] Feature adoption (drive usage)\n- [ ] Re-engagement (dormant users)\n- [ ] Nurture (leads/trials)\n- [ ] Announcement (all users)\n\nAUDIENCE:\n[Who's receiving this - segment, behavior, stage]\n\nGOAL:\n[What action you want them to take]\n\nSEQUENCE:\n- [ ] One-off email\n- [ ] 3-email sequence\n- [ ] 5-email sequence\n- [ ] Ongoing drip\n\nYOUR PRODUCT CONTEXT:\n[Brief description of product/feature]\n</campaign_inputs>\n\n<campaign_framework>\n\nYou design email campaigns that drive action. Your job: guide users through a journey, not spam their inbox.\n\nSEQUENCE STRUCTURE:Email 1: Welcome/Introduce\n- What they signed up for\n- Quick win / first value\n- Clear next step\n\nEmail 2: Education\n- How to get more value\n- Common use cases\n- Success stories\n\nEmail 3: Social proof\n- How others use it\n- Results/testimonials\n- Address objections\n\nEmail 4: Urgency/Scarcity (if applicable)\n- Time-limited offer\n- What they're missing\n- Easy action\n\nEmail 5: Last chance (if applicable)\n- Final reminder\n- Easy out (unsubscribe)\n- Alternative path\n\nTIMING:\n- Day 1: Email 1\n- Day 3: Email 2\n- Day 7: Email 3\n- Day 14: Email 4 (if no action)\n- Day 21: Email 5 (if no action)\n\n---\n\n## YOUR EMAIL SEQUENCE\n\n### Email 1: [Name/Purpose]\n\nSubject: [Compelling subject line]\n\nPreview text: [First line visible in inbox]\n\n---\n\nHi [Name],\n\n[Opening - acknowledge their action/situation]\n\n[Main message - value or education in 2-3 short paragraphs]\n\n[CTA - single clear action]\n\n[Closing - set expectations for next communication]\n\n[Signature]\n\n---\n\n### Email 2: [Name/Purpose]\n\n[Same structure]\n\n---\n\n[Continue pattern for each email in sequence]\n\n</campaign_framework>\n\n<campaign_patterns>\n\n### Pattern 1: Onboarding Sequence (New Users)\n\nEMAIL 1 - Day 1: Welcome\n\nSubject: Welcome to [Product]! Here's how to start\n\nHi [Name],\n\nThanks for signing up! You're now part of [X] teams using [Product] to [outcome].\n\nHere's the fastest way to get value:\n\nStep 1: [Action - link]\nStep 2: [Action - link]\nStep 3: [Action - link]\n\nMost users see results within [timeframe]. Need help? Reply to this email—I'm here.\n\n[Your name]\n\n---\n\nEMAIL 2 - Day 3: Quick Win\n\nSubject: [Name], try this quick win\n\nHi [Name],\n\nQuick question: Have you tried [specific feature] yet?\n\nIt's the fastest way to [benefit]. Here's how:\n\n[1-2-3 steps with screenshots]\n\n[Customer] used this and [specific result].\n\nTry it now: [CTA button]\n\n---\n\nEMAIL 3 - Day 7: Value Expansion\n\nSubject: Getting the most out of [Product]\n\nHi [Name],\n\nYou've been using [Product] for a week. Here are 3 ways to get even more value:\n\n1. [Feature]: [Use case and benefit]\n2. [Feature]: [Use case and benefit]\n3. [Feature]: [Use case and benefit]\n\nWhich sounds most useful? Just reply and I'll send you a guide.\n\n---\n\n### Pattern 2: Feature Adoption (Existing Users)\n\nEMAIL 1 - Announcement\n\nSubject: New: [Feature] is here\n\nHi [Name],\n\nYou asked, we built it. [Feature] is now live.\n\nWhat it does: [One sentence]\n\nWhy it matters for you: [Specific benefit for their use case]\n\nHow to use it: [Link to guide]\n\n[CTA: Try it now]\n\n---\n\nEMAIL 2 - Education\n\nSubject: 3 ways to use [Feature]\n\nHi [Name],\n\nSaw you haven't tried [Feature] yet. Here are 3 ways teams are using it:\n\nUse case 1: [Description + result]\nUse case 2: [Description + result]\nUse case 3: [Description + result]\n\nWhich fits your workflow? [CTA]\n\n---\n\nEMAIL 3 - Social Proof\n\nSubject: How [Customer] uses [Feature]\n\nHi [Name],\n\n[Customer], similar to your team, had [problem].\n\nThey used [Feature] to [solution]. Result: [specific metric].\n\n> \"[Testimonial]\" — [Name, Title]\n\nWant similar results? [CTA]\n\n---\n\n### Pattern 3: Re-engagement (Dormant Users)\n\nEMAIL 1 - We Miss You\n\nSubject: [Name], is everything OK?\n\nHi [Name],\n\nNoticed you haven't logged in since [date]. Everything alright?\n\nA lot has changed:\n- [New feature]\n- [Improvement]\n- [New capability]\n\nWorth another look? [CTA]\n\nOr should I stop emailing? (Totally fine—just let me know)\n\n---\n\nEMAIL 2 - What Changed\n\nSubject: What's new since you've been away\n\nHi [Name],\n\nQuick update on what you've missed:\n\n[3 bullets of biggest changes with links]\n\nIf something here sounds interesting, give it another try: [CTA]\n\nNot interested? [Unsubscribe link - make it easy]\n\n---\n\nEMAIL 3 - Final Goodbye\n\nSubject: Last email from me\n\nHi [Name],\n\nThis is my last email. Figured you've moved on and that's totally OK.\n\nIf I'm wrong and you want to give [Product] another shot, door's always open: [CTA]\n\nOtherwise, thanks for giving us a try. Best of luck with [their goal].\n\n[Signature]\n\n---\n\n### Pattern 4: Trial Nurture (Convert to Paid)\n\nEMAIL 1 - Day 1: Welcome\n\n[Same as onboarding]\n\n---\n\nEMAIL 2 - Day 5: Quick Value\n\nSubject: Are you seeing results yet?\n\nHi [Name],\n\nYou're 5 days into your trial. Quick question: Are you seeing the value you expected?\n\nMost successful trials follow this path:\n- Week 1: [Milestone]\n- Week 2: [Milestone]\n\nYou're on track if you've [specific action]. Need help? Reply here.\n\n---\n\nEMAIL 3 - Day 10: Social Proof\n\nSubject: How [similar customer] uses [Product]\n\n[Customer story with results]\n\nYou have 4 days left in trial. Ready to keep going? [CTA to upgrade]\n\n---\n\nEMAIL 4 - Day 12: Urgency\n\nSubject: [Name], 2 days left in your trial\n\nHi [Name],\n\nYour trial ends in 2 days. Don't lose:\n- [Data/setup you'll lose]\n- [Feature access]\n- [Your work]\n\nContinue with [plan]: [CTA]\n\nQuestions? Let's talk: [Calendar link]\n\n---\n\nEMAIL 5 - Day 14: Last Chance\n\nSubject: Last call - trial ends today\n\nHi [Name],\n\nYour trial ends at midnight. Two options:\n\n1. Upgrade now: Keep everything [CTA]\n2. Need more time? Reply and I'll extend you\n\nDon't want to continue? No problem—we'll delete your data in 30 days.\n\n</campaign_patterns>\n\n<meta_guidance>\n\nEmail campaign principles:One email = one goal\nDon't put 5 CTAs in one email\nOne clear action per email\n\nSubject line > body\n40% open rate difference between good/bad subject\nSpend time on subjects\n\nPreview text matters\nFirst sentence shows in inbox\nMake it count\n\nPersonal > corporate\nFrom a person, not \"Team@company\"\nReply-able email address\n\nShort > long\nIdeal: 3-4 short paragraphs\nException: Story-driven emails\n\nTiming matters\nDon't email daily (annoying)\nDon't wait 2 weeks (forgotten)\n3-7 day gaps work well\n\nUnsubscribe is OK\nMake it easy\nBetter than spam complaints\n\nTest everything\nSubject lines\nSend times\nCTAs\nCopy\n\nRemember:\n\nEmail is permission-based.\nDon't abuse the privilege.\n\nIf you wouldn't want this email, don't send it.\n\n</meta_guidance>\n\n</email_campaign>",
    "technique": "Sequence design, engagement triggers, conversion paths",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need email sequence for feature adoption, onboarding, or nurture"
  },
  {
    "name": "Demo Script",
    "category": "GTM",
    "prompt": "<demo_script>\n\n<demo_inputs>\nDEMO TYPE:\n- [ ] First call (introductory)\n- [ ] Deep dive (technical)\n- [ ] Executive (high-level)\n- [ ] Hands-on (let them drive)\n\nAUDIENCE:\n- Roles: [Who's attending]\n- Industry: [Their vertical if relevant]\n- Company size: [Impacts what you show]\n- Known pain points: [What you learned in discovery]\n\nYOUR PRODUCT:\n[Brief description]\n\nDEMO LENGTH:\n- [ ] 15 minutes\n- [ ] 30 minutes\n- [ ] 60 minutes\n\nGOAL:\n- [ ] Next meeting\n- [ ] Trial signup\n- [ ] Close deal\n- [ ] Technical validation\n</demo_inputs>\n\n<demo_framework>\n\nYou write demo scripts that sell outcomes, not features. Your job: show how the product solves THEIR problem, not catalog everything it does.\n\nTHE FLOW:1. Opening (2-3 min)\n- Confirm agenda\n- Recap their situation\n- Set expectations\n\n2. Discovery/Confirmation (5 min)\n- Validate what you learned\n- Uncover new info\n- Get them talking\n\n3. The Demo (Main chunk)\n- Show value first\n- Walk through their workflow\n- Handle objections as they arise\n\n4. Closing (5 min)\n- Recap value\n- Answer questions\n- Clear next steps\n\n---\n\n## YOUR DEMO SCRIPT\n\n### Pre-Demo Prep\n\nBefore the call, know:\n- [ ] Their role and goals\n- [ ] Pain points (from discovery or research)\n- [ ] Decision criteria\n- [ ] Timeline and urgency\n- [ ] Who else is involved\n\nSetup checklist:\n- [ ] Demo environment ready\n- [ ] Relevant data loaded\n- [ ] Screen sharing tested\n- [ ] Backup plan if tech fails\n\n---\n\n### Opening (First 3 minutes)\n\n\"Thanks for joining. Before I show you anything, I want to make sure I use your time well.\"Confirm agenda:\n\"Here's what I thought we'd cover today:\n- Quick intro to [product]\n- Walk through how it solves [their problem]\n- Answer your questions\n- Discuss next steps\n\nDoes that work? Anything else you want to see?\"\n\nRecap their situation:\n\"From our last conversation, sounds like you're dealing with [problem]. Did I get that right? Anything change since we talked?\"\n\nSet expectations:\n\"I'm going to show you [specific workflows], not every feature. If you want to see something specific, just interrupt me.\"\n\n---\n\n### Discovery Questions (Next 5 minutes)\n\nDon't skip this. Even if you've talked before.\n\n\"Before I jump in, a few quick questions:\"\n\nCurrent state:\n- \"Walk me through how you handle [problem] today.\"\n- \"What's working? What's not?\"\n- \"How much time does this take?\"\n\nDecision criteria:\n- \"What would make this a win for you?\"\n- \"What would make you choose us vs [alternative]?\"\n- \"Who else needs to sign off on this?\"\n\nUrgency:\n- \"What's driving the timeline?\"\n- \"What happens if you don't solve this?\"\n\n[Take notes. Use their words in the demo.]\n\n---\n\n### The Demo (Main content)\n\nRule: Show outcomes, not features.Structure:Part 1: The \"Wow\" Moment (Show value fast)\n\n\"Let me show you the end result first, then I'll walk you through how it works.\"\n\n[Show the outcome they want - dashboard, report, result]\n\n\"This is what [their goal] looks like when you're using [product]. Notice [specific thing that addresses their pain].\"\n\n[Pause for reaction. Let them respond.]\n\n---\n\nPart 2: Their Workflow\n\n\"Now let me show you how you'd actually use this. I'll walk through [their specific use case].\"\n\nStep 1: [Their process step]\n\n[Show how product handles this]\n\n\"Today you're doing [their current method]. With us, you'd [easier way]. Saves you [time/effort].\"\n\nStep 2: [Next step]\n\n[Continue pattern - always connecting to their workflow]\n\nStep 3: [Outcome]\n\n\"And here's where you end up: [result they wanted].\"\n\n---\n\nPart 3: Handle Objections ProactivelyAddress concerns before they ask:\n\n\"A question we often get: [common objection]\"\n\n\"Here's how we handle that: [answer with demo]\"\n\nCommon objections to address:\n- Integration with their tech stack\n- Learning curve / ease of use\n- Scalability\n- Security / compliance\n\n---\n\nPart 4: The Differentiator\n\n\"One thing that makes us different: [unique capability]\"\n\n[Show the feature competitors don't have]\n\n\"[Customer name] uses this for [use case]. They told us [specific result].\"\n\n---\n\n### Handling Questions (Throughout)\n\nWhen they ask questions:\n\n\"Great question. Let me show you.\"\n\n[Demo the answer, don't just tell]\n\nIf you don't know:\n\n\"I don't know off the top of my head. Let me find out and get back to you by [specific time].\"\n\n[Write it down visibly so they see you captured it]\n\n---\n\n### Closing (Last 5 minutes)\n\nRecap value:\n\n\"So to recap what you saw:\n- [Benefit 1 in their words]\n- [Benefit 2 in their words]\n- [Benefit 3 in their words]\"\n\nCheck understanding:\n\n\"Does this solve [the problem they mentioned]?\"\n\n[Wait for confirmation]\n\nAddress concerns:\n\n\"What questions do you have?\"\n\n[Answer thoroughly]\n\n\"Any concerns about moving forward?\"\n\n[Surface objections now, not later]\n\n---\n\n### Next Steps (Always get commitment)\n\nIf they're ready:\n\n\"Great. Next step: [specific action]\"\n- \"I'll send trial access today\"\n- \"Let's get contract over to you\"\n- \"I'll schedule technical deep-dive\"\n\nIf they need more:\n\n\"What do you need to see to move forward?\"\n\n[Get specific answer]\n\n\"OK, let's schedule [that thing] for [specific date].\"\n\nIf it's not a fit:\n\n\"Sounds like [their situation] might be better served by [alternative]. Here's why...\"\n\n[Be honest. Don't waste their time or yours.]\n\n---\n\n### Follow-Up (Within 24 hours)\n\nEmail template:\n\nSubject: [Their Company] demo - next steps\n\nHi [Name],\n\nThanks for your time today. Here's what we covered:\n- [Key point 1]\n- [Key point 2]\n\nNext steps:\n- [ ] [Action you're taking] - [By when]\n- [ ] [Action they're taking] - [By when]\n\nQuestions from the call:\n- [Question]: [Answer or \"Getting answer by [date]\"]\n\nResources:\n- [Link to relevant case study]\n- [Link to documentation]\n\nLet's connect [specific day] to [next step].\n\n[Your name]\n\n</demo_framework>\n\n<demo_patterns>\n\n### Pattern 1: Executive Demo (High-level)\n\nFocus: Business outcomes, not features\nLength: 15-20 minutes max\nTone: Strategic\n\nScript:\n\n\"I'll keep this high-level. Three things:\n\n1. The problem: [Business impact of their pain]\n2. The solution: [How you solve it]\n3. The result: [ROI they can expect]\n\nThen happy to dive deeper on anything.\"\n\n[Show dashboard/summary view only]\n[Use customer metrics and stories]\n[No feature demos unless asked]\n\n---\n\n### Pattern 2: Technical Demo (Deep dive)\n\nFocus: How it works, not why\nLength: 45-60 minutes\nTone: Detailed\n\nScript:\n\n\"I'll show you the full architecture and answer technical questions.\"\n\n[Walk through:]\n- Data flow\n- Integration points\n- Security model\n- API capabilities\n- Customization options\n\n[Have technical docs ready]\n[Screen share IDE or backend if needed]\n\n---\n\n### Pattern 3: Competitive Displacement\n\nFocus: Why switch from competitor\nLength: 30 minutes\nTone: Consultative\n\nScript:\n\n\"You're using [Competitor]. Let me show you what's different.\"\n\n[Head-to-head comparison:]\n- \"Here's how you do [task] with them...\"\n- \"Here's how you'd do it with us...\"\n- \"Notice: [specific advantage]\"\n\n[Use trap questions]\n[Show migration path]\n[Address switching costs]\n\n</demo_patterns>\n\n<meta_guidance>\n\nDemo script principles:Discovery > Demo\nDon't demo until you know their pain\n5 minutes of discovery saves 20 minutes of irrelevant demo\n\nTheir workflow > feature tour\nShow how it fits their day\nNot every feature you have\n\nOutcomes > features\n\"You'll save 10 hours/week\"\nNot \"We have automation\"\n\nQuestions = engagement\nPause and ask\n\"Does this make sense?\"\n\"Is this what you expected?\"\n\nSilence is OK\nLet them react\nDon't fill every gap\n\nHandle objections early\nDon't let them fester\nAddress concerns as they arise\n\nAlways get next step\nNever end with \"I'll follow up\"\nGet specific commitment\n\nRemember:\n\nBest demos are conversations, not presentations.\n\nIf you're talking 90% of the time, you're doing it wrong.\n\n</meta_guidance>\n\n</demo_script>",
    "technique": "Discovery-first approach, value-oriented flow, objection anticipation",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need structured script for product demo or sales presentation"
  },
  {
    "name": "Cohort Analysis",
    "category": "Analytics",
    "prompt": "<cohort_analysis>\n\n<cohort_inputs>\nYOUR DATA:\n[Paste cohort data, or describe what you have access to]\n\nExamples:\n- Retention by week/month\n- Feature adoption by cohort\n- Revenue by cohort\n- Engagement metrics over time\n\nCONTEXT:\n- What changed: [Product changes, marketing shifts]\n- Time period: [Date range you're analyzing]\n- Cohort definition: [Signup week, month, channel, etc.]\n\nWHAT YOU WANT TO UNDERSTAND:\n- [ ] Retention trends\n- [ ] Feature adoption\n- [ ] Revenue/LTV patterns\n- [ ] Product improvements impact\n- [ ] Segment differences\n</cohort_inputs>\n\n<analysis_framework>\n\nYou analyze cohorts to find patterns others miss. Your job: turn retention tables into insights that drive decisions.\n\nTHE PROCESS:1. Understand the data\nWhat are you looking at?\n\n2. Spot the patterns\nWhat's changing over time?\n\n3. Form hypotheses\nWhy are you seeing these patterns?\n\n4. Recommend actions\nWhat should you do based on this?\n\n---\n\n## COHORT ANALYSIS\n\n### What You're Looking At\n\nCohort definition: [How users are grouped]\nMetric being tracked: [Retention, revenue, engagement, etc.]\nTime periods: [Week 0, Week 1, Week 2... or Month 0, Month 1...]\nTotal cohorts analyzed: [How many cohorts]\n\n---\n\n### Key Patterns\n\nPATTERN 1: [Name the trend]What you see:\n[Describe the pattern in the data]\n\nExample: \"Recent cohorts (Jan-Mar) retain 20% better at Week 4 than older cohorts (Oct-Dec)\"\n\nWhy this matters:\n[Business implication]\n\nPossible causes:\n- [Hypothesis 1]\n- [Hypothesis 2]\n- [Hypothesis 3]\n\n---\n\nPATTERN 2: [Another trend]\n\n[Same structure]\n\n---\n\n### Cohort Comparison\n\nBest performing cohorts:\n- [Cohort]: [Metric] at [timeframe]\n- What was different: [Context about this period]\n\nWorst performing cohorts:\n- [Cohort]: [Metric] at [timeframe]\n- What was different: [Context about this period]\n\nThe gap:\n[Quantify difference and what it means]\n\n---\n\n### Time-Based Insights\n\nWeek 0-1 (Activation):\n[What happens in first week]\n- Drop-off rate: [%]\n- Key pattern: [What you notice]\n\nWeek 2-4 (Early retention):\n[What happens after initial usage]\n- Retention stabilizes at: [%]\n- Key pattern: [What you notice]\n\nWeek 5+ (Mature retention):\n[Long-term patterns]\n- Retention curve flattens at: [%]\n- Key pattern: [What you notice]\n\nCritical drop-off point:\n[Where most loss happens and why it matters]\n\n---\n\n### Hypotheses About What's Working\n\nHypothesis 1:\n[Your theory about why performance changed]\n\nEvidence:\n[Data points supporting this]\n\nHow to validate:\n[What you'd need to check]\n\n---\n\nHypothesis 2:\n[Another theory]\n\n[Same structure]\n\n---\n\n### Segment Insights (If Applicable)\n\nBy acquisition channel:\n- [Channel]: [Performance]\n- [Channel]: [Performance]\n- Insight: [What this tells you]\n\nBy user type:\n- [Segment]: [Performance]\n- [Segment]: [Performance]\n- Insight: [What this tells you]\n\nBy product tier:\n- [Tier]: [Performance]\n- [Tier]: [Performance]\n- Insight: [What this tells you]\n\n---\n\n### Red Flags / Warning Signs\n\n⚠️ [Issue]:\n[What's concerning and why]\n\n⚠️ [Issue]:\n[Another concern]\n\n---\n\n### What to Do Next\n\nIMMEDIATE (This week):\n- [ ] [Action based on finding]\n- [ ] [Another action]\n\nSHORT-TERM (This month):\n- [ ] [Deeper investigation or experiment]\n- [ ] [Product change to test]\n\nMONITORING:\n- Track: [Specific metric]\n- For cohorts: [Which ones]\n- Decision point: [What would trigger action]\n\n</analysis_framework>\n\n<cohort_patterns>\n\n### Common Cohort Patterns & What They Mean\n\nIMPROVING OVER TIME\nNewer cohorts retain better than older cohorts\n\nLikely causes:\n- Product improvements working\n- Better onboarding\n- Better customer targeting\n- Learning from past cohorts\n\nWhat to do:\n- Document what changed\n- Double down on improvements\n- Extract playbook for consistency\n\n---\n\nDECLINING OVER TIME\nNewer cohorts retain worse than older cohorts\n\nLikely causes:\n- Product changes made it worse\n- Lower quality customer acquisition\n- Market saturation (getting worse-fit customers)\n- Competitive pressure increased\n\nWhat to do:\n- Identify when decline started\n- What changed at that time?\n- Talk to churning customers\n- Consider reverting changes\n\n---\n\nPLATEAU PATTERN\nAll cohorts flatten to same retention rate\n\nWhat this means:\nYou've found your natural retention floor\n\nLikely causes:\n- Core value is strong\n- But missing something for next level\n- Product-market fit for subset\n\nWhat to do:\n- Identify what users who stay have in common\n- Focus on activating more users to that level\n- Improve experience for retained users\n\n---\n\nSTEEP DROP-OFF EARLY\nBig loss in first week/month, then stable\n\nLikely causes:\n- Onboarding isn't working\n- Activation isn't happening\n- Wrong customers signing up\n- Value not obvious quickly\n\nWhat to do:\n- Focus on Week 0-1 experience\n- Time-to-value too long\n- Improve activation flow\n- Better customer qualification\n\n---\n\nSTEADY DECLINE (No flatten)\nCohorts keep losing users over time\n\nLikely causes:\n- No habit formation\n- Competitors are better\n- Product doesn't deliver sustained value\n- Pricing issue (value < cost)\n\nWhat to do:\n- Deep customer interviews\n- Understand why people leave\n- Fundamental product or PMF issue\n\n---\n\nSEASONAL PATTERN\nCertain cohorts perform differently by time of year\n\nLikely causes:\n- Business use case (B2B)\n- Consumer behavior (fitness, tax, etc.)\n- Budget cycles\n- Marketing channel mix changes\n\nWhat to do:\n- Plan for seasonality\n- Adjust acquisition strategy\n- Build features for off-season retention\n\n</cohort_patterns>\n\n<meta_guidance>\n\nCohort analysis principles:Compare cohorts, not absolute numbers\n\"January users retain at 60%\" means nothing without context\n\"January cohort retains 10% better than December\" is insight\n\nLook for inflection points\nWhen did performance change?\nWhat happened at that time?\n\nSegment cohorts\nDon't just look at all users\nBreak down by channel, feature, tier\n\nTie to product changes\nMap cohorts to your release calendar\nSee cause and effect\n\nFocus on early retention\nWeek 1 retention predicts Week 12\nFix activation before growth\n\nKnow your retention curve\nWhere does it flatten?\nThat's your loyal user base\n\nRemember:\n\nCohort analysis isn't about having perfect data.\nIt's about finding patterns that suggest where to dig deeper.\n\nOne clear insight > 10 vague observations.\n\n</meta_guidance>\n\n</cohort_analysis>",
    "technique": "Data interpretation, pattern recognition, actionable insight extraction",
    "tools": "Claude Projects, ChatGPT Projects, NotebookLM",
    "useCase": "Understand retention and behavior patterns by signup cohort"
  },
  {
    "name": "Brainstorm Growth Loops",
    "category": "Strategy & Planning",
    "prompt": "<outcome_roadmap>\n\n<roadmap_inputs>\nSTRATEGIC GOALS:\n[Company/team OKRs or north star]\n\nCURRENT STATE:\n- Key problems: [What's broken or limiting]\n- Customer pain points: [From research]\n- Business gaps: [What's holding back growth]\n\nCONSTRAINTS:\n- Team capacity: [Size and timeline]\n- Technical dependencies: [Platform limitations]\n- Market timing: [Competitive or seasonal factors]\n\nROADMAP HORIZON:\n- [ ] This quarter\n- [ ] Next 2 quarters\n- [ ] This year\n</roadmap_inputs>\n\n<roadmap_framework>\n\nYou build roadmaps that preserve flexibility while giving direction. Your job: define outcomes to achieve, not features to ship.\n\nTHE DIFFERENCE:Feature-based roadmap:\n\"Q2: Build SSO integration\"\n\"Q3: Add analytics dashboard\"\n\nOutcome-based roadmap:\n\"Q2: Enable enterprise buyers to deploy across teams (may require SSO, may not)\"\n\"Q3: Help users understand what's working (analytics is one option)\"\n\nWhy outcomes are better:\n- Preserves solution flexibility\n- Focuses on impact, not output\n- Easier to prioritize\n- Less commitment to specific features\n\n---\n\n## OUTCOME-BASED ROADMAP\n\nTeam: [Your team]\nTimeframe: [Period]\nLast updated: [Date]\n\n---\n\n### Strategic Context\n\nCompany goal:\n[What the company is trying to achieve]\n\nOur role:\n[How your team contributes to that goal]\n\nSuccess looks like:\n[Specific outcome that shows you're winning]\n\n---\n\n### Roadmap Theme\n\nThis period is about:\n[One sentence focus - the through-line connecting everything]\n\n---\n\n### Now (Current Quarter)\n\nOUTCOME 1: [User outcome or business result]Why this matters:\n[Connection to strategy and impact]\n\nCurrent state:\n- Problem: [Specific pain point]\n- Impact: [Who affected, how often]\n- Baseline metric: [Current performance]\n\nTarget outcome:\n- Success metric: [What improves]\n- Target: [Specific number or %, timeframe]\n\nHow we might solve it:\n[Possible approaches - note the flexibility]\n- Option A: [Approach]\n- Option B: [Alternative approach]\n- TBD: [Will decide based on research]\n\nWhat we'll learn:\n[Validation or insights we need]\n\n---\n\nOUTCOME 2: [Another outcome]\n\n[Same structure]\n\n---\n\n### Next (Following Quarter)\n\nOUTCOME 1: [Future outcome]\n\n[Similar structure but less detail]\n\nDependencies:\n[What needs to happen first]\n\nOpen questions:\n[What we'll learn in current quarter that informs this]\n\n---\n\n### Later (Future Quarters)\n\nOUTCOME 1: [Longer-term outcome]Why eventually:\n[Strategic importance but not urgent]\n\nWhat could change our mind:\n[Signals that would make us pull this forward]\n\n---\n\n### What We're NOT Doing\n\nIntentionally deferred:\n- [Outcome or area we considered but postponed]\n- Why not now: [Reasoning]\n- Reconsidered: [When we'll revisit]\n\nExplicitly out of scope:\n- [Request or area we're not addressing]\n- Why no: [Strategic reason]\n\n---\n\n### How We'll Measure Success\n\nMetrics we're moving:\n\n| Metric | Baseline | Target | Timeframe |\n|--------|----------|--------|-----------|\n| [Metric] | [Current] | [Goal] | [When] |\n| [Metric] | [Current] | [Goal] | [When] |\n\nLeading indicators:\n[Early signals we're on track]\n\nLagging indicators:\n[Proof of long-term success]\n\n---\n\n### Dependencies & Risks\n\nDepends on other teams:\n- [Team]: [What we need from them]\n- [Team]: [What we need from them]\n\nTechnical dependencies:\n- [Dependency] - Risk: [What if it's delayed]\n\nMarket/external factors:\n- [Factor] - Contingency: [How we'll adapt]\n\n---\n\n### Flexibility & Decision Points\n\nWe'll decide by [date]:\n[Specific choice between options]\n\nWe'll pivot if:\n[Conditions that would change our plan]\n\nWe'll know we're wrong if:\n[Metrics or signals that indicate we should change course]\n\n</roadmap_framework>\n\n<roadmap_examples>\n\n### Example 1: Growth Team Roadmap\n\nStrategic Context:\nCompany goal: Double paid conversions\nOur role: Improve trial → paid conversion\n\nTheme: Make value obvious before trial ends\n\n---\n\nNOW: Enable users to see value in first 3 daysCurrent state:\n- 40% of trials never complete setup\n- Users who complete setup in first week convert at 60% vs 15%\n\nTarget:\n- 70% complete setup in first 3 days\n- 50% overall conversion rate\n\nHow we might solve:\n- Guided onboarding flow\n- Pre-populated demo data\n- Templated quick starts\n- [Will test and decide]\n\n---\n\nNEXT: Help users prove ROI to stakeholdersWhy eventually:\nUsers love product but can't get buy-in\n\nPossible approaches:\n- Usage reports they can share\n- ROI calculator\n- Integration with their tools\n\n[Will validate in customer interviews]\n\n---\n\n### Example 2: Platform Team Roadmap\n\nStrategic Context:\nCompany goal: Support 10x growth\nOur role: Infrastructure that scales\n\nTheme: Performance and reliability at scale\n\n---\n\nNOW: Zero customer-impacting outagesCurrent state:\n- 2-3 incidents per month\n- Average 30min downtime\n- Root cause: Database connection limits\n\nTarget:\n- <1 incident per month\n- <5min MTTR\n\nHow we might solve:\n- Connection pooling\n- Circuit breakers\n- Better monitoring\n- [Architecture review to decide]\n\n---\n\nNEXT: Support 100K concurrent usersCurrent: Starts degrading at 10K\n\nApproaches to evaluate:\n- Horizontal scaling\n- Caching layer\n- Database sharding\n\n</roadmap_examples>\n\n<meta_guidance>\n\nOutcome roadmap principles:Outcomes > Outputs\nNot \"ship feature X\"\nBut \"enable outcome Y\"\n\nProblems > Solutions\nNot \"build analytics dashboard\"\nBut \"help users understand what's working\"\n\nFlexibility > Commitment\nDon't commit to specific features\nCommit to solving problems\n\nMetrics > Vague goals\n\"Improve onboarding\" is vague\n\"70% complete setup in 3 days\" is measurable\n\nThemes > Random list\nRoadmap should tell a story\nNot disconnected feature list\n\nRegular updates\nOutcome roadmaps are living docs\nUpdate monthly as you learn\n\nWhen to be specific:\n- Current quarter: More detail\n- Next quarter: Less detail\n- Future: Themes only\n\nRemember:\n\nBest roadmaps give teams direction without micromanaging.\n\nYou're aligning on the destination, not dictating the route.\n\n</meta_guidance>\n\n</outcome_roadmap>",
    "technique": "Systems thinking, network effects, incentive design",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Design viral or scalable growth mechanisms for product"
  },
  {
    "name": "AI Customer Intelligence",
    "category": "Discovery",
    "prompt": "<sentiment_analysis>\n\n<sentiment_inputs>\nYOUR FEEDBACK DATA:\n[Paste customer feedback, reviews, support tickets, NPS comments, etc.]\n\nOR describe what you have:\n- Source: [Where feedback came from]\n- Volume: [How many pieces of feedback]\n- Time period: [When collected]\n\nWHAT YOU WANT TO KNOW:\n- [ ] Overall sentiment (positive/negative/neutral split)\n- [ ] Top themes/topics\n- [ ] Pain points to address\n- [ ] Features customers love\n- [ ] Competitive mentions\n- [ ] Segment differences\n\nCONTEXT:\n- Recent changes: [Anything that might affect sentiment]\n- Known issues: [What you're already aware of]\n</sentiment_inputs>\n\n<sentiment_framework>\n\nYou analyze feedback to find patterns humans would miss. Your job: turn 100 comments into 5 actionable insights.\n\nTHE PROCESS:1. Sentiment scoring\nHow do people feel?\n\n2. Theme extraction\nWhat are they talking about?\n\n3. Pain point prioritization\nWhat hurts most?\n\n4. Opportunity identification\nWhat delights them?\n\n5. Action recommendations\nWhat should you do?\n\n---\n\n## SENTIMENT ANALYSIS\n\n### Overall Sentiment\n\nSentiment breakdown:\n- Positive: [X%]\n- Neutral: [Y%]\n- Negative: [Z%]\n\nCompared to [baseline or previous period]:\n[Up/down/flat and by how much]\n\nOverall health: [Healthy/Concerning/Critical]\n\n---\n\n### Top Themes\n\nTHEME 1: [Most mentioned topic]Frequency: [Mentioned in X% of feedback]\n\nSentiment: [Mostly positive/negative/mixed]\n\nRepresentative quotes:\n- \"[Customer quote 1]\"\n- \"[Customer quote 2]\"\n- \"[Customer quote 3]\"\n\nWhat this tells us:\n[Interpretation - is this good, bad, opportunity?]\n\n---\n\nTHEME 2: [Second most common topic]\n\n[Same structure]\n\n---\n\nTHEME 3: [Third topic]\n\n[Same structure]\n\n---\n\n[Continue for top 5-7 themes]\n\n---\n\n### Pain Points (Ranked by Intensity × Frequency)\n\nPAIN POINT 1: [Specific issue]Severity: [Critical/High/Medium]\nFrequency: [Mentioned by X% of respondents]\nTrend: [Getting better/worse/stable]\n\nCustomer impact:\n[What this is costing them]\n\nQuotes:\n- \"[Direct quote showing pain]\"\n- \"[Another quote]\"\n\nRecommended action:\n[What to do about this]\n\n---\n\nPAIN POINT 2:\n\n[Same structure]\n\n---\n\n### What Customers Love\n\nDelight factor 1: [What they praise]Frequency: [Mentioned by X%]\n\nWhy they love it:\n[What value they get]\n\nQuotes:\n- \"[Enthusiastic quote]\"\n- \"[Another positive quote]\"\n\nOpportunity:\n[How to lean into this more]\n\n---\n\n### Competitive Intelligence\n\nCompetitors mentioned:\n- [Competitor A]: [Mentioned X times]\n- Context: [Why they came up]\n- Comparison: [What customers said]\n\n- [Competitor B]: [Mentioned Y times]\n- Context: [Why mentioned]\n- Comparison: [How we're positioned]\n\nWhat this reveals:\n[Competitive insight or concern]\n\n---\n\n### Segment Insights (If data available)\n\nBy customer type:[Segment A]:\n- Sentiment: [X% positive]\n- Top concern: [Issue]\n- Top praise: [What they love]\n\n[Segment B]:\n- Sentiment: [Y% positive]\n- Top concern: [Issue]\n- Top praise: [What they love]\n\nKey difference:\n[How segments differ in needs/satisfaction]\n\n---\n\n### Trends Over Time\n\nImproving:\n- [Topic/feature] sentiment went from [X%] → [Y%] positive\n- Why: [Likely cause]\n\nDeclining:\n- [Topic/feature] sentiment went from [X%] → [Y%] positive\n- Why: [Likely cause]\n\nNew themes:\n- [Topic] started appearing [timeframe]\n- Signals: [What this might mean]\n\n---\n\n### Red Flags / Warning Signs\n\n🚨 [Urgent issue]:\n[Specific problem mentioned frequently]\n- Impact: [High/Medium severity]\n- Action needed: [What to do]\n\n⚠️ [Concerning pattern]:\n[Trend that's worrying]\n- Monitor: [What to watch]\n\n---\n\n### Recommended Actions\n\nIMMEDIATE (This week):\n1. [Action addressing critical pain point] - Owner: [Name]\n2. [Communication or quick fix] - Owner: [Name]\n\nSHORT-TERM (This month):\n1. [Product change or improvement]\n2. [Process change]\n\nLONG-TERM (This quarter):\n1. [Strategic investment based on feedback]\n\nVALIDATION NEEDED:\n1. [Thing to investigate with customers]\n- Hypothesis: [What you think]\n- Validation method: [How to confirm]\n\n---\n\n### Quote Bank (For Marketing/Sales)\n\nBest testimonials:\n- \"[Powerful positive quote]\" - [Customer type]\n- \"[Another strong quote]\" - [Customer type]\n\nUse for: Case studies, website, sales collateral\n\n</sentiment_framework>\n\n<sentiment_patterns>\n\n### Pattern Recognition\n\nCLUSTER: Multiple complaints about same root cause\n\nExample: \"Slow,\" \"Laggy,\" \"Takes forever,\" \"Performance issues\"\n→ All pointing to: Speed problems\n\nAction: Don't treat as 4 separate issues. One performance fix addresses all.\n\n---\n\nCLUSTER: Feature requests that solve same job\n\nExample: \"Want templates,\" \"Need examples,\" \"Show me how others do it\"\n→ All pointing to: Lack of guidance/starting point\n\nAction: Could solve with templates, or examples, or onboarding\n\n---\n\nCLUSTER: Praise for unexpected use case\n\nExample: Multiple people using [feature] for [unintended purpose]\n→ Signals: New market or product direction\n\nAction: Lean into it, optimize for that use case\n\n---\n\nPATTERN: Sentiment changes after specific date\n\nExample: Negative feedback spiked after [release date]\n→ Signals: Recent change broke something\n\nAction: Investigate what shipped, consider rollback\n\n---\n\nPATTERN: Segment divergence\n\nExample: Enterprise loves it, SMB frustrated\n→ Signals: Product-market fit for one segment, not other\n\nAction: Consider separate products or better segmentation\n\n</sentiment_patterns>\n\n<meta_guidance>\n\nSentiment analysis principles:Themes > individual comments\nDon't get distracted by one loud voice\nFind patterns across many\n\nIntensity matters\n\"It's okay\" vs \"I love it\" both positive\nBut very different levels of satisfaction\n\nContext is everything\n\"Slow\" might mean:\n- Feature is slow\n- Learning curve is slow\n- Onboarding is slow\nDig deeper to understand\n\nAbsence of complaints ≠ satisfaction\nCustomers rarely praise the baseline\nLook for enthusiasm, not just lack of negativity\n\nRecency bias\nRecent feedback feels more important\nBut may not represent majority\n\nVolume ≠ importance\n10 people complaining loudly\nMight be less important than\n100 people quietly churning\n\nRemember:\n\nSentiment analysis is starting point, not answer.\n\nUse it to identify what to investigate deeper.\n\nThen talk to actual customers.\n\n</meta_guidance>\n\n</sentiment_analysis>",
    "technique": "Pattern extraction, theme clustering, signal detection",
    "tools": "Claude Projects, ChatGPT Projects, NotebookLM",
    "useCase": "Analyze customer feedback, reviews, or support tickets at scale"
  },
  {
    "name": "Launch Checklist",
    "category": "Operations",
    "prompt": "<launch_checklist>\n\n<launch_inputs>\nWHAT YOU'RE LAUNCHING:\n[Feature, product, major change]\n\nLAUNCH TYPE:\n- [ ] Beta (limited users)\n- [ ] GA (General availability)\n- [ ] Rollout (phased)\n- [ ] Migration (forcing change)\n\nSCOPE:\n- User impact: [How many users, which segments]\n- Business impact: [Revenue, reputation risk]\n- Technical scope: [How complex]\n\nLAUNCH DATE:\n[Target date]\n\nTEAM:\n[Who's involved - eng, design, marketing, sales, support]\n</launch_inputs>\n\n<checklist_framework>\n\nYou create launch checklists that prevent disasters. Your job: think of everything that could go wrong, make sure it's covered.\n\n---\n\n## LAUNCH CHECKLIST\n\nFeature: [Name]\nLaunch date: [Date]\nOwner: [Name]\nType: [Beta/GA/Rollout]\n\n---\n\n### PRODUCT READINESS\n\n✅ Core functionality\n- [ ] Feature works end-to-end\n- [ ] All acceptance criteria met\n- [ ] Edge cases handled\n- [ ] Error states tested\n- [ ] Performance acceptable (load time, scale)\n\n✅ Quality\n- [ ] QA testing complete\n- [ ] No critical bugs\n- [ ] Known issues documented\n- [ ] Regression testing passed (didn't break other features)\n\n✅ User experience\n- [ ] Design review approved\n- [ ] Copy finalized\n- [ ] Mobile responsive\n- [ ] Accessibility tested\n- [ ] Loading states clear\n\n✅ Technical\n- [ ] Code review complete\n- [ ] Deployed to staging\n- [ ] Database migrations tested\n- [ ] API endpoints documented\n- [ ] Monitoring/alerting configured\n- [ ] Feature flag implemented (if rollout)\n- [ ] Rollback plan documented\n\n---\n\n### DOCUMENTATION\n\n✅ Internal docs\n- [ ] Technical spec updated\n- [ ] Architecture diagram current\n- [ ] API documentation complete\n- [ ] Runbook for on-call\n- [ ] Known limitations documented\n\n✅ User-facing docs\n- [ ] Help articles written\n- [ ] Video tutorial (if complex)\n- [ ] FAQs prepared\n- [ ] In-app tooltips/guides\n- [ ] Changelog entry drafted\n\n✅ Team materials\n- [ ] Sales battlecard updated\n- [ ] Demo script prepared\n- [ ] Support playbook ready\n- [ ] Internal FAQ for team\n\n---\n\n### COMMUNICATION\n\n✅ Internal comms\n- [ ] Team announced and aligned\n- [ ] Support team trained\n- [ ] Sales team briefed\n- [ ] Success team prepared\n- [ ] Execs informed\n\n✅ External comms\n- [ ] Release notes drafted\n- [ ] Email announcement ready\n- [ ] Social posts prepared\n- [ ] Blog post (if major)\n- [ ] In-app notification configured\n\n✅ Customer-specific\n- [ ] Beta customers notified\n- [ ] Key accounts briefed personally\n- [ ] Breaking changes communicated early\n\n---\n\n### GO-TO-MARKET\n\n✅ Marketing\n- [ ] Landing page updated\n- [ ] Pricing page updated (if relevant)\n- [ ] Email campaign scheduled\n- [ ] Social media planned\n- [ ] Press outreach (if newsworthy)\n\n✅ Sales enablement\n- [ ] CRM updated\n- [ ] Sales deck updated\n- [ ] Demo environment ready\n- [ ] Objection handling prepared\n- [ ] Competitive positioning ready\n\n✅ Analytics\n- [ ] Tracking instrumented\n- [ ] Success metrics defined\n- [ ] Dashboard created\n- [ ] Alerts configured\n- [ ] A/B test setup (if applicable)\n\n---\n\n### RISK MITIGATION\n\n✅ Rollback plan\n- [ ] Feature flag to disable\n- [ ] Database rollback tested\n- [ ] Customer impact of rollback understood\n- [ ] Decision criteria for rollback defined\n\n✅ Support readiness\n- [ ] Extra support coverage scheduled\n- [ ] Escalation path clear\n- [ ] Known issues + workarounds documented\n- [ ] Slack channel for launch monitoring\n\n✅ Monitoring\n- [ ] Error rates baseline\n- [ ] Performance metrics baseline\n- [ ] Success metrics tracked\n- [ ] On-call schedule confirmed\n\n✅ Contingency\n- [ ] What if: feature breaks\n- [ ] What if: users confused\n- [ ] What if: negative feedback\n- [ ] What if: competitors respond\n\n---\n\n### COMPLIANCE & SECURITY\n\n✅ Security\n- [ ] Security review complete\n- [ ] Penetration testing done (if needed)\n- [ ] Data privacy assessed\n- [ ] Access controls verified\n\n✅ Legal/Compliance\n- [ ] Terms of service updated (if needed)\n- [ ] Privacy policy updated (if needed)\n- [ ] GDPR compliance checked\n- [ ] Legal review (if required)\n\n✅ Business\n- [ ] Contracts updated (if pricing change)\n- [ ] Billing system ready (if monetized)\n- [ ] Finance informed (if revenue impact)\n\n---\n\n### LAUNCH DAY\n\n✅ Pre-launch (Morning of)\n- [ ] Final smoke test in production\n- [ ] Team synced on plan\n- [ ] Support ready\n- [ ] Marketing materials queued\n\n✅ During launch\n- [ ] Feature enabled / rolled out\n- [ ] Monitoring active\n- [ ] Announcement sent\n- [ ] Team available for issues\n\n✅ Post-launch (First 24hrs)\n- [ ] Error rates normal\n- [ ] User feedback reviewed\n- [ ] Quick fixes deployed (if needed)\n- [ ] Team debriefed\n\n---\n\n### POST-LAUNCH (Week 1)\n\n✅ Measure success\n- [ ] Success metrics pulled\n- [ ] User feedback collected\n- [ ] Support tickets analyzed\n- [ ] Performance reviewed\n\n✅ Follow-up\n- [ ] Results shared with team\n- [ ] Quick wins implemented\n- [ ] Lessons learned documented\n- [ ] Next iteration planned\n\n---\n\n### LAUNCH DECISION\n\nGO / NO-GO Criteria:Must have (blockers if missing):\n- [ ] [Critical requirement]\n- [ ] [Critical requirement]\n\nShould have (launch anyway but address soon):\n- [ ] [Nice-to-have]\n- [ ] [Nice-to-have]\n\nFinal decision: [Go / No-go / Delay]\nMade by: [Name]\nDate: [When]\n\n</checklist_framework>\n\n<meta_guidance>\n\nLaunch checklist principles:Customize for launch size\nSmall feature: Shorter checklist\nMajor product launch: Every item\n\nAssign owners\nEvery checklist item should have a name\nNo \"someone should...\"\n\nSet deadlines\nNot just \"before launch\"\nBut \"3 days before launch\"\n\nTrack completion\nUse actual checkboxes\nReview progress daily in final week\n\nLearn from past launches\nWhat went wrong last time?\nAdd those items to checklist\n\nDon't skip steps\nChecklist exists because things get forgotten\nEspecially when rushing\n\nRemember:\n\nA launch checklist is boring.\nA botched launch is expensive.\n\nTake the 30 minutes to check everything.\n\n</meta_guidance>\n\n</launch_checklist>",
    "technique": "Completeness verification, risk mitigation, stakeholder alignment",
    "tools": "Claude Projects, Notion AI",
    "useCase": "Make sure you don't forget critical steps before shipping"
  },
  {
    "name": "Debug AI Feature Quality",
    "category": "AI Features",
    "prompt": "<debug_ai_quality>\n\n<quality_inputs>\nPASTE BAD EXAMPLES:\n[Copy-paste 3-5 actual outputs that are wrong/bad/unhelpful]\n\nWHAT YOU EXPECTED:\n[What you wanted it to do instead]\n\nYOUR SETUP:\n- What's the AI doing: [Summarize, generate, classify, etc.]\n- Your prompt (if you have it): [Paste current prompt]\n- Model: [GPT-4, Claude, other]\n- Context you give it: [User input, docs, etc.]\n\nTHE PROBLEM:\n- [ ] Outputs are wrong/inaccurate\n- [ ] Outputs are unhelpful/generic\n- [ ] Outputs are too long/short\n- [ ] Outputs miss the point\n- [ ] Inconsistent quality\n- [ ] Other: [describe]\n</quality_inputs>\n\n<debug_framework>\n\nYou debug AI features by looking at actual outputs. Your job: spot patterns in failures, suggest concrete fixes.\n\n---\n\n## QUALITY DIAGNOSIS\n\n### What You're Seeing\n\nBad example 1:\n[Paste output]\n\nWhat's wrong with it:\n[Specific issue]\n\n---\n\nBad example 2:\n[Paste output]\n\nWhat's wrong:\n[Issue]\n\n---\n\nPattern in failures:\n[What these bad examples have in common]\n\n---\n\n### Root Cause Hypothesis\n\nMost likely issue:\n\n- [ ] Prompt is too vague - Model doesn't understand what you want\n- [ ] Missing context - Model doesn't have info it needs\n- [ ] Wrong instructions - You told it to do X but want Y\n- [ ] Model limitations - Task is too hard for this model\n- [ ] Input quality - Garbage in, garbage out\n- [ ] Inconsistent outputs - Model is \"creative\" when you want deterministic\n\nEvidence:\n[Why you think this is the issue]\n\n---\n\n### Quick Fixes to Try\n\nFIX 1: Make prompt more specificCurrent prompt (probably):\n\"Summarize this text\"\n\nTry instead:\n\"Summarize this customer support ticket in 2 sentences:\n1. What is the customer's main problem?\n2. What do they want us to do?\n\nKeep it under 50 words.\"\n\nWhy this works: Specific structure > vague instruction\n\n---\n\nFIX 2: Add examplesCurrent: Instructions only\n\nTry instead:\n\"Here are 3 examples of good outputs:\n\nInput: [example]\nOutput: [example]\n\nNow do this for: [actual input]\"\n\nWhy this works: Models learn from examples better than instructions\n\n---\n\nFIX 3: Add constraintsWhat to add:\n- Length limits: \"In exactly 3 bullet points\"\n- Format: \"Return as JSON: {summary: ..., action: ...}\"\n- Tone: \"Professional but friendly\"\n- What NOT to include: \"Don't make recommendations, just summarize\"\n\nWhy this works: Constraints reduce randomness\n\n---\n\nFIX 4: Give it more contextWhat's it missing?\n- User history?\n- Product documentation?\n- Previous conversation?\n- Business rules?\n\nHow to add:\nPaste relevant context into prompt before the actual task\n\n---\n\nFIX 5: Test with better inputsIf inputs are low quality:\n- Clean/format them first\n- Extract key info before sending to AI\n- Filter out noise\n\n---\n\n### Testing Your Fix\n\nHow to validate:\n\n1. Take 10 real examples (including the bad ones)\n2. Run through new prompt\n3. Count how many are now good\n4. If <80% good → iterate again\n\nDon't:\n- Test on 1-2 examples (overfitting)\n- Only test on easy cases\n- Ship without testing edge cases\n\n---\n\n### When It's Not a Prompt Issue\n\nModel is wrong tool if:\n- Task requires 100% accuracy (use rules instead)\n- Task requires real-time data (model is outdated)\n- Task is simple classification (use simpler ML)\n\nConsider:\n- Hybrid approach (AI + rules)\n- Different model (Claude vs GPT-4 vs fine-tuned)\n- Human-in-the-loop (AI suggests, human approves)\n\n</debug_framework>\n\n<meta_guidance>\n\nReal talk about AI quality:You won't get perfect on first try\nExpect to iterate 5-10 times\n\nExamples > instructions\n2-3 good examples in prompt worth 1000 words of instructions\n\nTest with real data\nYour edge cases are where it breaks\n\nMost issues are prompt issues\nBefore blaming the model, fix your prompt\n\nTemperature matters\n- High temperature (0.7-1.0) = creative, varied\n- Low temperature (0-0.3) = consistent, focused\nIf outputs are too random, lower temperature\n\nRemember:\nAI features are products, not magic.\nThey need iteration like everything else.\n\n</meta_guidance>\n\n</debug_ai_quality>",
    "technique": "Prompting Technique: Output",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Your AI feature is giving bad outputs, need to figure out why"
  },
  {
    "name": "Reduce AI Costs\n",
    "category": "AI Features",
    "prompt": "<reduce_ai_costs>\n\n<cost_inputs>\nYOUR CURRENT SITUATION:\n- Monthly AI spend: $[amount]\n- Main use case: [What AI does in your product]\n- Model: [GPT-4, Claude, etc.]\n- Volume: [Requests per day/month]\n- Average tokens per request: [If you know]\n\nWHY COSTS ARE HIGH:\n- [ ] High volume of requests\n- [ ] Long prompts (lots of context)\n- [ ] Long outputs\n- [ ] Using expensive model for simple tasks\n- [ ] Not sure / don't know\n\nCONSTRAINTS:\n- Can't reduce: [Features that must stay]\n- Quality bar: [Acceptable trade-offs]\n- Timeline: [How urgent]\n</cost_inputs>\n\n<cost_framework>\n\nYou cut AI costs without killing quality. Your job: find waste, optimize smartly.\n\n---\n\n## COST REDUCTION PLAN\n\n### Where Money is Going\n\nQuick math:\n- Requests/month: [X]\n- Avg cost per request: $[Y]\n- = $[Total]\n\nBiggest drivers:\n1. [Input tokens or output tokens or # of requests]\n2. [Second biggest factor]\n\nQuick win: Focus on #1\n\n---\n\n### Cost Reduction Strategies\n\nSTRATEGY 1: Use cheaper model for simple tasksCurrent: Everything goes to GPT-4 / Claude Opus\n\nInstead:\n- Simple tasks → GPT-3.5 / Claude Haiku (10x cheaper)\n- Complex tasks → Keep expensive model\n\nWhat's \"simple\":\n- Classification (spam/not spam)\n- Short summaries\n- Sentiment analysis\n- Extraction (pull out key info)\n\nPotential savings: [Estimate based on % of simple tasks]\n\nHow to implement:\nAdd logic to route by task type\n\n---\n\nSTRATEGY 2: Reduce prompt lengthWhat's in your prompt:\n- [ ] Tons of examples\n- [ ] Long instructions\n- [ ] Full conversation history\n- [ ] Docs/knowledge base\n\nWhat to cut:\n- Old examples → Keep best 3\n- Redundant instructions → Simplify\n- Full history → Last 5 messages\n- Docs → Only relevant sections\n\nPotential savings: If prompt is 3000 tokens, cut to 1000 = 67% savings on input\n\nHow to measure:\nTrack avg input tokens before/after\n\n---\n\nSTRATEGY 3: Reduce output lengthCurrent: Letting AI write however much it wants\n\nInstead:\n- Add length constraints to prompt\n- \"Respond in 50 words max\"\n- \"Use exactly 3 bullet points\"\n- Stop sequences\n\nPotential savings: Output is usually biggest cost\n\n---\n\nSTRATEGY 4: Cache repeated contentIf you're sending same context repeatedly:\n(Product docs, examples, instructions)\n\nUse prompt caching:\n- Anthropic Claude: Prompt caching\n- OpenAI: Cached prompts feature\n- Custom: Store responses for identical inputs\n\nPotential savings: 50-90% on repeated context\n\n---\n\nSTRATEGY 5: Reduce unnecessary callsWhere are you calling AI that you don't need to?\n\n- [ ] Every keystroke (debounce it)\n- [ ] Identical requests (cache responses)\n- [ ] Already have answer (check cache first)\n- [ ] Could use rules instead (simple if/then logic)\n\nExample waste:\nUser types \"refund\" → AI categorizes\nYou could just check if message contains \"refund\"\n\nPotential savings: 20-50% of requests are redundant\n\n---\n\nSTRATEGY 6: Batch requestsCurrent: Individual API call per request\n\nInstead:\n- Batch multiple requests together\n- Process async when possible\n- Aggregate similar requests\n\nGood for:\n- Background processing\n- Bulk operations\n- Non-real-time features\n\n---\n\n### Implementation Priority\n\nDO FIRST (Easy, high impact):\n1. [Strategy X] - [X hours to implement] - [Saves $Y/month]\n2. [Strategy Y] - [Effort] - [Savings]\n\nDO NEXT (Medium effort):\n3. [Strategy Z]\n\nDO LATER (Hard but worth it):\n4. [Strategy W]\n\n---\n\n### Monitoring & Targets\n\nTrack these metrics:\n- Cost per request\n- Requests per user\n- Token usage (input/output)\n- Cache hit rate\n\nTargets:\n- Month 1: Reduce by [X%]\n- Month 2: Reduce by [Y%]\n- Goal: $[Target monthly spend]\n\nRed flags:\nIf quality drops, roll back and try different approach\n\n---\n\n### Cost/Quality Trade-offs\n\nWhat you CAN do without hurting quality:\n- Shorter prompts (be concise)\n- Cheaper models for easy tasks\n- Caching repeated content\n\nWhat MIGHT hurt quality:\n- Too-short outputs (users need detail)\n- Over-aggressive caching (stale responses)\n- Cheapest model for everything\n\nWhat WILL hurt quality:\n- No context\n- Tiny prompts\n- Always using worst model\n\nThe balance:\nTest each change, measure quality impact\n\n</cost_framework>\n\n<meta_guidance>\n\nReal talk about AI costs:80/20 rule applies:\n- 20% of requests probably cost 80% of money\n- Find those, optimize those first\n\nDon't optimize blindly:\n- Measure before/after\n- A/B test changes\n- Watch for quality degradation\n\nCheapest isn't always best:\n- Sometimes expensive model is worth it\n- Sometimes cheaper model is fine\n- Test to find out\n\nHidden costs:\n- Engineering time to optimize\n- Quality issues from over-optimization\n- Customer churn if experience degrades\n\nRemember:\nGoal isn't zero AI spend.\nGoal is efficient AI spend.\n\nSometimes spending more is right move (if it drives more value).\n\n</meta_guidance>\n\n</reduce_ai_costs>",
    "technique": "Usage analysis, optimization strategies",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "OpenAI/Anthropic bill is getting expensive, need to cut costs"
  },
  {
    "name": "Simple AI Eval System\n",
    "category": "AI Features",
    "prompt": "\n\n<simple_ai_evals>\n\n<eval_inputs>\nYOUR AI FEATURE:\n[What does it do? Summarize, generate, answer questions, etc.]\n\n**MANDATORY UPLOADS:**\n- [ ] test_inputs.csv - 20+ real examples [id, input, expected_output]\n- [ ] failures.csv - 5+ actual failures [id, input, bad_output, issue]\n\nYOUR QUALITY BAR:\n[Pass rate to ship? Typical: 80-90%]\n</eval_inputs>\n\n<eval_framework>\n\nYou create eval systems grounded in reality, not imagination.\n\n---\n\n## THE EVAL PHILOSOPHY\n\n**Why most evals fail:**\nTeams build evals from hypothetical examples and wonder why production still breaks. Generic test cases miss the specific ways YOUR system fails with YOUR users on YOUR data.\n\n**The truth about AI failures:**\n- 80% of failures come from 20% of input patterns\n- These patterns are hiding in your logs right now\n- You can't predict them - you have to observe them\n- Every domain has unique failure modes\n\n**What makes evals valuable:**\n- Built from real user inputs (not synthetic)\n- Test actual failure modes (not imagined ones)\n- Updated as you discover new issues\n- Specific enough to debug (\"fails on dates\" vs \"quality issues\")\n\n**The iterative approach:**\nDon't try to build perfect evals upfront:\n1. Start with real data → Build basic tests\n2. Ship and observe → Find new failures\n3. Add those failures to eval suite → Repeat\n\nGood evals prevent yesterday's bugs from returning, while you discover tomorrow's bugs.\n\n---\n\n## YOUR EVAL SYSTEM\n\n### Step 1: Verify Real Data\n\n**If uploads missing, STOP and say:**\n\n\"I need your actual production data to create useful evals.\n\n**Why real data matters:**\nA legal AI fails differently than a marketing AI. Technical users break systems differently than casual users. Your specific prompts create unique failure modes.\n\n**What to upload:**\n1. Real inputs from logs/tickets (anonymize if needed)\n2. Actual bad outputs you've seen (with notes on what broke)\n3. Expert-labeled correct outputs (for comparison)\n\nSpend 2 hours gathering real data → Get 10x better evals than I can invent.\"\n\n---\n\n### Step 2: Analyze Your Data\n\nUse the analysis tool to inspect uploaded files:\n- Load your CSVs\n- Calculate actual length distributions\n- Identify edge cases that exist in your data\n- Extract failure patterns from your failures file\n\n**Document what you found (using actual numbers):**\n\nFrom [filename.csv]:\n- Total examples: [N from file]\n- Length range: [min-max from data]\n- Natural clusters: [found in analysis]\n- Edge cases: [specific unusual examples]\n\nFrom failures:\n- Failure #1: [actual issue from file]\n- Failure #2: [actual issue from file]\n- Common pattern: [what you observe]\n\n---\n\n### Step 3: Create Test Categories\n\n**Use only uploaded examples - never invent test cases.****CATEGORY A: Happy Path**\nSelect 10-15 typical examples from uploaded data:\n\n| ID | Source | Input (first 50 chars) | Expected | Why Testing |\n|----|--------|------------------------|----------|-------------|\n| HP-001 | Row 3 | \"[actual text]...\" | \"[actual]\" | Typical case |\n| HP-002 | Row 7 | \"[actual text]...\" | \"[actual]\" | Common pattern |\n\n**CATEGORY B: Edge Cases**\nSelect boundary examples found in your data:\n\n| ID | Source | Input | Expected | Edge Type |\n|----|--------|-------|----------|-----------|\n| EC-001 | Row 45 | \"[shortest input found]\" | \"[expected]\" | Min length |\n| EC-002 | Row 12 | \"[longest input found]\" | \"[expected]\" | Max length |\n| EC-003 | Row 23 | \"[special chars found]\" | \"[expected]\" | Special chars |\n\n**CATEGORY C: Known Failures**\nEvery failure from uploaded failures.csv:\n\n| ID | Source | Input | Bad Output | Should Produce |\n|----|--------|-------|------------|----------------|\n| F-001 | Failure #1 | \"[actual]\" | \"[actual bad]\" | \"[correct]\" |\n\n**Coverage gaps:**\nAfter analysis, these scenarios are MISSING:\n- Gap: [specific missing scenario]\n- Action: User needs to provide [N] examples of [scenario]\n\n---\n\n### Step 4: Define Pass Criteria\n\n**Derive from uploaded data patterns, not assumptions.****For Happy Path:**\nAnalyze expected outputs to establish:\n- Typical length: [calculate avg ± std dev from expected_output column]\n- Required elements: [extract common patterns from expected outputs]\n- Forbidden content: [patterns from failures that indicate hallucination]\n\nExample criteria:\nTest HP-001:\n□ Length in range [X-Y chars] (from data analysis)\n□ Contains required elements: [from expected output]\n□ No hallucinated content: [patterns from failures]\n□ Matches structure: [observed in expected outputs]\n\n\n**For Edge Cases:**\nDefine how system should handle boundaries found in your data:\n- Minimum input: Should handle gracefully, return [based on similar examples]\n- Maximum input: Should process fully or truncate predictably\n- Special chars: Should preserve or handle consistently\n\n**For Failures:**\nDefine regression tests:\nTest F-001:\n□ Does NOT repeat exact failure\n□ Does NOT show similar hallucination pattern\n□ DOES include content that was missing\n□ Meets general quality bar\n\n\n---\n\n### Step 5: Run Evals\n\n**Implementation approach:**\n- Use analysis tool to load test data\n- For each test, get your AI's actual output\n- Check against pass criteria\n- Report: pass rate, specific failures, patterns\n\n**Report format:**\n=== EVAL RESULTS ===\nPass rate: [X]% on YOUR data\nTotal: [N tests]\nPassed: [P]\nFailed: [F]\nFailed tests:\n• HP-003: [specific failure on actual input]\n• EC-001: [what broke on edge case]\n• F-002: [regression detected]\nPatterns in failures:\n• [Common thread in failures]\n• [Another pattern]\nRecommendations:\n1. [Specific fix based on actual failures]\n2. [Another specific fix]\n\n\n---\n\n### Step 6: Maintain Evals\n\n**Eval maintenance philosophy:**\n\nEvals decay if not maintained:\n- Production reveals new edge cases → Add to eval suite\n- User behavior changes → Update test distribution\n- System improves → Raise quality bar\n\n**Quarterly eval review:**\n- Run evals, check pass rate\n- Add last quarter's production failures\n- Remove tests for fixed issues (or keep as regression tests)\n- Adjust pass criteria if quality bar changed\n\n**When to update:**\n- New feature launched: Add feature-specific tests\n- Bug escaped to production: Add that exact case to evals\n- User segment changed: Update test distribution\n- Quality expectations changed: Adjust criteria\n\n</eval_framework>\n\n<quality_checks>\n\n**Before shipping your eval suite:**\n\n□ **Data grounding check:**\nEvery test comes from uploaded data? (not invented)\n\n□ **Coverage check:**\nTests cover the actual distribution of your inputs?\n- If 40% of real inputs have numbers, do 40% of tests?\n- If 5% are edge cases, do 5% test edges?\n\n□ **Failure mode check:**\nEvery known failure has a regression test?\n\n□ **Measurability check:**\nPass/fail is objective? Another person would judge the same way?\n\n□ **Gap honesty:**\nExplicitly called out what's NOT covered?\n\n**Self-critique questions:**\n- Did I invent ANY test cases not in uploaded files?\n- Are criteria based on actual data patterns or my assumptions?\n- Have I been honest about coverage gaps?\n- Can someone reproduce these evals with just the uploaded files?\n\n</quality_checks>\n\n<meta_guidance>\n\n**Real talk about evals:**\n\n**Evals are not perfect:**\n- They catch known issues, not unknown ones\n- 90% pass rate doesn't mean 90% of production works\n- Evals test what you thought to test\n\n**Evals are still valuable:**\n- Prevent regressions (yesterday's bugs stay fixed)\n- Fast feedback (find issues before production)\n- Shared quality bar (team agrees what \"good\" means)\n- Debug tool (understand what changed and why)\n\n**Start small, iterate:**\n- 20 tests that catch real issues > 200 generic tests\n- Ship evals, gather production data, add new tests\n- Better to have imperfect evals than no evals\n\n**Common mistakes:**\n- Building evals without real data (useless)\n- Never updating evals (they decay)\n- Chasing 100% pass rate (wrong goal)\n- Testing only happy path (misses edge cases)\n- Vague criteria (not measurable)\n\n**Success looks like:**\n- Evals catch 80% of issues before production\n- When bug escapes, you add it to evals same day\n- Pass rate trends up as system improves\n- Team trusts evals enough to ship based on them\n\nRemember: The goal isn't perfect evals. The goal is fewer surprises in production.\n\n</meta_guidance>\n\n</simple_ai_evals>",
    "technique": " case generation, quality criteria",
    "tools": "Claude Projects for creating eval sets",
    "useCase": "Need to test if AI feature actually works before shipping"
  },
  {
    "name": "AI Product Roadmap",
    "category": "AI Features",
    "prompt": "<ai_roadmap>\n\n<ai_inputs>\nYOUR PRODUCT:\n[What you build and who uses it]\n\nCURRENT AI USAGE:\n- [ ] No AI yet\n- [ ] Some AI features (which: [list])\n- [ ] AI-first product\n\nWHY YOU'RE EXPLORING AI:\n- [ ] Competitive pressure\n- [ ] Customer requests\n- [ ] Efficiency opportunity\n- [ ] New capabilities possible\n- [ ] Everyone's doing it (be honest!)\n\nCONSTRAINTS:\n- Team: [Eng capacity, AI expertise level]\n- Budget: [Can you spend on AI costs?]\n- Timeline: [How soon do you need results?]\n- Data: [Do you have training data / user content?]\n</ai_inputs>\n\n<roadmap_framework>\n\nYou build AI roadmaps that are practical, not hype. Your job: find where AI actually helps, avoid where it doesn't.\n\n---\n\n## AI OPPORTUNITY ASSESSMENT\n\n### Where AI Actually Makes Sense\n\nAI is good at:\n- Summarizing/synthesizing large amounts of text\n- Generating content (drafts, variations, suggestions)\n- Classification/categorization\n- Answering questions from knowledge base\n- Extraction (pull structured data from unstructured)\n- Translation/transformation\n\nAI is bad at:\n- Tasks requiring 100% accuracy\n- Real-time updated information\n- Simple deterministic logic\n- Anything high-stakes without human review\n\n---\n\n### Your Product's AI Opportunities\n\nFor each, assess:\n- Value to users (High/Med/Low)\n- Feasibility (Easy/Med/Hard)\n- Urgency (Now/Soon/Later)\n\n---\n\nOPPORTUNITY 1: [Specific use case]What it does:\n[One sentence description]\n\nUser value:\n- Saves them: [Time/effort]\n- Enables them to: [New capability]\n- Improves: [Quality/accuracy]\n\nHow it would work:\n[Basic flow]\n\nFeasibility:\n- Model capability: [Can models do this today?]\n- Data needed: [Do you have it?]\n- Integration complexity: [Easy/Med/Hard]\n\nRough cost:\n[Requests per day × cost per request]\n\nRisks:\n- Quality issues if: [What could go wrong]\n- Cost blowup if: [Unexpected usage]\n\nPriority: [High/Med/Low]\n\n---\n\nOPPORTUNITY 2: [Another use case]\n\n[Same structure]\n\n---\n\nOPPORTUNITY 3: [Another]\n\n[Same structure]\n\n---\n\n### Prioritization Framework\n\nHigh priority (Do first):\n- High user value\n- Easy to build\n- Low cost\n- Clear use case\n\nExample: Summarize long documents\n\n---\n\nMedium priority (Do next):\n- High value but harder\n- OR easier but less valuable\n\nExample: Generate personalized email drafts\n\n---\n\nLow priority (Maybe later):\n- Unclear value\n- Very hard\n- Expensive\n- Better alternatives exist\n\nExample: AI chatbot (unless chat is core product)\n\n---\n\n### Practical Roadmap\n\nMONTH 1-2: Learn & ValidateGoal: Prove AI can work for one use case\n\nProject: [Simplest, highest-value opportunity]\n\nApproach:\n- Build basic version with Claude/GPT\n- Test with 10 users\n- Measure: Did they find it useful?\n- Budget: $500-1000\n\nSuccess = Users want to keep using it\n\n---\n\nMONTH 3-4: Ship & IterateGoal: Get to production quality\n\nWhat to build:\n- Polish UX\n- Add error handling\n- Set up evals\n- Monitor quality\n\nSuccess = 70%+ of users engage with feature\n\n---\n\nMONTH 5-6: ExpandGoal: Add 1-2 more AI features\n\nPick from: [Your med-priority opportunities]\n\nApply learnings from first feature\n\n---\n\nNEXT 6 MONTHS:Theme: [Strategic focus]\n\nProjects:\n- [Feature 1]: [Timeline]\n- [Feature 2]: [Timeline]\n- [Infrastructure]: [If needed - evals, monitoring, cost controls]\n\n---\n\n### What NOT to Do\n\nDon't:\n- [ ] Build AI chatbot as first project (too hard)\n- [ ] Try to use AI for everything (most things don't need it)\n- [ ] Ship without testing (AI is unpredictable)\n- [ ] Assume \"AI will figure it out\" (you need clear instructions)\n- [ ] Over-promise to customers (manage expectations)\n\n---\n\n### Build vs Buy Decisions\n\nWhen to use OpenAI/Anthropic APIs:\n- General capability (summarize, generate, classify)\n- Getting started\n- Don't have ML team\n\nWhen to consider fine-tuning:\n- Very specific task\n- Lots of training data\n- Need lower cost at scale\n- Quality issues with base models\n\nWhen to build custom ML:\n- Extremely high volume (cost prohibitive for APIs)\n- Latency critical (<100ms)\n- Offline/edge deployment\n- You have ML team\n\nFor most PMs: Start with APIs\n\n---\n\n### Success Metrics\n\nDon't just track:\n- \"AI feature usage\" (vanity metric)\n\nTrack:\n- User engagement (do they come back?)\n- User outcomes (do they achieve goal faster?)\n- Quality (thumbs up/down, issues reported)\n- Cost efficiency (cost per successful outcome)\n\nExample good metrics:\n- \"Users who use AI summary save 5 min per doc\"\n- \"AI-generated drafts accepted 70% of time\"\n- \"Cost per summary: $0.03\"\n\n</roadmap_framework>\n\n<ai_patterns>\n\n### Common AI Features by Product Type\n\nB2B SaaS:\n- Summarize reports/data\n- Generate drafts (emails, proposals)\n- Extract insights from usage data\n- Autocomplete (messages, code)\n\n---\n\nContent/Media:\n- Generate variations\n- Personalized recommendations\n- Auto-tagging/categorization\n- Content moderation\n\n---\n\nProductivity:\n- Meeting summarization\n- Email triage/prioritization\n- Task extraction\n- Smart search\n\n---\n\nE-commerce:\n- Product descriptions\n- Personalized suggestions\n- Customer support automation\n- Search/discovery\n\n---\n\nDeveloper Tools:\n- Code completion\n- Bug detection\n- Documentation generation\n- Test case generation\n\n</ai_patterns>\n\n<meta_guidance>\n\nReal talk about AI roadmaps:Most AI projects fail because:\n- Unclear use case (AI for AI's sake)\n- Overly ambitious first project\n- No way to measure success\n- Shipping without evals\n\nStart small:\n- One clear use case\n- Simple implementation\n- Measure everything\n- Learn before scaling\n\nAI is not strategy:\n- \"Add AI\" is not a roadmap\n- Find real user problems\n- AI is one solution tool\n\nManage expectations:\n- AI is powerful but not perfect\n- 80% good is great for AI\n- Some errors are expected\n- Human-in-loop for important decisions\n\nRemember:\nBest AI features feel like magic but are built methodically.\n\nStart with one valuable, achievable use case.\nShip it, learn from it, then expand.\n\n</meta_guidance>\n\n</ai_roadmap>",
    "technique": "Opportunity identification, feasibility assessment",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to figure out where to use AI in product, prioritize AI projects"
  },
  {
    "name": "Customize Resume to Job\n",
    "category": "Career",
    "prompt": "<customize_resume>\n\n<resume_inputs>\nPASTE YOUR CURRENT RESUME:\n[Copy-paste your full resume]\n\nPASTE THE JOB DESCRIPTION:\n[Copy-paste the full job posting]\n\nYOUR SITUATION:\n- [ ] Perfect fit (just need to highlight right things)\n- [ ] Mostly qualified (need to emphasize relevant experience)\n- [ ] Stretch role (need to make connections clear)\n- [ ] Career pivot (need to translate experience)\n\nCONSTRAINTS:\n- Keep it: [ ] 1 page [ ] 2 pages\n- Don't lie or exaggerate\n- Keep core facts the same\n</resume_inputs>\n\n<resume_framework>\n\nYou tailor resumes to get interviews. Your job: make their experience obviously relevant to this specific job.\n\nTHE REALITY:\n- Recruiters spend 7 seconds on your resume\n- ATS scans for keywords from job description\n- They're looking for reasons to say no\n\nYOUR GOAL:\nMake it impossible to say no.\n\n---\n\n## TAILORED RESUME\n\n### Analysis: Job Requirements vs Your Experience\n\nWhat they want (from JD):Must-haves:\n- [Requirement 1]\n- [Requirement 2]\n- [Requirement 3]\n\nNice-to-haves:\n- [Skill/experience]\n- [Skill/experience]\n\nKeywords they're scanning for:\n[List 10-15 key terms from JD]\n\n---\n\nWhat you have:Strong matches: ✅\n- [Your experience that directly matches]\n- [Another strong match]\n\nPartial matches: ⚠️\n- [Experience that's related but not exact]\n- [How to frame it]\n\nGaps: ❌\n- [What they want that you don't have]\n- [How to mitigate or ignore]\n\n---\n\n### Your Tailored Resume\n\n[YOUR NAME]\n[Contact info - keep same]\n\n---\n\nSUMMARY (Optional - use if career pivot or needs context)\n\n[2-3 sentences that directly connect your background to this role]\n\nExample:\n\"Product Manager with 5 years building B2B SaaS tools for [their industry]. Led cross-functional teams to ship [relevant outcome]. Experienced in [their tech stack/methodology].\"\n\nYour summary:\n[Tailored to this job]\n\n---\n\nEXPERIENCE[Company Name] | [Your Title] | [Dates]\n\nBEFORE (Generic):\n- \"Led product development for mobile app\"\n- \"Worked with engineering team\"\n- \"Increased user engagement\"\n\nAFTER (Tailored):[Rewritten to match their language and priorities]\n\n- [Bullet highlighting most relevant achievement using THEIR keywords]\n- [Bullet that speaks to their must-have requirement]\n- [Bullet showing impact with metrics]\n- [Bullet demonstrating skill they mentioned]\n\nFormula for each bullet:\n[Action verb] + [What you did] + [Impact with numbers] + [Keywords from JD]\n\n---\n\n[Previous Role]\n\n[Same treatment - rewrite bullets to emphasize relevance]\n\n---\n\nSKILLSBEFORE (Kitchen sink):\nPython, Java, SQL, Excel, Figma, Photoshop, Jira, Public speaking...\n\nAFTER (Targeted):Technical: [List skills from JD that you have, in THEIR order of importance]\nTools: [Their specific tools/platforms mentioned]\nMethodologies: [Their frameworks - Agile, Scrum, etc. if they mentioned]\n\n---\n\nEDUCATION\n[Keep same unless relevant to emphasize]\n\n---\n\n### Specific Changes Made\n\nKeywords added:\n- Replaced \"[your term]\" with \"[their term]\" (e.g., \"customer\" → \"client\")\n- Added \"[keyword from JD]\" to 3 bullets\n- Emphasized \"[their priority]\" throughout\n\nAchievements reframed:\n- Highlighted [X] which matches their requirement for [Y]\n- Moved [relevant experience] higher\n- De-emphasized [less relevant stuff]\n\nGaps addressed:\n- [How you handled missing requirement]\n- [What you emphasized instead]\n\n---\n\n### Resume Checklist\n\n✅ ATS-friendly:\n- [ ] Includes exact keywords from JD (not synonyms)\n- [ ] Simple formatting (no tables, text boxes, images)\n- [ ] Standard section headers (Experience, Skills, Education)\n- [ ] Saved as .docx or .pdf (check job posting preference)\n\n✅ Recruiter-friendly:\n- [ ] Relevant experience in top 1/3 of page\n- [ ] Numbers/metrics in bullets\n- [ ] No generic statements (\"team player,\" \"fast learner\")\n- [ ] Clear job titles and companies\n\n✅ Honest:\n- [ ] Didn't add skills you don't have\n- [ ] Didn't inflate titles or dates\n- [ ] Can back up every claim in interview\n\n---\n\n### Cover Letter Hook (Bonus)\n\nFirst paragraph that connects your experience to their needs:\n\n\"I'm reaching out about the [Job Title] role. In my current role at [Company], I [specific achievement that matches their top requirement]. I noticed you're looking for someone to [their key challenge], which is exactly what I did when I [your relevant experience].\"\n\n</resume_framework>\n\n<resume_patterns>\n\n### Pattern 1: Direct Match (Lucky you)\n\nJob wants: \"5 years PM experience in B2B SaaS\"\nYou have: Exactly that\n\nStrategy:\n- Lead with strongest matches\n- Use their exact language\n- Add metrics to prove impact\n- Don't bury the lead\n\n---\n\n### Pattern 2: Translatable Experience\n\nJob wants: \"Product Manager\"\nYou have: \"Project Manager\" or \"Business Analyst\"\n\nStrategy:\n- Emphasize product decisions you made\n- Highlight cross-functional leadership\n- Show user/customer focus\n- De-emphasize execution-only work\n\nExample transformation:\nBEFORE: \"Managed project timeline and resources\"\nAFTER: \"Led product roadmap prioritization based on user research, working with engineering and design to ship features that increased engagement 30%\"\n\n---\n\n### Pattern 3: Industry Pivot\n\nJob wants: Healthcare experience\nYou have: Fintech experience\n\nStrategy:\n- Focus on transferable skills (PM skills, not domain)\n- Emphasize similar problems (compliance, security, etc.)\n- Show quick learning ability\n- Research their space and reference it\n\nAdd to summary:\n\"While my experience is in fintech, I've successfully navigated [similar challenge they face], and am excited to apply [transferable skill] to healthcare.\"\n\n---\n\n### Pattern 4: Level Up\n\nJob wants: Senior PM\nYou have: PM experience (but not \"senior\")\n\nStrategy:\n- Emphasize leadership and strategy\n- Show scope of impact\n- Highlight mentoring/influencing\n- Frame achievements at higher level\n\nReframe bullets:\nBEFORE: \"Shipped feature X\"\nAFTER: \"Defined product strategy for [area], leading team of 3 PMs to deliver [outcome]\"\n\n</resume_patterns>\n\n<meta_guidance>\n\nResume tailoring principles:It's not lying, it's framing:\n- Same facts, different emphasis\n- Highlight what matters to them\n- De-emphasize what doesn't\n\nUse their language:\n- They say \"clients\" → you say \"clients\" (not \"customers\")\n- They say \"Agile\" → mention Agile (not just \"iterative\")\n- They say \"growth\" → emphasize growth metrics\n\nMetrics matter:\n- \"Increased engagement\" = vague\n- \"Increased engagement 40% in 6 months\" = concrete\n- Even rough estimates better than nothing\n\nDon't keyword stuff:\n- Yes: Naturally work in relevant terms\n- No: \"Python Python Python experienced Python developer\"\n\nOne resume per job:\n- Generic resume gets generic results\n- Takes 20 minutes to tailor\n- 3x better response rate\n\nRemember:\nGoal is to get the interview.\nOnce you're in the room, your skills speak for themselves.\n\nThe resume just has to get you in the room.\n\n</meta_guidance>\n\n</customize_resume>",
    "technique": "Keyword optimization, relevance matching, impact highlighting",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to tailor your resume to specific job posting to get past ATS and recruiter screen"
  },
  {
    "name": "Create Work Product for Job Application",
    "category": "Career",
    "prompt": "<pm_work_sample>\n\n<sample_inputs>\nWHAT'S THIS FOR:\n- [ ] Job application (proactive)\n- [ ] Take-home assignment (they requested)\n- [ ] Portfolio piece\n- [ ] Interview presentation\n\nTHE PROMPT (if they gave one):\n[Paste their assignment/question]\n\nOR choose your own:\n- [ ] Product strategy for [their product]\n- [ ] Feature analysis/recommendation\n- [ ] Market opportunity assessment\n- [ ] Competitive analysis\n- [ ] Roadmap recommendation\n\nYOUR EXPERIENCE TO DRAW FROM:\n[Brief description of relevant projects you've worked on]\n\nWHAT YOU KNOW ABOUT THEIR COMPANY:\n[Product, market, recent news, challenges]\n</sample_inputs>\n\n<sample_framework>\n\nYou create work samples that show how you think. Your job: demonstrate PM skills in 2 pages of clear, actionable insights.\n\n---\n\n## WORK SAMPLE STRUCTURE\n\n### The 2-Pager Formula\n\nPage 1: The thinking\n- Problem/Opportunity\n- Analysis\n- Recommendation\n\nPage 2: The details\n- How it works\n- Why it matters\n- Next steps\n\nGoal: Show you can think strategically AND get into details\n\n---\n\n### TEMPLATE: PRODUCT STRATEGY\n\nTitle: [Product Strategy for X]\n\n---\n\nTHE OPPORTUNITYCurrent state:\n[What's true today - market, product, users]\n\nThe gap:\n[What's missing or could be better]\n\nWhy now:\n[Timing, urgency, market window]\n\nImpact if we act:\n[Quantify the opportunity - revenue, users, market share]\n\n---\n\nANALYSISWhat I looked at:\n- [Data source or research method 1]\n- [Data source or research method 2]\n- [Data source or research method 3]\n\nKey insights:Insight 1: [Finding]\n[Evidence supporting this]\n→ Implication: [What this means for strategy]\n\nInsight 2: [Finding]\n[Evidence]\n→ Implication: [Strategic meaning]\n\nInsight 3: [Finding]\n[Evidence]\n→ Implication: [Strategic meaning]\n\n---\n\nRECOMMENDATIONI recommend: [Clear, specific strategy]Why this approach:\n1. [Reason tied to insight]\n2. [Another reason]\n3. [Third reason]\n\nExpected impact:\n- [Metric 1]: [X → Y in Z timeframe]\n- [Metric 2]: [Impact]\n- [Metric 3]: [Impact]\n\n---\n\nHOW IT WORKS (Page 2)Phase 1: [Timeframe - e.g., Months 1-3]Focus: [What to build/do]\n\nKey deliverables:\n- [Specific output]\n- [Specific output]\n\nSuccess looks like:\n[Metric or outcome]\n\n---\n\nPhase 2: [Timeframe]\n[Same structure]\n\n---\n\nPhase 3: [Timeframe]\n[Same structure]\n\n---\n\nRISKS & MITIGATIONSRisk 1: [What could go wrong]\n- Likelihood: High/Med/Low\n- Impact: High/Med/Low\n- Mitigation: [How to address]\n\nRisk 2: [Another risk + mitigation]\n\n---\n\nWHAT I'D NEED TO VALIDATEOpen questions:\n1. [Question I'd answer through research]\n2. [Question I'd validate with customers]\n3. [Question I'd test with prototype]\n\nHow I'd validate:\n[Brief description of research plan]\n\n---\n\nNEXT STEPSImmediate:\n- [Action in week 1]\n\nShort-term:\n- [Action in month 1]\n\nLong-term:\n- [Action in quarter 1]\n\n---\n\n### TEMPLATE: FEATURE RECOMMENDATION\n\nTitle: [Feature] for [Product]\n\n---\n\nTHE PROBLEMWho has this problem:\n[Specific user segment]\n\nWhat they're struggling with:\n[Specific pain point, not generic]\n\nHow they solve it today:\n[Current workaround or competitor solution]\n\nCost of not solving:\n- For users: [Time, money, frustration]\n- For business: [Churn, revenue, NPS]\n\n---\n\nTHE SOLUTIONI recommend building: [Specific feature]How it works:\n1. [User action]\n2. [System response]\n3. [Outcome]\n\nWhy users will love it:\n- [Benefit 1 in user language]\n- [Benefit 2]\n- [Benefit 3]\n\nWhy this, not alternatives:\n- Considered: [Alternative A] → Rejected because [reason]\n- Considered: [Alternative B] → Rejected because [reason]\n- This approach: [Why it's best]\n\n---\n\nSUPPORTING EVIDENCEUser research:\n\"[Quote from user interview]\"\n- [X] customers mentioned this pain in [timeframe]\n\nCompetitive landscape:\n- [Competitor A]: Has this, but [weakness]\n- [Competitor B]: Doesn't have this\n- Gap: [Opportunity for differentiation]\n\nData:\n- [Metric showing problem scope]\n- [Usage data supporting need]\n\n---\n\nIMPLEMENTATION (Page 2)MVP Scope:\n✅ Include:\n- [Core functionality]\n- [Must-have element]\n\n❌ Exclude (future phases):\n- [Nice-to-have]\n- [Complex addition]\n\nTechnical considerations:\n[Any obvious implementation notes, if you know them]\n\nEstimated effort:\n[If you know engineering velocity: Small/Medium/Large]\n[If not: \"Would estimate with eng team\"]\n\n---\n\nSUCCESS METRICSPrimary metric:\n[The one number that shows success]\n- Current: [Baseline]\n- Target: [Goal] by [timeframe]\n\nSecondary metrics:\n- [Supporting metric]\n- [Another metric]\n\nHow we'll measure:\n[Tracking plan]\n\n---\n\nROLLOUT PLANPhase 1: Beta\n- Who: [Small user group]\n- Goal: Validate usage and quality\n- Timeline: [2-4 weeks]\n\nPhase 2: General availability\n- Who: [All users]\n- Support: [Documentation, onboarding needed]\n- Timeline: [Date]\n\n---\n\nRISKS\n\n[Same structure as strategy template]\n\n</sample_framework>\n\n<quality_checks>\n\n### What Makes a Strong Work Sample\n\n✅ Shows strategic thinking:\n- Not just \"add feature X\"\n- But \"here's why it matters and what we'll learn\"\n\n✅ Demonstrates research:\n- Cites specific sources\n- References data or users\n- Shows you did homework\n\n✅ Makes clear recommendations:\n- Not wishy-washy \"could do A or B\"\n- Specific: \"I recommend X because Y\"\n\n✅ Addresses trade-offs:\n- Acknowledges alternatives\n- Explains why you'd prioritize this\n\n✅ Is skimmable:\n- Clear headers\n- Bullets over paragraphs\n- Bold key points\n- Visual (simple chart/diagram) if helpful\n\n✅ Shows PM judgment:\n- Prioritization\n- User empathy\n- Business sense\n- Realistic about constraints\n\n---\n\n### What Makes a Weak Work Sample\n\n❌ Too generic:\n- Could apply to any product\n- No specific insights about this company\n\n❌ No evidence:\n- Just opinions\n- No data, research, or user input\n\n❌ Feature list:\n- Just describes what to build\n- Doesn't explain why or how to prioritize\n\n❌ Too long:\n- More than 2-3 pages\n- Buries the lead\n- No clear structure\n\n❌ Unrealistic:\n- Ignores obvious constraints\n- Suggests massive undertaking as \"quick win\"\n\n</quality_checks>\n\n<meta_guidance>\n\nReal talk about work samples:Time investment:\n- Take-home assignment: They expect 2-4 hours\n- Proactive sample: Spend 3-5 hours max\n- Diminishing returns after that\n\nUse your real experience:\n- Draw on projects you've actually done\n- Reframe for their context\n- Your real insights > generic frameworks\n\nShow don't tell:\n- Don't say \"I'm strategic\"\n- Show strategic thinking in the work\n- Let quality speak for itself\n\nLess is more:\n- 2 great pages > 5 mediocre pages\n- Every sentence should add value\n- Cut anything generic\n\nMake it skimmable:\n- Hiring manager has 10 minutes\n- They'll skim before deep read\n- Make skimming satisfying\n\nThe gut check questions:\n\n1. Does this show how I think?\n2. Is this specific to this company?\n3. Would I be proud to discuss this in interview?\n4. Does it demonstrate PM skills (not just analysis)?\n\nRemember:\n\nWork sample gets you to next round.\nIt doesn't have to be perfect.\nIt has to be good enough to make them want to talk to you.\n\nShip it and move on.\n\n</meta_guidance>\n\n</pm_work_sample>",
    "technique": "Strategic analysis, clear communication, professional presentation",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to show your thinking for job application or take-home assignment"
  },
  {
    "name": "Draft Referral Request Email",
    "category": "Career",
    "prompt": "<referral_email>\n\n<referral_inputs>\nWHO YOU'RE ASKING:\n- Name: [Their name]\n- Your relationship: [How you know them]\n- [ ] Close friend/former colleague\n- [ ] Acquaintance/met a few times\n- [ ] LinkedIn connection only\n- [ ] Cold reach (alumni, mutual connection)\n\nTHE JOB:\n- Company: [Company name]\n- Role: [Job title]\n- Why you want it: [Brief reason]\n- How you found it: [Job posting, their post, etc.]\n\nWHAT YOU KNOW ABOUT THEM:\n- Their role at company: [Current position]\n- How long there: [Tenure]\n- Your connection: [What you have in common]\n\nYOUR CONSTRAINT:\n- [ ] Need referral ASAP (deadline)\n- [ ] Just exploring\n- [ ] Already applied, need boost\n</referral_inputs>\n\n<email_framework>\n\nYou write referral requests that respect relationships and make it easy to say yes. Your job: clear ask, low friction, graceful out.\n\n---\n\n## REFERRAL EMAIL TEMPLATES\n\n### Template 1: Former Colleague (Warm)\n\nSubject: Quick favor - [Company] PM role\n\n---\n\nHi [Name],\n\nHope you're doing well! Saw you're at [Company] now - congrats, seems like a great fit.\n\nI'm looking at the [Role Title] position on your team and would love to throw my hat in the ring. I know you're busy, but would you be comfortable giving me a referral?\n\nQuick context: [1-2 sentences about why you're interested and why you're qualified]\n\nHappy to send my resume and any other info that would help. Totally understand if now's not a good time - no pressure either way!\n\nLet me know,\n[Your name]\n\n---\n\n### Template 2: Acquaintance (Need to Reconnect)\n\nSubject: [Mutual connection/Context] - reaching out\n\n---\n\nHi [Name],\n\nHope this isn't too out of the blue - we [met at X / worked together on Y / connected through Z]. I've been following your work at [Company] and impressed with [specific thing about their product or their work].\n\nI'm exploring PM roles and saw an opening for [Role] at [Company]. Given your experience there, would you have 15 minutes to chat about the team and role? I'd also love to ask if you'd be open to referring me if it seems like a good fit.\n\nNo worries at all if you're swamped - I know these asks can be a lot.\n\nThanks for considering,\n[Your name]\n\n---\n\n### Template 3: LinkedIn Connection (Weaker Tie)\n\nSubject: [Company] PM role - asking for advice\n\n---\n\nHi [Name],\n\nWe're connected on LinkedIn through [mutual connection / same school / same interest], and I've been following your journey at [Company].\n\nI'm considering applying for the [Role] position and was hoping to get your perspective on the team and role before I apply. Would you have 15-20 minutes for a quick call?\n\nIf it seems like a good fit after we chat, I'd be grateful for any help with a referral - but totally understand if that's not possible.\n\nEither way, would love to learn from your experience at [Company].\n\nBest,\n[Your name]\n\n---\n\n### Template 4: Cold Reach (Alumni/Shared Background)\n\nSubject: Fellow [School/Background] alum - [Company] PM role\n\n---\n\nHi [Name],\n\nI'm reaching out as a fellow [School] alum (found you through [source]). I saw you're a PM at [Company] and wanted to reach out about the [Role] opening.\n\nQuick background: [1-2 sentences about your experience and why you're interested in this specific role]\n\nI know this is a cold email, but I'd really appreciate any insights into the team and role. If you think I'd be a good fit and you're comfortable with it, I'd be grateful for a referral.\n\nTotally understand if you prefer not to - I know you don't know me. Either way, thanks for considering.\n\n[Your name]\n\n[LinkedIn profile link]\n[Resume attached or link]\n\n---\n\n## EMAIL COMPONENTS\n\n### The Subject Line\n\nGood subjects:\n- \"Quick favor - [Company] PM role\"\n- \"[Mutual connection] suggested I reach out\"\n- \"Fellow [background] - exploring [Company]\"\n\nBad subjects:\n- \"Help!\" (vague)\n- \"Referral please\" (demanding)\n- \"Opportunity at [Company]\" (sounds like spam)\n\n---\n\n### The Opening\n\nIf you know them well:\n\"Hope you're doing well! [Personal touch about them or shared memory]\"\n\nIf acquaintance:\n\"Hope this isn't too out of the blue - we [context of how you met]\"\n\nIf cold:\n\"I'm reaching out as a [shared background] - found you through [source]\"\n\n---\n\n### The Ask\n\nMake it clear and low-friction:Good asks:\n- \"Would you be comfortable giving me a referral?\"\n- \"Would you be open to referring me if you think I'd be a good fit?\"\n- \"Could I get 15 minutes to learn about the role and potentially ask for a referral?\"\n\nBad asks:\n- \"Can you help me get a job?\" (too vague)\n- \"Will you refer me?\" (too demanding)\n- \"Let me know ASAP\" (too pushy)\n\n---\n\n### Why You're Qualified\n\nKeep it to 1-2 sentences:Good:\n\"I'm a PM with 3 years at [company], focused on [relevant area]. I've shipped [relevant accomplishment] which seems aligned with this role.\"\n\nBad:\n\"I have 5 years experience doing various things...\" (too vague)\n[Full resume in email] (too much)\n\n---\n\n### The Graceful Out\n\nAlways include:\n\"Totally understand if now's not a good time - no pressure either way.\"\n\"No worries at all if you're swamped.\"\n\"Either way, appreciate you considering it.\"\n\nWhy this matters:\n- Removes pressure\n- Makes them more likely to help\n- Maintains relationship if they say no\n\n---\n\n### Call-to-Action\n\nIf warm:\n\"Let me know if you need my resume or any other info!\"\n\nIf cold:\n\"Happy to send my resume or set up a quick call - whatever works best for you.\"\n\nKeep it open-ended, let them choose next step\n\n---\n\n## FOLLOW-UP STRATEGY\n\n### If they say yes:\n\nImmediate response:\n\n\"Thank you so much! Really appreciate it.\n\nHere's my info:\n- Resume: [attach or link]\n- LinkedIn: [link]\n- Job posting: [link]\n\nLet me know if you need anything else. Thanks again!\"\n\n---\n\n### If they say \"let's chat first\":\n\nResponse:\n\n\"Absolutely! Here are some times that work for me:\n- [Option 1]\n- [Option 2]\n- [Option 3]\n\nOr feel free to send a calendar link if that's easier.\"\n\n---\n\n### If they don't respond:\n\nWait 1 week, then:\n\n\"Hi [Name],\n\nJust bumping this up in case it got buried. Totally understand if you're busy or prefer not to - no worries at all!\n\n[Your name]\"\n\nOne follow-up max. Then let it go.\n\n---\n\n### If they say no:\n\nResponse:\n\n\"Totally understand - thanks for considering it!\n\nIf you have any other advice about [Company] or the PM role there, I'd still love to hear it. But no pressure either way.\n\nThanks again,\n[Your name]\"\n\nMaintain relationship, don't burn bridge\n\n</email_framework>\n\n<meta_guidance>\n\nReal talk about referrals:Referrals work:\n- 5-10x higher chance of interview\n- Worth the awkwardness\n- Most people want to help\n\nRelationship matters:\n- Close friend: Direct ask, informal\n- Acquaintance: Coffee first, referral second\n- Cold: Very low expectations, focus on learning\n\nMake it easy for them:\n- Clear subject line\n- Brief email (<150 words)\n- Specific ask\n- All info ready (resume, link)\n- Easy to say no\n\nDon't:\n- Send without context\n- Ask multiple people at same company\n- Be pushy about timeline\n- Follow up more than once\n- Take rejection personally\n\nTiming:\n- Ask before you apply (referral works better)\n- Give them 2-3 days to respond\n- One follow-up after 1 week\n- Then move on\n\nRemember:\n\nMost people say yes.\nBut some will say no.\nThat's okay.\n\nOne good referral > ten perfect emails.\n\nSend it and move on.\n\n</meta_guidance>\n\n</referral_email>",
    "technique": "Relationship-appropriate tone, clear ask, low friction",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need referral for job but don't want to be awkward or pushy"
  },
  {
    "name": "Generate Interview Practice Questions",
    "category": "Career",
    "prompt": "<interview_prep>\n\n<prep_inputs>\nPASTE YOUR RESUME:\n[Copy-paste your resume]\n\nPASTE THE JOB DESCRIPTION:\n[Copy-paste the full JD]\n\nWHAT YOU KNOW ABOUT INTERVIEW:\n- Interview type: [ ] Phone screen [ ] Hiring manager [ ] Panel [ ] Final round\n- Who's interviewing you: [Names/roles if you know]\n- Interview focus: [ ] General fit [ ] Technical/execution [ ] Strategic [ ] Behavioral\n- Duration: [Time length]\n- Any other context: [Company stage, known priorities, etc.]\n\nYOUR CONCERNS:\n- [ ] Gap in resume to explain\n- [ ] Career transition story\n- [ ] Specific weak area (analytics, technical, etc.)\n- [ ] Less experience than required\n- [ ] Other: [What you're worried about]\n</prep_inputs>\n\n<prep_framework>\n\nYou generate practice questions they'll likely ask. Your job: realistic questions based on actual interview patterns, plus guidance on how to answer.\n\n---\n\n## YOUR INTERVIEW PREP\n\n### Questions They'll Definitely Ask\n\nThese come up in 90%+ of PM interviews:\n\n---\n\nQ1: \"Walk me through your background / Tell me about yourself\"Why they ask:\nIcebreaker, see how you tell your story\n\nWhat they're evaluating:\n- Can you be concise?\n- Is there a narrative arc?\n- Why PM, why now, why here?\n\nYour answer structure:\n\n\"I'm a PM with [X years] focused on [area]. Currently at [Company] where I [key accomplishment].\n\nBefore that, I was at [Previous] where I [relevant experience].\n\nI got into PM because [brief why PM story].\n\nI'm excited about [Company] because [specific reason related to role].\"\n\nKeep it to 90 seconds max.Your draft:\n[Write your 90-second version here]\n\n---\n\nQ2: \"Why do you want to work here / Why are you interested in this role?\"Why they ask:\nChecking if you actually care or just spray-and-pray\n\nWhat they're evaluating:\n- Did you do homework?\n- Genuine interest or just need a job?\n- Understand what we do?\n\nBad answer:\n\"Great company, good culture, want to grow\"\n\nGood answer:\n\"Three reasons: [Specific thing about product], [Specific thing about market/mission], [Specific thing about role that aligns with your experience]\"\n\nYour draft:\n[Write your specific answer]\n\n---\n\nQ3: \"Tell me about a product you shipped\"Why they ask:\nCan you execute? Do you understand PM fundamentals?\n\nWhat they're evaluating:\n- Full lifecycle understanding\n- Metrics-driven\n- Cross-functional collaboration\n- Problem-solving\n\nAnswer framework (STAR):Situation: [Context - what problem, why important]\n\nTask: [What you needed to do]\n\nAction: [What you actually did - be specific]\n- Discovery: [How you validated problem]\n- Execution: [How you prioritized, worked with eng/design]\n- Measurement: [How you tracked success]\n\nResult: [Outcome with metrics - X → Y, improved by Z%]\n\nYour story:\n[Pick your best product story and draft the STAR]\n\n---\n\nQ4: \"Tell me about a time you had to say no\"Why they ask:\nCan you prioritize? Do you just say yes to everything?\n\nWhat they're evaluating:\n- Judgment\n- Stakeholder management\n- Data-driven decisions\n\nAnswer framework:Situation: [Stakeholder wanted X]\n\nWhy you said no: [Data, strategy, resources - the real reason]\n\nHow you communicated it: [What you actually said]\n\nAlternative offered: [What you did instead, if anything]\n\nOutcome: [What happened - did they understand? Did it work out?]\n\nYour story:\n[Draft your example]\n\n---\n\n### Questions Based on Your Resume\n\nThese are specific to your background:\n\n---\n\nQ5: About [Specific project from your resume]\n\n\"I see you [project]. Can you walk me through that?\"\n\nThey want to know:\n- What was your actual role (vs team's work)\n- How you made decisions\n- What you learned\n\nPrepare:\n- The problem you were solving\n- How you approached it\n- What you'd do differently now\n\n---\n\nQ6: About [Gap or transition in your resume]\n\n\"I notice you [gap / switched from X to Y / left Company after short time]. Can you tell me about that?\"\n\nThey're checking:\n- Any red flags?\n- Valid reasons?\n- What did you learn?\n\nYour answer:\n[Be honest, brief, forward-looking]\n\n\"[Honest reason]. What I learned: [Takeaway]. Since then, [What you've done].\"\n\n---\n\nQ7: About [Skill they need that you might lack]\n\n\"This role requires [X]. I see you've done [related Y]. How would you approach [X]?\"\n\nThey're testing:\n- Self-awareness\n- Learning agility\n- Transferable skills\n\nYour answer:\n\"I haven't done [X] specifically, but I have experience with [related Y]. Here's how I'd approach [X]: [Show your thinking]. I'm a fast learner - for example, [story of learning something quickly].\"\n\n---\n\n### Questions Based on the Job\n\nThese are specific to what they need:\n\n---\n\nQ8: [Their main problem]\n\nBased on JD, they're struggling with: [Problem]\n\nLikely question:\n\"How would you approach [their problem]?\"\n\nor\n\n\"What's your experience with [their challenge]?\"\n\nYour answer structure:\n1. Show you understand the problem\n2. Share relevant experience\n3. Outline how you'd approach it here\n\nYour draft:\n[Prepare your answer]\n\n---\n\nQ9: [Their product/strategy]\n\n\"What do you think of our product?\"\n\nor\n\n\"If you were CEO, what would you prioritize?\"\n\nThey're checking:\n- Did you use the product?\n- Do you have product sense?\n- Can you think strategically?\n\nYour answer:\n[Prepare 2-3 specific observations/suggestions]\n\n---\n\nQ10: [Their key metric]\n\n\"How would you improve [metric they care about]?\"\n\nYour answer:\n- Clarify the baseline\n- Hypothesize why it's at current level\n- Suggest 2-3 approaches to test\n- Explain how you'd prioritize\n\n---\n\n### Behavioral Questions (Most Common)\n\nQ11: \"Tell me about a time you failed\"Framework:\n- What happened (own it)\n- What you learned\n- What you do differently now\n\n---\n\nQ12: \"Tell me about a conflict with a teammate\"Framework:\n- The disagreement (be specific)\n- How you resolved it (show maturity)\n- Outcome (focus on learning/growth)\n\n---\n\nQ13: \"How do you prioritize?\"Framework:\n- Your framework (RICE, impact/effort, etc.)\n- Example of using it\n- Trade-offs you consider\n\n---\n\nQ14: \"How do you work with engineers?\"Framework:\n- Your philosophy (respect, collaboration, not just task giver)\n- Specific example\n- What you've learned about what works\n\n---\n\nQ15: \"Tell me about a time you influenced without authority\"Framework:\n- Context (why you had no authority)\n- Your approach (data, storytelling, building coalition)\n- Outcome\n\n---\n\n### Questions to Test Product Sense\n\nQ16: \"How would you improve [popular product]?\"Framework:\n1. Clarify: Who's the user? What goal?\n2. Problem: What's broken or missing?\n3. Solution: 2-3 options\n4. Prioritize: Which and why\n5. Measure: How you'd know it worked\n\n---\n\nQ17: \"Design [product] for [user]\"Framework:\n1. Clarify constraints\n2. User needs (what job to be done?)\n3. Key features (prioritize 3)\n4. How it works (simple flow)\n5. Success metrics\n\n---\n\nQ18: \"Should we build [feature]?\"Framework:\n1. Ask clarifying questions (who wants it, why, alternatives)\n2. Framework (pros/cons, impact/effort)\n3. Recommendation (yes/no with reasoning)\n4. How to validate (experiment plan)\n\n---\n\n### Analytical/Technical Questions\n\nQ19: \"How would you measure success of [feature]?\"Framework:\n- North star metric\n- Supporting metrics\n- Guardrail metrics (what might go wrong)\n\n---\n\nQ20: \"Estimate [market size / users / revenue]\"Framework:\n- Clarify question\n- Break down (top-down or bottom-up)\n- Show your math\n- State assumptions\n- Sanity check\n\n---\n\nQ21: \"Diagnose why [metric] dropped\"Framework:\n1. Clarify (how much, when, which segment)\n2. Hypotheses (5+ possible causes)\n3. How to investigate (what data to check)\n4. Prioritize hypotheses\n5. Recommend solution\n\n---\n\n### Strategy Questions\n\nQ22: \"Where should [product] be in 3 years?\"Framework:\n- Market trends\n- User needs evolution\n- Competitive landscape\n- Your vision (specific, ambitious, achievable)\n\n---\n\nQ23: \"Should we enter [market / build new product]?\"Framework:\n- Size the opportunity\n- Assess fit (why us, why now)\n- Risks\n- Recommendation\n\n---\n\n### Questions You Should Ask Them\n\nAbout the role:\n- \"What does success look like in first 90 days?\"\n- \"What's the biggest challenge this role will tackle?\"\n- \"Why is this role open / what happened to last person?\"\n\nAbout the team:\n- \"How does PM team work with eng/design?\"\n- \"How are decisions made?\"\n- \"What's the team's biggest strength and biggest opportunity?\"\n\nAbout the company:\n- \"What keeps you up at night about the business?\"\n- \"What's your biggest competitive threat?\"\n- \"What would make this role obsolete in 2 years?\"\n\n---\n\n### Questions Based on Your Specific Concerns\n\nIF: Gap in resumeThey might ask:\n\"What have you been doing for the past [timeframe]?\"\n\nYour answer:\n[Honest, brief reason + what you learned/did + why you're ready now]\n\n---\n\nIF: Career transition (not PM before)They might ask:\n\"Why transition to PM now?\"\n\nYour answer:\n- What you loved about previous role that's PM-adjacent\n- What you've done to learn PM (courses, side projects, books)\n- Why this specific PM role is perfect entry point\n\n---\n\nIF: Less experience than requiredThey might ask:\n\"This role says [X years], you have [Y years]. Why should we consider you?\"\n\nYour answer:\n- Quality of experience (accomplished more in less time)\n- Relevant depth (you've done exactly what they need)\n- Learning velocity (show you close gaps fast)\n\n</prep_framework>\n\n<meta_guidance>\n\nInterview prep reality:You won't remember scripted answers:\n- Don't memorize word-for-word\n- Know your stories and frameworks\n- Be conversational\n\nPractice out loud:\n- Record yourself\n- Practice with friend\n- Time yourself (most answers should be 2-3 minutes)\n\nHave 5-6 solid stories:\n- Product you shipped\n- Conflict you resolved\n- Time you failed\n- Time you influenced\n- Data-driven decision\n- Customer insight\n\nUse these stories across multiple questions:\nSame story can answer \"conflict,\" \"influence,\" \"data-driven,\" etc.\n\nThe 90-second rule:\nMost answers should be 90 seconds to 2 minutes\nException: Product deep-dives can go 3-5 minutes\n\nAsk clarifying questions:\n\"That's a great question. Can I ask - are you more interested in [X] or [Y] aspect?\"\nShows PM thinking, buys you time\n\nRemember:\n\nInterview is conversation, not interrogation.\n\nThey want you to succeed.\nThey're rooting for you.\n\nIf you get stuck, it's okay to say:\n\"Let me think about that for a second...\"\n\nBe human. Be honest. Show your thinking.\n\n</meta_guidance>\n\n</interview_prep>",
    "technique": "Pattern matching, question prediction, answer frameworks",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Interview coming up, need to practice likely questions based on your background"
  },
  {
    "name": "Review Interview Transcript",
    "category": "Career",
    "prompt": "<review_pm_interview>\n\n<transcript_inputs>\nPASTE WHAT HAPPENED:\n[Interview transcript or your detailed notes of Q&A]\n\nTHE ROLE:\n- Company: [Name]\n- Stage: [ ] Early startup [ ] Growth [ ] Late stage/enterprise\n- Product type: [B2B/B2C, what they build]\n- What they're hiring for: [From JD - what problems PM will solve]\n\nYOUR ANSWERS TO KEY QUESTIONS:\n[Paste or describe your responses to their main questions]\n</transcript_inputs>\n\n<analysis_framework>\n\nYou're a PM interview coach analyzing performance. Follow this thinking process:\n\nSTEP 1: Identify what PM competency each question tested\n\nFor each question, determine:\n- Product sense (understanding users, problems, solutions)\n- Execution (shipping, working with eng, metrics)\n- Strategy (vision, prioritization, market thinking)\n- Leadership (influence, conflict, stakeholder management)\n- Technical (understanding systems, APIs, tradeoffs)\n- Analytical (metrics, data, estimation)\n\nSTEP 2: Evaluate answer quality against PM standards\n\nFor each answer, assess:\n- Structure: Did they use a framework (STAR, CIRCLES, etc.)?\n- Specificity: Concrete examples with names, numbers, dates?\n- User focus: Did they talk about users or just features?\n- Metrics: Did they quantify impact (X → Y, improved by Z%)?\n- Tradeoffs: Did they acknowledge what you're NOT doing?\n- Business connection: Linked to company goals/revenue?\n- Realism: Understood constraints, didn't over-promise?\n\nSTEP 3: Check for PM failure modes\n\nCommon ways PMs fail interviews:\n- Feature factory: Lists features, no strategy/why\n- No metrics: Builds things, never measures success\n- User-blind: Talks about technology, not problems\n- Can't prioritize: Everything is P0, no tradeoffs\n- Too vague: \"Worked with stakeholders\" (doing what?)\n- Blame culture: \"Eng was slow, design was bad\"\n- Lack of ownership: \"Team did X\" not \"I did X\"\n\nMark which failure modes appeared in this interview.\n\nSTEP 4: Grade core PM competencies\n\nScore A/B/C/D on:\n- Product sense (do they get users?)\n- Strategic thinking (see big picture?)\n- Execution ability (can they ship?)\n- Communication (clear and structured?)\n- Metrics fluency (data-driven?)\n- Stakeholder management (influence?)\n\nSTEP 5: Identify the gap pattern\n\nIs the main issue:\n- Inexperience showing: Right thinking, no stories\n- Wrong role fit: Not PM thinking (too IC or too high-level)\n- Poor preparation: Good PM, bad interview skills\n- Communication: Good work, can't articulate it\n- Red flags: Concerning behaviors (blaming, defensive)\n\nSTEP 6: Build specific improvement plan\n\nBased on gaps, prescribe:\n- Which 5-6 stories to prepare (with STAR structure)\n- Which PM frameworks to learn (RICE, Jobs-to-be-done, etc.)\n- How to reframe weak answers\n- Practice exercises (mock interviews, case studies)\n- Knowledge gaps to fill (metrics, technical, strategy)\n\n</analysis_framework>\n\n---\n\n## INTERVIEW PERFORMANCE REPORT\n\n### Overall Assessment\n\nPerformance Grade: [A/B/C/D]One-line summary:\n[Did they demonstrate PM competency? Pass/borderline/fail?]\n\nMain strengths:\n- [What they did well]\n- [Another strength]\n\nMain gaps:\n- [Critical issue 1]\n- [Critical issue 2]\n\nLikely outcome: [Advance / Maybe / Probably not]\n\n---\n\n### PM Competency Scorecard\n\n| Competency | Grade | Evidence |\n|------------|-------|----------|\n| Product Sense | [A/B/C/D] | [Why this grade] |\n| Strategic Thinking | [A/B/C/D] | [Why] |\n| Execution Ability | [A/B/C/D] | [Why] |\n| Communication | [A/B/C/D] | [Why] |\n| Metrics/Analytical | [A/B/C/D] | [Why] |\n| Stakeholder Mgmt | [A/B/C/D] | [Why] |\n\nOverall: [Pass/Borderline/Fail]\n\n---\n\n### Question-by-Question Analysis\n\nQ1: \"[The question]\"Competency tested: [What this question was assessing]\n\nYour answer summary:\n[What you said in 2-3 sentences]\n\nWhat worked:\n✅ [Specific good thing - e.g., \"Used STAR format\"]\n✅ [Another strength]\n\nWhat didn't work:\n❌ [Specific issue - e.g., \"No metrics mentioned\"]\n❌ [Another problem]\n\nPM failure modes present:\n- [ ] Feature factory thinking\n- [ ] No metrics\n- [ ] User-blind\n- [ ] Can't prioritize\n- [ ] Too vague\n- [ ] Blame culture\n\nScore: [A/B/C/D]How a strong PM would answer this:\n\n[Rewritten answer showing proper structure]\n\n\"Situation: [Context in 1 sentence]\nTask: [What needed to happen]\nAction: [What you did - specific steps]\n- Talked to 10 customers, found [insight]\n- Prioritized using [framework] because [reason]\n- Worked with eng on [specific approach]\nResult: [Metric] went from X → Y in Z timeframe, leading to [business impact]\"\n\nWhy this is better:\n- Specific metrics (X → Y)\n- Shows PM process (discovery → decision → ship)\n- User-focused\n- Demonstrates judgment\n\n---\n\n[Repeat for each major question]\n\n---\n\n### Critical Patterns\n\nSTRENGTH PATTERN:\n[What you consistently did well across answers]\n\nExample: \"You always included metrics and showed data-driven thinking. Every answer had concrete numbers.\"\n\n---\n\nWEAKNESS PATTERN:\n[What you consistently struggled with]\n\nExample: \"You never mentioned users. Every answer was about features and technology, not problems being solved.\"\n\n---\n\nPM FAILURE MODES DETECTED:❌ Feature Factory Thinking:\n- [Example from interview]\n- Impact: Makes you seem tactical, not strategic\n\n❌ No Metrics:\n- [Example]\n- Impact: Can't prove you create value\n\n❌ [Other failure mode]:\n- [Example]\n- [Impact]\n\n---\n\n### Red Flags Raised\n\nConcerns interviewer may have:\n\n🚩 [Concern 1]:\n- What triggered it: [Specific moment]\n- Why it's a problem: [Perception created]\n- How to address: [What to do differently]\n\n🚩 [Concern 2]:\n[Same structure]\n\n---\n\n### Strongest Moments\n\nBest answers:1. [Question]\n- Why it was strong: [Specific, structured, showed PM skills]\n- What made it work: [Framework used, metrics included, etc.]\n\n2. [Question]\n- Why: [Clear thinking, good judgment, etc.]\n\nUse these as your template.\n\n---\n\n### Weakest Moments\n\nAnswers that hurt you:1. [Question]\n- Why it was weak: [Rambling, no point, no metrics]\n- What was missing: [Structure, specifics, user focus]\n- Likely impact: [How interviewer perceived this]\n\n2. [Question]\n[Same structure]\n\nThese need complete rewrites.\n\n---\n\n### Answer Rewrites\n\nFor your 3 weakest answers:\n\n---\n\nQUESTION: \"[Question you struggled with]\"\n\nWhat you said:\n[Your actual rambling/unclear answer]\n\nWhy this failed:\n- [Problem 1: Too vague]\n- [Problem 2: No metrics]\n- [Problem 3: Didn't answer question]\n\nRewritten answer:\n\n[Proper PM answer using framework]\n\nWhy this version works:\n- [Reason 1]\n- [Reason 2]\n\n---\n\n[Repeat for 2 more weak answers]\n\n---\n\n### Improvement Plan\n\nIMMEDIATE (Before next interview):Stories to prepare:\n1. [Situation]: Product you shipped - NEEDS metrics, user impact, why decisions made\n2. [Situation]: Conflict with stakeholder - NEEDS how you influenced, data you used\n3. [Situation]: Failed launch - NEEDS what you learned, what you'd do differently\n4. [Situation]: Prioritization decision - NEEDS framework, tradeoffs, outcome\n5. [Situation]: Technical decision - NEEDS why, constraints, business impact\n6. [Situation]: Data-driven decision - NEEDS hypothesis, experiment, result\n\nFor each story, write out full STAR format. Practice out loud 3x each.\n\n---\n\nPM frameworks to learn/review:\n- [ ] [Framework]: Because you need [competency]\n- [ ] [Framework]: To answer [question type] better\n- [ ] [Framework]: To show [PM skill]\n\nResources:\n[Specific articles/videos for gaps identified]\n\n---\n\nAnswer structure practice:\n- Time yourself (2-3 min max)\n- Record and listen for:\n- Filler words (um, like, you know)\n- Lack of structure\n- Missing metrics\n- Vague language\n\n---\n\nSHORT-TERM (This week):\n\n- [ ] Complete story rewrites with STAR format\n- [ ] Mock interview with PM friend\n- [ ] Practice product sense questions (how would you improve X?)\n- [ ] Review: [Specific company/product] deeper\n- [ ] Fix resume bullet about [weak experience] to be clearer\n\n---\n\nKNOWLEDGE GAPS TO FILL:\n\nBased on struggles:\n- [ ] [Topic]: Read [resource]\n- [ ] [Skill]: Practice [exercise]\n- [ ] [Framework]: Watch [video]\n\n---\n\n### If You Advance\n\nFor next round, focus on:\n1. [Competency to emphasize]\n2. [Story type to prepare]\n3. [Skill to demonstrate better]\n\nLikely next round questions:\n[Based on role and what they're hiring for]\n\n---\n\n### If You Don't Advance\n\nKey learnings:\n1. [Lesson 1]\n2. [Lesson 2]\n3. [Lesson 3]\n\nApply these to next interview.Next steps:\n- [ ] Thank you email (still send it)\n- [ ] Practice improved answers\n- [ ] Apply lessons to next opportunity\n\n---\n\n### Bottom Line\n\nWhat cost you this interview:\n[The 1-2 main issues that hurt most]\n\nWhat would make you successful next time:\n[The 1-2 things to focus on]\n\nYou're [ready / almost ready / need more prep] for PM interviews at [stage/type] companies.\n\n</review_pm_interview>",
    "technique": " competency assessment, answer quality evaluation",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Just finished interview (or mock), need feedback on what you said"
  },
  {
    "name": "Sprint Planning",
    "category": "Operations",
    "prompt": "<sprint_planning>\n\n<sprint_inputs>\nTEAM CAPACITY:\n- Engineers: [Number available this sprint]\n- Sprint length: [1 or 2 weeks]\n- Historical velocity: [Points per sprint, or just \"we usually finish X tickets\"]\n- This sprint's constraints: [PTO, holidays, other work]\n\nPASTE BACKLOG:\n[Your prioritized list of work - tickets, features, bugs, tech debt]\n\nCONTEXT:\n- Recent velocity: [Last 3 sprints - did you over/under commit?]\n- Team health: [ ] Crushing it [ ] Normal [ ] Struggling/underwater\n- Big picture: [What is this sprint working toward?]\n</sprint_inputs>\n\n<planning_framework>\n\nYou help PMs plan realistic sprints. Your analysis process:\n\nStep 1: Calculate actual capacity\nStart with theoretical capacity, then account for reality:\n- Meetings take 20-30% of time\n- Bug fixes and support interruptions\n- PR reviews and code review\n- Onboarding if new team members\n- Scope creep and unknowns\n\nRule of thumb: Plan for 60-70% of theoretical capacity.\n\nStep 2: Analyze the backlog\nFor each item, assess:\n- Is it actually ready to start? (design done, requirements clear, no blockers)\n- What's the real size? (not story points, but \"will this take days or weeks?\")\n- Dependencies on other work or teams?\n- Risk level: well-understood vs lots of unknowns\n- Can it be split into smaller chunks?\n\nStep 3: Apply prioritization forcing function\nIf backlog has 20 things and you can do 8, use this process:\n- What breaks if we don't do it? (critical)\n- What's blocking other work? (unblock)\n- What's needed for upcoming deadline? (time-sensitive)\n- What's high value/low effort? (quick wins)\n- What can wait? (defer)\n\nStep 4: Sequence intelligently\nDon't just stack work randomly:\n- Front-load risky/unknown work (so you have time to adjust)\n- Group related work (context switching is expensive)\n- Sequence dependencies (do foundation before feature)\n- Leave buffer at end (something will go wrong)\n\nStep 5: Reality check\nRed flags that you're over-committing:\n- Planned capacity = 100% of team time\n- All work is \"critical\"\n- Multiple large unknowns\n- Dependencies on other teams\n- No buffer for bugs/support\n- Last 3 sprints you missed goals\n\nStep 6: Define what \"done\" means\nFor this sprint specifically:\n- What's the sprint goal in one sentence?\n- What's the minimum to call sprint successful?\n- What would you cut if week 1 goes poorly?\n- What's your flex capacity if things go well?\n\nNow create a realistic sprint plan. Show your reasoning about what's in, what's out, and why.\n\n</planning_framework>\n\n</sprint_planning>",
    "technique": "Capacity planning, dependency mapping, realistic scoping",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Turn messy backlog into realistic 2-week sprint plan"
  },
  {
    "name": "Backlog Grooming",
    "category": "Operations",
    "prompt": "\n\n<backlog_grooming>\n\n<backlog_inputs>\nPASTE YOUR MESSY BACKLOG:\n[List of tickets/stories that need grooming - just titles is fine, or full descriptions if you have them]\n\nBACKLOG CONTEXT:\n- How many tickets total: [Number]\n- How old is oldest ticket: [Timeframe]\n- Common issues: [ ] Vague requirements [ ] No priorities [ ] Duplicates [ ] Stale [ ] All of the above\n\nYOUR GOALS:\n- [ ] Clean up old/stale tickets\n- [ ] Write better requirements  \n- [ ] Prioritize properly\n- [ ] Prep for sprint planning\n- [ ] All of the above\n</backlog_inputs>\n\n<grooming_framework>\n\nYou help PMs turn messy backlogs into actionable work. Follow this process:\n\n**STEP 1: Categorize every ticket**\n\nPut each ticket in a bucket:\n\n**READY (can be worked on next sprint):**\n- Requirements clear and agreed\n- Design complete if needed\n- No blockers\n- Sized/estimated\n- Acceptance criteria defined\n\n**NEEDS WORK (not ready but should be):**\n- Vague description\n- Missing acceptance criteria  \n- Not sized\n- Design needed but not done\n- Needs spike/research first\n\n**BLOCKED (can't work on yet):**\n- Waiting on other team\n- Dependency not done\n- Need customer feedback first\n- Technical blocker\n\n**STALE (probably close/archive):**\n- >6 months old\n- No one cares anymore\n- Better solution exists\n- Not aligned to strategy\n\n**DUPLICATE (merge or close):**\n- Same ask as other ticket\n- Subset of bigger ticket\n\n**STEP 2: Assess ticket quality**\n\nFor each ticket, check these elements:\n\n**Title:**\n- [ ] Clear what it is (not \"Fix bug\" but \"Fix login error on mobile Safari\")\n- [ ] Starts with verb (Add, Fix, Update, Remove)\n- [ ] Short enough to scan (<60 chars)\n\n**Description:**\n- [ ] User story format: \"As [user], I want [action] so that [benefit]\"\n- [ ] OR: Problem statement: \"Currently [pain], we should [solution]\"\n- [ ] Context: Why this matters\n\n**Acceptance criteria:**\n- [ ] Specific, testable conditions\n- [ ] Format: \"Given [context], when [action], then [outcome]\"\n- [ ] Covers happy path and edge cases\n\n**Size/estimate:**\n- [ ] Has story points or t-shirt size\n- [ ] Team calibrated on what size means\n- [ ] If >8 points, should be split\n\n**Dependencies:**\n- [ ] Lists what must happen first\n- [ ] Lists what's blocked by this\n\n**Good ticket example:**\n```\nTitle: Add password reset for mobile app users\n\nAs a mobile app user who forgot their password\nI want to reset it from the login screen\nSo I can regain access without contacting support\n\nAcceptance Criteria:\n- Given I'm on login screen, when I tap \"Forgot password\", then I see email input\n- Given I enter valid email, when I submit, then I get reset link via email\n- Given invalid email, when I submit, then I see error \"Email not found\"\n- Given I click reset link, when I create new password, then I can login\n\nSize: 5 points\nDependencies: None\nDesign: [Figma link]\n```\n\n**Bad ticket example:**\n```\nTitle: Password thing\n\nWe should let users reset passwords\n\nSize: ?\n```\n\n**STEP 3: Apply prioritization framework**\n\nFor each ticket, assign to one tier:\n\n**P0 (Now - next 1-2 sprints):**\n- Blocking other work\n- Customer commitment\n- Critical bug\n- Regulation/security issue\n\n**P1 (Soon - next 1-2 months):**\n- Moves key metric  \n- High customer demand\n- Strategic initiative\n- Major improvement\n\n**P2 (Later - this quarter):**\n- Nice to have\n- Small improvements\n- Low-hanging fruit\n\n**P3 (Backlog - someday/maybe):**\n- Good idea but not urgent\n- Needs more validation\n- Low impact\n\n**Don't prioritize (close/archive):**\n- Stale (>6 months, no interest)\n- Duplicate\n- Doesn't align to strategy\n- Better solution exists\n\n**STEP 4: Identify tickets that need splitting**\n\nBig tickets (>8 points) should be broken down:\n\n**How to split:**\n- By user flow (checkout flow → cart + payment + confirmation)\n- By platform (web + mobile)\n- By MVP vs polish (working version + nice-to-haves)\n- By happy path vs edge cases (core flow + error handling)\n\n**Red flag patterns:**\n- Ticket with \"and\" multiple times (\"Add X and Y and Z\")\n- Ticket that takes >2 weeks\n- Ticket with 10+ acceptance criteria\n- Ticket that's actually an epic\n\n**STEP 5: Write missing acceptance criteria**\n\nFor tickets without clear AC, create them:\n\n**Format:**\n```\nGiven [starting state/context]\nWhen [user action]\nThen [expected result]\n```\n\n**Cover:**\n- Happy path (works as expected)\n- Error cases (what if it fails)\n- Edge cases (boundary conditions)\n- Non-functional (performance, security)\n\n**STEP 6: Kill what should die**\n\nBe ruthless about closing old tickets:\n\n**Close if:**\n- >6 months old and no one's asking\n- \"We should\" but no one actually cares\n- Duplicate of better ticket\n- Problem solved differently\n- Not aligned to current strategy\n\n**Archive vs Close:**\n- Archive = might revisit\n- Close = never doing this\n\n**Don't hoard backlog items \"just in case.\"**\n\n</grooming_framework>\n\n---\n\n## BACKLOG GROOMING REPORT\n\n### Summary\n\n**Backlog health:** [Good / Needs work / Disaster]\n\n**Total tickets:** [X]\n\n**Breakdown:**\n- Ready to work: [Y]\n- Need refinement: [Z]\n- Should close/archive: [W]\n\n**Top priority:**\n[The 1-2 things that should be worked on next]\n\n---\n\n### Ticket Quality Assessment\n\n**BY READINESS:**\n\n**✅ READY (Can sprint on these):** [X tickets]\n- [Ticket name]\n- [Ticket name]\n- [Ticket name]\n\n**⚠️ NEEDS WORK:** [Y tickets]\n\n**[Ticket name]**\n- **What's missing:** [ ] Clear requirements [ ] Acceptance criteria [ ] Design [ ] Sizing\n- **Blocker:** [What needs to happen to make this ready]\n- **Owner:** [Who should refine this]\n\n[Repeat for tickets needing work]\n\n**🚫 BLOCKED:** [Z tickets]\n- [Ticket]: Blocked by [dependency]\n- [Ticket]: Blocked by [external team]\n\n**💀 STALE/SHOULD CLOSE:** [W tickets]\n- [Ticket]: [Why close]\n- [Ticket]: [Why close]\n\n---\n\n### Priority Assignments\n\n**P0 - NOW (Next 1-2 sprints):**\n\n1. **[Ticket name]** - [Size]\n   - **Why P0:** [Blocking work / customer commit / critical bug]\n   - **Ready:** [Yes/No]\n\n2. [Continue]\n\n**Total P0 work:** [X points or days]\n\n---\n\n**P1 - SOON (Next 1-2 months):**\n\n1. **[Ticket name]** - [Size]\n   - **Why P1:** [Moves metric / strategic / high demand]\n   - **Ready:** [Status]\n\n[Continue]\n\n**Total P1 work:** [Y points]\n\n---\n\n**P2 - LATER (This quarter):**\n- [Ticket 1]\n- [Ticket 2]\n- [Total: Z tickets]\n\n---\n\n**P3 - SOMEDAY:**\n- [Ticket 1]\n- [Ticket 2]\n- [Total: W tickets]\n\n---\n\n###\n\n Tickets That Need Splitting\n\n**[Big ticket name]** - Currently [13 points]\n\n**Why split:** [Too big / Multiple features / Long timeline]\n\n**Proposed split:**\n\n**Part 1:** [Smaller piece] - [5 points]\n- [Specific scope]\n- [Acceptance criteria]\n\n**Part 2:** [Another piece] - [5 points]\n- [Specific scope]\n- [AC]\n\n**Part 3:** [Polish] - [3 points]\n- [Nice-to-haves]\n\n---\n\n[Repeat for other big tickets]\n\n---\n\n### Tickets with Improved Descriptions\n\n**Before:** \n```\nTitle: Fix search\n\nSearch is broken, make it better\n```\n\n**After:**\n```\nTitle: Fix mobile search returning 0 results for product names\n\nAs a mobile user searching for products\nI want search to return relevant results  \nSo I can find what I'm looking for\n\nCurrent problem: Searching \"iphone case\" returns 0 results, but product exists\n\nAcceptance Criteria:\n- Given I search for exact product name, then I see that product in results\n- Given I search with typo (1-2 letters off), then I see suggested products\n- Given I search for nonexistent product, then I see \"No results\" with suggested alternatives\n\nSize: 8 points\n```\n\n---\n\n[Show 3-5 examples of improved tickets]\n\n---\n\n### Tickets to Close/Archive\n\n**CLOSE (Never doing):**\n\n**[Ticket name]**\n- **Why close:** [Reason - duplicate / stale / not aligned / better solution]\n- **Alternative:** [If there's a better ticket for this, link it]\n\n[Repeat]\n\n**Total to close:** [X tickets]\n\n---\n\n**ARCHIVE (Maybe someday):**\n- [Ticket 1] - [Why archive not close]\n- [Ticket 2]\n- [Total: Y tickets]\n\n---\n\n### Duplicates Found\n\n**Keep: [Ticket A]**\n**Close:** [Ticket B, Ticket C]\n**Reason:** [They're all asking for same thing]\n\n---\n\n### Action Items\n\n**IMMEDIATE (This week):**\n- [ ] Close [X] stale tickets\n- [ ] Refine these P0 tickets: [List]\n- [ ] Get design for: [Tickets needing design]\n- [ ] Size these tickets: [Unestimated work]\n\n**BEFORE NEXT SPRINT PLANNING:**\n- [ ] All P0 tickets have clear AC\n- [ ] P0 tickets are sized\n- [ ] Split big tickets: [List]\n- [ ] Confirm with eng these are ready\n\n**ONGOING:**\n- [ ] Review P2/P3 monthly - close if stale\n- [ ] New tickets must have AC before labeling \"ready\"\n- [ ] Anything >6 months old gets reviewed\n\n---\n\n### Backlog Maintenance Recommendations\n\n**Current issues:**\n- [Pattern 1 - e.g., \"Too many vague tickets\"]\n- [Pattern 2 - e.g., \"Not closing old work\"]\n\n**Process improvements:**\n- [Suggestion 1]\n- [Suggestion 2]\n\n**Grooming cadence:**\n- Weekly: [What to review]\n- Monthly: [Deeper cleanup]\n\n</backlog_grooming>",
    "technique": "Ticket quality assessment, priority clarification, scope definition",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Clean up messy backlog, write better tickets, prioritize work"
  },
  {
    "name": "A/B Test Design\n",
    "category": "Analytics",
    "prompt": "\n\n<ab_test_design>\n\n<test_inputs>\nWHAT YOU WANT TO TEST:\n[Describe the change - new feature, UI change, algo tweak, etc.]\n\nWHY YOU'RE TESTING IT:\n- Hypothesis: [What you believe will happen]\n- Expected impact: [Metric you think will improve]\n- Risk: [What could go wrong if you ship to everyone]\n\nYOUR PRODUCT CONTEXT:\n- Daily/monthly active users: [Volume]\n- Current baseline metric: [The metric you're trying to move]\n- Existing conversion/engagement rate: [If relevant]\n- How users are assigned: [Session, user ID, random]\n\nCONSTRAINTS:\n- Timeline: [How long can you run test]\n- Traffic: [Can you split 50/50 or need different allocation]\n- Technical: [Any limitations on what you can test]\n</test_inputs>\n\n<test_framework>\n\nYou design A/B tests that produce reliable results. Follow this analysis:\n\n**STEP 1: Form a clear hypothesis**\n\nBad hypothesis: \"New design will be better\"\nGood hypothesis: \"Adding social proof to checkout will increase conversion rate from 5% to 6%\"\n\n**Hypothesis format:**\nIf we [change]\nThen [metric] will [improve by X%]\nBecause [reasoning based on user behavior]\n\n\n**Check:**\n- Is it specific? (Not \"improve,\" but \"increase by X%\")\n- Is it measurable? (Clear metric)\n- Is it testable? (Can you prove/disprove)\n- Is the expected change realistic? (6% → 7% yes, 6% → 60% no)\n\n**STEP 2: Choose primary metric carefully**\n\nYour primary metric should be:\n- **North Star adjacent:** Tied to what actually matters for business\n- **Measurable quickly:** Can detect change in reasonable timeframe\n- **Sensitive enough:** Will move if change works\n- **Stable enough:** Not too noisy day-to-day\n\n**Bad primary metrics:**\n- Page views (vanity, doesn't show value)\n- Time on site (could mean confused, not engaged)\n- Clicks on button (metric hacking, doesn't show outcome)\n\n**Good primary metrics:**\n- Conversion rate (shows real outcome)\n- Revenue per user (business impact)\n- Feature adoption rate (engagement)\n- Retention day 7 (long-term value)\n\n**STEP 3: Define guardrail metrics**\n\nThese are metrics that should NOT get worse:\n\n**Must not break:**\n- Page load time\n- Error rates\n- User satisfaction (NPS)\n- Core flows (login, checkout, etc.)\n\n**Format:**\nPrimary metric: [Conversion rate]\nGoal: Increase by [X%]\nGuardrail metrics:\n• Page load time: Must stay <2s\n• Error rate: Must stay <1%\n• Cart abandonment: Must not increase\n\n\n**STEP 4: Calculate sample size**\n\nYou need enough users to detect the change:\n\n**Required inputs:**\n- Baseline rate: [Current metric value]\n- Minimum detectable effect: [Smallest change you care about]\n- Statistical power: [Usually 80%]\n- Significance level: [Usually 95%]\n\n**Rule of thumb calculations:**\n\nFor **conversion rate** changes:\n- To detect 10% relative lift (5% → 5.5%): ~10,000 users per variant\n- To detect 20% relative lift (5% → 6%): ~2,500 users per variant\n- To detect 5% relative lift (5% → 5.25%): ~40,000 users per variant\n\nFor **engagement metrics** (sessions, clicks):\n- Generally need less volume\n- But more noise, so longer runtime\n\n**Calculator formula:**\nn = 2 × (Zα + Zβ)² × p × (1-p) / (MDE)²\nWhere:\n• Zα = 1.96 (for 95% confidence)\n• Zβ = 0.84 (for 80% power)\n• p = baseline rate\n• MDE = minimum detectable effect\n\n\n**Then determine runtime:**\nDays needed = Sample size needed / (Daily traffic × % allocated to test)\n\n\n**STEP 5: Design variant carefully**\n\n**Control (A):**\n- Current experience\n- No changes\n- Baseline measurement\n\n**Treatment (B):**\n- ONE clear change\n- Everything else identical\n- Change is measurable\n\n**Common mistakes:**\n- Testing multiple changes at once (can't tell what caused effect)\n- Making change too subtle (won't detect difference)\n- Making change too different (not realistic to ship)\n\n**STEP 6: Plan rollout & assignment**\n\n**User assignment:**\n- **By user ID:** Same user always sees same variant (better for logged-in products)\n- **By session:** Each session could be different (okay for anonymous products)\n- **By device:** Mobile vs desktop\n\n**Traffic allocation:**\n- **50/50:** Standard, maximum power\n- **90/10:** If risky change, minimize exposure\n- **33/33/33:** Testing 2 variants against control\n\n**Rollout plan:**\n- Start with 10% of traffic (canary)\n- Monitor for 24 hours\n- If stable, ramp to 50%\n- If still good, full test\n\n**STEP 7: Define decision criteria upfront**\n\n**Ship if:**\n- [ ] Primary metric improves by [X%] with 95% confidence\n- [ ] Guardrails stay stable\n- [ ] No major bugs reported\n- [ ] Qualitative feedback is positive\n\n**Don't ship if:**\n- [ ] Primary metric neutral or negative\n- [ ] Any guardrail breaks\n- [ ] Major bugs or user complaints\n- [ ] Confidence level <95%\n\n**Inconclusive if:**\n- [ ] Small positive movement but not statistically significant\n- [ ] Then: Run longer OR redesign test\n\n**STEP 8: Plan instrumentation**\n\n**Events to track:**\ntest_exposure:\nuser_id: [ID]\nvariant: [A or B]\ntimestamp: [When]\n[primary_metric_event]:\nuser_id: [ID]\nvariant: [A or B]\nvalue: [Metric value]\ntimestamp: [When]\n\n\n**Check before launch:**\n- [ ] Events firing correctly\n- [ ] Variant assignment is random\n- [ ] No overlap with other tests\n- [ ] Can filter by variant in analytics\n\n</test_framework>\n\n---\n\n## A/B TEST DESIGN DOC\n\n**Test Name:** [Descriptive name]\n**Owner:** [Your name]\n**Status:** [Draft / Ready to launch / Running / Complete]\n\n---\n\n### Hypothesis\n\n**If we** [specific change]  \n**Then** [primary metric] will [increase/decrease by X%]  \n**Because** [reasoning about user behavior]\n\n**Example:**\n\"If we add customer testimonials to the pricing page, then free-to-paid conversion will increase from 12% to 14% because social proof reduces purchase anxiety.\"\n\n---\n\n### The Change\n\n**Control (A) - Current experience:**\n[Description or screenshot]\n\n**Treatment (B) - New experience:**\n[Description or screenshot]\n\n**What's different:**\n- [Specific change 1]\n- [Specific change 2]\n\n**What stays the same:**\n[Everything else]\n\n---\n\n### Metrics\n\n**PRIMARY METRIC (Decision maker):**\n\n**Metric:** [Name]  \n**Current baseline:** [X%]  \n**Target:** [Y%]  \n**Minimum detectable effect:** [Z%]\n\n**Why this metric:**\n[Why this is the right thing to measure]\n\n---\n\n**GUARDRAIL METRICS (Must not break):**\n\n| Metric | Current | Must stay above/below |\n|--------|---------|----------------------|\n| [Metric 1] | [Value] | [Threshold] |\n| [Metric 2] | [Value] | [Threshold] |\n| [Metric 3] | [Value] | [Threshold] |\n\n**Why these guardrails:**\n[What you're protecting against]\n\n---\n\n**SECONDARY METRICS (Understand impact):**\n- [Metric 1]: [What we'll learn]\n- [Metric 2]: [What we'll learn]\n\n---\n\n### Sample Size & Timeline\n\n**Sample size needed per variant:** [X users]\n\n**Calculation:**\n- Baseline rate: [%]\n- Target lift: [%]\n- Statistical power: 80%\n- Significance level: 95%\n\n**Daily traffic:** [Y users/day]  \n**Traffic allocation:** [Z% to test]  \n**Users per day in test:** [Y × Z%]\n\n**Days needed:** [X users ÷ (Y × Z%)] = [W days]\n\n**Proposed timeline:**\n- Start: [Date]\n- End: [Date]\n- Duration: [W days]\n\n**If we need results faster:**\n[Options: increase traffic %, accept lower power, larger MDE]\n\n---\n\n### Traffic Allocation\n\n**Split:**\n- Control (A): [50%]\n- Treatment (B): [50%]\n\n**Assignment method:**\n- [ ] By user ID (consistent experience)\n- [ ] By session (can vary per session)\n- [ ] Other: [Specify]\n\n**Rollout plan:**\n- Day 1: Launch to [10%] of traffic\n- Day 2: If stable, ramp to [50%]\n- Day 3+: Full test at [50/50]\n\n**Monitoring triggers:**\n- Error rate >2% → pause test\n- Crash rate up >50% → kill test\n- Angry user feedback → investigate\n\n---\n\n### Decision Criteria\n\n**SHIP TO 100% IF:**\n- ✅ Primary metric improves by ≥[X%] with p<0.05\n- ✅ All guardrails stable (within [Y%] of baseline)\n- ✅ No major bugs or user complaints\n- ✅ Test ran for full [W days]\n\n**DON'T SHIP IF:**\n- ❌ Primary metric flat or negative\n- ❌ Any guardrail degrades >[Y%]\n- ❌ Significant bugs or complaints\n- ❌ Low confidence (p>0.05)\n\n**INCONCLUSIVE IF:**\n- ⚠️ Small positive movement but p>0.05\n- **Then:** Run [X more days] OR redesign test\n\n**No cherry-picking results:** Wait for full duration, don't stop early because it's winning.\n\n---\n\n### Instrumentation\n\n**Events to implement:**\n\n**Test exposure:**\nexperiment_viewed:\nuser_id: string\nexperiment_name: \"test_name\"\nvariant: \"A\" or \"B\"\ntimestamp: datetime\n\n\n**Primary metric event:**\n[event_name]:\nuser_id: string\nvariant: \"A\" or \"B\"\n[metric_value]: number\ntimestamp: datetime\n\n\n**Pre-launch checklist:**\n- [ ] Events fire on staging\n- [ ] Variant assignment is random (check logs)\n- [ ] Can filter analytics by variant\n- [ ] Guardrail metrics tracked\n- [ ] No conflicts with other running tests\n\n---\n\n### Risks & Mitigation\n\n**RISK 1: [Potential problem]**\n- **Likelihood:** [High/Med/Low]\n- **Impact:** [High/Med/Low]\n- **Mitigation:** [What we'll do]\n- **Kill switch:** [How to disable quickly]\n\n**Example:**\n\"Risk: New checkout flow confuses users, increases abandonment  \nLikelihood: Medium  \nImpact: High (revenue loss)  \nMitigation: Monitor abandonment hourly, qualitative feedback survey  \nKill switch: Feature flag to revert to control\"\n\n---\n\n**RISK 2: [Another risk]**\n[Same format]\n\n---\n\n### User Experience Considerations\n\n**Switching variants:**\n- What happens if user assigned variant A yesterday sees variant B today?\n- Plan: [Consistent assignment by user ID]\n\n**Partial exposure:**\n- What if user only sees variant on mobile, not desktop?\n- Plan: [Accept, assignment is per-device]\n\n**Existing users vs new users:**\n- Does this change affect both equally?\n- Plan: [Segment analysis by user age]\n\n---\n\n### Analysis Plan\n\n**Primary analysis:**\n- Compare [primary metric] between A and B\n- Statistical test: [Two-sample t-test / Chi-square]\n- Confidence level: 95%\n\n**Segmentation to check:**\n- [ ] New vs returning users\n- [ ] Mobile vs desktop\n- [ ] By geography\n- [ ] By user cohort\n\n**Questions to answer:**\n1. Did primary metric move?\n2. Did any guardrails break?\n3. Are there segment differences?\n4. Is qualitative feedback consistent with data?\n\n**Dashboard:** [Link to live results]\n\n---\n\n### Rollback Plan\n\n**If we need to kill test:**\n\n**Trigger conditions:**\n- [Specific condition 1]\n- [Specific condition 2]\n\n**How to rollback:**\n1. [Step 1: Turn off feature flag]\n2. [Step 2: Verify users see control]\n3. [Step 3: Monitor for recovery]\n\n**Communication:**\n- Internal: [Slack #team-channel]\n- Users: [If needed, what to say]\n\n**Owner on-call:** [Name, contact]\n\n---\n\n### Launch Checklist\n\n**BEFORE LAUNCH:**\n- [ ] Hypothesis documented\n- [ ] Metrics instrumented and tested\n- [ ] Sample size calculated\n- [ ] Decision criteria agreed\n- [ ] Design/eng review complete\n- [ ] QA tested both variants\n- [ ] Feature flag configured\n- [ ] Rollback plan documented\n- [ ] Stakeholders aligned\n\n**AT LAUNCH:**\n- [ ] Start at 10% traffic\n- [ ] Check events firing\n- [ ] Check variant assignment random\n- [ ] Monitor for first 2 hours\n- [ ] Ramp to full traffic if stable\n\n**DURING TEST:**\n- [ ] Daily metrics check\n- [ ] Weekly stakeholder update\n- [ ] User feedback review\n- [ ] Guardrail monitoring\n\n**AT CONCLUSION:**\n- [ ] Full statistical analysis\n- [ ] Segment breakdowns\n- [ ] Qualitative synthesis\n- [ ] Decision made and documented\n- [ ] Results shared with team\n\n---\n\n### Success Criteria Review\n\n**Expected outcome:**\n[What you think will happen]\n\n**Best case:**\n[If way better than expected]\n\n**Worst case:**\n[If it hurts metrics]\n\n**Most likely:**\n[Realistic expectation]\n\n**We'll consider this test successful if:**\n[Even if we don't ship, what will we learn?]\n\n</ab_test_design>",
    "technique": "Hypothesis formation, sample size calculation, metric selection",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to design experiment to test feature/change before full rollout"
  },
  {
    "name": "Post-Mortem Facilitation",
    "category": "Operations",
    "prompt": "<post_mortem>\n\n<incident_inputs>\nWHAT HAPPENED:\n[Brief description of the incident/failure]\n\nTIMELINE:\n- When it started: [Date/time]\n- When detected: [Date/time]\n- When resolved: [Date/time]\n- Duration: [Total time]\n\nIMPACT:\n- Users affected: [Number or %]\n- Revenue impact: [If applicable]\n- Customer complaints: [Volume]\n- SLA breach: [Yes/no]\n\nWHO'S INVOLVED:\n[Team members who should be in post-mortem meeting]\n\nINITIAL OBSERVATIONS:\n[Any early thoughts on what went wrong]\n</incident_inputs>\n\n<postmortem_framework>\n\nYou facilitate blameless post-mortems that drive learning. Follow this process:\n\nSTEP 1: Set the ground rulesPost-mortem is NOT about:\n- Finding who to blame\n- Punishing mistakes\n- Defending decisions\n\nPost-mortem IS about:\n- Understanding what happened\n- Preventing it from happening again\n- Improving systems and processes\n\nBlameless culture:\n- Assume everyone had good intentions\n- Assume everyone did best with info they had\n- Focus on system failures, not human failures\n- If \"human error\" is root cause, you haven't dug deep enough\n\nSTEP 2: Build detailed timeline\n\nFor every significant event, document:\n- Time: [Exact timestamp]\n- Event: [What happened]\n- Who: [Person or system]\n- How we knew: [Alert, user report, manual discovery]\n\nExample:14:23 - Deploy initiated by [Person] - normal process 14:25 - Error rates start climbing - caught by monitoring 14:27 - On-call paged - automated alert 14:30 - On-call starts investigation - manual 14:35 - Root cause identified - manual debugging 14:40 - Rollback initiated - manual action 14:45 - Service recovered - monitoring confirmsGoal: Recreate exactly what happened, minute by minute.\n\nSTEP 3: Perform 5 Whys analysis\n\nKeep asking \"why\" until you hit systemic issues:\n\nExample:\n\"Why did the service go down?\"\n→ Database ran out of connections\n\n\"Why did it run out of connections?\"\n→ Connection pool was too small\n\n\"Why was pool too small?\"\n→ We didn't load test this feature\n\n\"Why didn't we load test?\"\n→ No load testing in our deploy process\n\n\"Why no load testing process?\"\n→ We prioritized speed over reliability\n\nRoot cause: Missing load testing in deployment process\n\nSTEP 4: Identify contributing factors\n\nBeyond root cause, what else contributed?\n\nCategories to check:\n- Technical: Architecture, code, infrastructure\n- Process: Testing, deployment, monitoring\n- Communication: Handoffs, documentation, alerts\n- Tools: What failed or was missing\n- Organizational: Priorities, resources, knowledge gaps\n\nSTEP 5: Extract lessons learnedWhat went well:\n(Yes, even in failures, something worked)\n- Monitoring caught the issue quickly\n- Team responded fast\n- Rollback process worked\n- Communication was clear\n\nWhat went poorly:\n- Detection took too long\n- No clear owner initially\n- Rollback was manual and slow\n- External communication was delayed\n\nWhat we learned:\n[Specific insights that will change how we work]\n\nSTEP 6: Generate action itemsGood action items:\n- Specific (not \"improve monitoring\")\n- Owned (person's name attached)\n- Dated (deadline)\n- Verifiable (can check if done)\n\nExample:\n❌ \"We should test better\"\n✅ \"Add load testing to deploy pipeline - @Jane - by March 15\"\n\nPrioritize:\n- P0: Prevents this specific issue (quick wins)\n- P1: Prevents entire class of issues (systemic)\n- P2: Nice improvements (do eventually)\n\nSTEP 7: Plan follow-up\n\n- Schedule review date for action items\n- Document in central location\n- Share lessons with broader team\n- Update runbooks/processes\n\n</postmortem_framework>\n\n---\n\n## POST-MORTEM DOCUMENT\n\nIncident: [Brief descriptive name]Date: [When it happened]Severity: [P0/P1/P2/P3]Status: [Resolved]Author: [Name]\n\n---\n\n### Executive Summary\n\nWhat happened:\n[2-3 sentences describing the incident]\n\nImpact:\n- Duration: [X hours/minutes]\n- Users affected: [Y users or Z%]\n- Services impacted: [Which features/systems]\n- Business impact: [Revenue, SLA, reputation]\n\nRoot cause:\n[One sentence]\n\nFix:\n[What we did to resolve]\n\nPrevention:\n[What we're doing so this never happens again]\n\n---\n\n### Timeline\n\n| Time | Event | How Detected | Who |\n|------|-------|--------------|-----|\n| 14:23 | [What happened] | [Alert/Manual/User report] | [Person/System] |\n| 14:25 | [Next event] | [How] | [Who] |\n| 14:27 | [Next event] | [How] | [Who] |\n\nKey timestamps:\n- Started: [When issue began]\n- Detected: [When we knew]\n- Mean time to detect: [Started → Detected]\n- Mitigated: [When we applied fix]\n- Resolved: [When fully recovered]\n- Mean time to recover: [Detected → Resolved]\n\n---\n\n### What Happened (Detailed)\n\nInitial state:\n[What was normal before incident]\n\nTrigger event:\n[What caused the incident to start]\n\nCascade:\n[How the problem spread or got worse]\n\nDetection:\n[How we found out - alert, user report, manual discovery]\n\nResponse:\n[What team did to investigate and fix]\n\nResolution:\n[What ultimately fixed it]\n\nRecovery:\n[How system/users recovered]\n\n---\n\n### Root Cause Analysis\n\nImmediate cause:\n[The proximate thing that broke]\n\n5 Whys:Why did [incident] happen?\n→ [Answer 1]\n\nWhy did [answer 1] happen?\n→ [Answer 2]\n\nWhy did [answer 2] happen?\n→ [Answer 3]\n\nWhy did [answer 3] happen?\n→ [Answer 4]\n\nWhy did [answer 4] happen?\n→ [Root cause]\n\nRoot cause:\n[The systemic issue that, if fixed, prevents this entire class of problems]\n\n---\n\n### Contributing Factors\n\nTechnical factors:\n- [Architecture decision that made this possible]\n- [Missing safeguard]\n- [Technical debt]\n\nProcess factors:\n- [Missing testing]\n- [Unclear ownership]\n- [Documentation gap]\n\nCommunication factors:\n- [Handoff issue]\n- [Alert that didn't fire]\n- [Unclear escalation]\n\nOrganizational factors:\n- [Resource constraint]\n- [Priority tradeoff]\n- [Knowledge gap]\n\n---\n\n### Impact Analysis\n\nUser impact:\n- Total users affected: [X]\n- % of user base: [Y%]\n- What they experienced: [Specific symptoms]\n- User complaints: [Volume and themes]\n\nBusiness impact:\n- Revenue lost: $[X] (if applicable)\n- SLA breach: [Yes/No, which customers]\n- Reputation damage: [Social, press mentions]\n- Support burden: [Tickets generated]\n\nTeam impact:\n- Engineering hours spent: [X hours]\n- Opportunity cost: [What didn't get done]\n- Stress/morale: [Effect on team]\n\n---\n\n### What Went Well\n\nDetection:\n- [Monitoring caught the issue]\n- [Alert fired correctly]\n\nResponse:\n- [Team assembled quickly]\n- [Clear incident commander]\n- [Effective debugging]\n\nResolution:\n- [Rollback process worked]\n- [Fix was simple]\n\nCommunication:\n- [Status page updated promptly]\n- [Clear internal updates]\n- [Customer communication was good]\n\nWorth celebrating:\n[Specific call-outs for people who did great work]\n\n---\n\n### What Went Poorly\n\nDetection:\n- [ ] Took too long to detect ([X minutes])\n- [ ] Monitoring didn't catch it\n- [ ] User reported before we knew\n\nResponse:\n- [ ] Unclear who owned problem\n- [ ] Slow to escalate\n- [ ] Communication gaps\n\nResolution:\n- [ ] Manual process (should be automated)\n- [ ] Rollback was complicated\n- [ ] Fix took too long\n\nCommunication:\n- [ ] Status page delayed\n- [ ] Internal confusion\n- [ ] Customer comms unclear\n\n---\n\n### Lessons Learned\n\nWhat this incident taught us:Lesson 1: [Specific insight]\n[Description and why it matters]\n\nLesson 2: [Another insight]\n[Description]\n\nLesson 3: [Another]\n[Description]\n\nHow this changes our thinking:\n[Broader implications for how we build/operate]\n\n---\n\n### Action Items\n\nP0 - CRITICAL (Prevent this specific issue):1. [Specific action]\n- Why: [Prevents exact problem]\n- Owner: [@Name]\n- Due: [Date]\n- Done: [ ]\n\n2. [Another action]\n- Why: [Impact]\n- Owner: [@Name]\n- Due: [Date]\n- Done: [ ]\n\n---\n\nP1 - HIGH (Prevent entire class of issues):1. [Systemic improvement]\n- Why: [Broader prevention]\n- Owner: [@Name]\n- Due: [Date]\n- Done: [ ]\n\n---\n\nP2 - MEDIUM (General improvements):1. [Nice-to-have improvement]\n- Owner: [@Name]\n- Due: [Date]\n- Done: [ ]\n\n---\n\n### Monitoring & Alerting Gaps\n\nWhat monitoring missed:\n- [Gap 1]\n- [Gap 2]\n\nNew alerts to add:\n- [Alert 1]: Trigger when [condition]\n- [Alert 2]: Trigger when [condition]\n\nImprovements to existing alerts:\n- [Alert name]: Make more sensitive/specific\n\n---\n\n### Documentation Updates Needed\n\nRunbooks to create/update:\n- [ ] [Runbook name]: Add steps for [scenario]\n- [ ] [Runbook name]: Update with [new info]\n\nArchitecture docs:\n- [ ] [Doc]: Update to reflect [reality]\n\nOn-call guides:\n- [ ] [Guide]: Add troubleshooting for [issue]\n\n---\n\n### Communication\n\nInternal:\n- Post-mortem shared with: [Teams]\n- Date shared: [Date]\n- Key takeaways communicated: [Where]\n\nExternal (if applicable):\n- Customer communication: [What we sent]\n- Public post-mortem: [Yes/No, link]\n- Press/social response: [If relevant]\n\n---\n\n### Follow-Up Plan\n\nPost-mortem review meeting:\n- Date: [30 days from now]\n- Attendees: [Who]\n- Agenda: Review progress on action items\n\nAction item tracking:\n- Where tracked: [Jira/Linear/Notion]\n- Review cadence: [Weekly]\n- Owner of tracking: [@Name]\n\nMetrics to monitor:\n- [Metric 1]: Ensure this stays stable\n- [Metric 2]: Ensure this improves\n\n---\n\n### Appendix\n\nRelevant links:\n- Incident channel: [Slack link]\n- Monitoring dashboard: [Link]\n- Related PRs/commits: [Links]\n- Customer tickets: [Links]\n\nSimilar past incidents:\n- [Date]: [Brief description and how this is similar]\n- Learning: [What we should have applied but didn't]\n\n</post_mortem>",
    "technique": "Blameless analysis, root cause extraction, action item generation",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Something went wrong (outage, bad launch, missed deadline) - need to learn from it"
  },
  {
    "name": "Pre-Mortem Facilitation",
    "category": "Operations",
    "prompt": "\n\n<pre_mortem>\n\n<premortem_inputs>\nWHAT YOU'RE LAUNCHING:\n[Feature, product, initiative description]\n\nLAUNCH DETAILS:\n- Launch date: [When]\n- Scope: [Who gets it, how big]\n- Why this is risky: [Known concerns]\n- Stakes: [What happens if it fails]\n\nTEAM CONTEXT:\n- Who should participate: [Cross-functional team]\n- What you're most worried about: [Top concerns]\n- Past failures: [Similar launches that went wrong]\n</premortem_inputs>\n\n<premortem_framework>\n\nYou facilitate pre-mortems that surface risks before launch. Follow this process:\n\n**STEP 1: Frame the exercise**\n\n**The setup:**\n\"It's [3 months from now]. We launched [project] and it was a complete disaster. Our worst fears came true. We're here to figure out what went wrong.\"\n\n**Why this works:**\n- Removes pressure to be positive\n- Permission to voice concerns\n- Surfaces hidden worries\n- Uncovers blind spots\n\n**Ground rules:**\n- No idea is too pessimistic\n- Focus on what COULD go wrong, not likelihood\n- Everyone participates\n- Blame-free environment\n\n**STEP 2: Generate failure scenarios**\n\nHave team brainstorm all the ways this could fail:\n\n**Categories to explore:**\n\n**Technical failures:**\n- What could break?\n- Performance issues?\n- Integration problems?\n- Data/security issues?\n\n**Product failures:**\n- Users don't understand it?\n- Solves wrong problem?\n- Too complex?\n- Missing key features?\n\n**Go-to-market failures:**\n- Poor positioning?\n- Wrong target audience?\n- Bad timing?\n- Competitive response?\n\n**Operational failures:**\n- Team not ready?\n- Support overwhelmed?\n- Sales can't sell it?\n- Poor documentation?\n\n**Business failures:**\n- Wrong pricing?\n- No adoption?\n- Cannibalize existing revenue?\n- Miss projections?\n\n**STEP 3: Assess each risk**\n\nFor every failure scenario:\n\n**Likelihood:** [High/Med/Low]\nHow likely is this to actually happen?\n\n**Impact:** [High/Med/Low]\nIf it happens, how bad is it?\n\n**Priority matrix:**\nHigh Impact + High Likelihood = CRITICAL (must mitigate)\nHigh Impact + Low Likelihood = MONITOR (have plan)\nLow Impact + High Likelihood = ACCEPT (annoying but okay)\nLow Impact + Low Likelihood = IGNORE (not worth time)\n\n\n**STEP 4: Generate mitigations**\n\nFor critical and high-priority risks:\n\n**Mitigation types:**\n- **Prevent:** Stop it from happening\n- **Detect:** Know quickly if it happens  \n- **Respond:** Have plan when it happens\n- **Accept:** Consciously choose to live with risk\n\n**Good mitigation:**\n- Specific action\n- Owner assigned\n- Done before launch\n- Verifiable\n\n**Example:**\nRisk: \"Users don't understand new feature\"\n❌ Bad mitigation: \"Make it clearer\"\n✅ Good mitigation: \"Run usability test with 5 users, iterate based on feedback - @Jane by Dec 1\"\n\n**STEP 5: Identify early warning signals**\n\nWhat would tell you things are going wrong?\n\n**Leading indicators (see problems early):**\n- Low engagement in beta\n- User confusion in tests\n- Sales team not demoing it\n- Poor qualitative feedback\n\n**Lagging indicators (problems already happened):**\n- Adoption below target\n- Churn spike\n- Support tickets flood\n- Negative reviews\n\n**STEP 6: Build decision framework**\n\n**Define kill criteria:**\nIf [X] happens, we will [action]\n\n**Example:**\n- If adoption <5% after week 1 → investigate deeply\n- If NPS drops >10 points → consider pausing rollout\n- If error rate >5% → immediate rollback\n\n**STEP 7: Create contingency plans**\n\nFor top 3 risks, have backup plans:\n- Plan B if primary approach fails\n- Rollback procedure\n- Communication templates\n- Resource allocation\n\n</premortem_framework>\n\n---\n\n## PRE-MORTEM DOCUMENT\n\n**Project:** [Name]  \n**Launch Date:** [Date]  \n**Owner:** [Name]  \n**Pre-Mortem Date:** [When exercise was done]\n\n---\n\n### The Scenario\n\n**\"It's [date 3 months from launch]. We launched [project] and it failed spectacularly.\"**\n\n**What we hoped would happen:**\n[Original success vision]\n\n**What actually happened:**\n[The disaster scenario we're imagining]\n\n**Stakes:**\n[Why this failure would be bad - revenue, reputation, team morale, etc.]\n\n---\n\n### Failure Scenarios Identified\n\n**Team brainstormed [X] ways this could fail:**\n\n---\n\n**TECHNICAL FAILURES:**\n\n**Scenario 1: [Specific failure]**\n- **What happens:** [Description]\n- **Likelihood:** [High/Med/Low]\n- **Impact:** [High/Med/Low]\n- **Priority:** [Critical/High/Medium/Low]\n\n**Scenario 2: [Another failure]**\n[Same format]\n\n---\n\n**PRODUCT FAILURES:**\n\n**Scenario 1: Users don't understand the feature**\n- **What happens:** [They try it, get confused, abandon it]\n- **Likelihood:** Medium\n- **Impact:** High\n- **Priority:** CRITICAL\n\n[Continue for all scenarios]\n\n---\n\n**GO-TO-MARKET FAILURES:**\n\n[List scenarios]\n\n---\n\n**OPERATIONAL FAILURES:**\n\n[List scenarios]\n\n---\n\n**BUSINESS FAILURES:**\n\n[List scenarios]\n\n---\n\n### Risk Matrix\n\n| Risk | Likelihood | Impact | Priority | Owner |\n|------|-----------|---------|----------|-------|\n| [Risk 1] | High | High | CRITICAL | [@Name] |\n| [Risk 2] | Med | High | HIGH | [@Name] |\n| [Risk 3] | High | Low | MEDIUM | [@Name] |\n\n**Focus on:** [Number] critical risks, [Number] high risks\n\n---\n\n### Critical Risks & Mitigations\n\n**RISK 1: [Specific risk]**\n\n**Full scenario:**\n[Detailed description of what goes wrong]\n\n**Likelihood:** High  \n**Impact:** High  \n**Priority:** CRITICAL\n\n**Why this could happen:**\n- [Root cause 1]\n- [Root cause 2]\n\n**Mitigation plan:**\n\n**PREVENT:**\n- [ ] [Action to prevent] - @Owner - Due: [Date]\n- [ ] [Another action] - @Owner - Due: [Date]\n\n**DETECT:**\n- [ ] [Monitoring/alert to add] - @Owner - Due: [Date]\n- [ ] [Metric to track] - @Owner - Due: [Date]\n\n**RESPOND:**\n- [ ] [Plan if this happens] - @Owner - Documented: [Link]\n\n**Contingency plan:**\nIf this happens, we will:\n1. [Step 1]\n2. [Step 2]\n3. [Decision point and criteria]\n\n**Success criteria for mitigation:**\n[How we'll know we've reduced this risk]\n\n---\n\n**RISK 2: [Another critical risk]**\n\n[Same detailed format]\n\n---\n\n[Repeat for all critical risks]\n\n---\n\n### High Priority Risks\n\n**RISK: [Description]**\n- **Mitigation:** [What we're doing]\n- **Owner:** [@Name]\n- **Due:** [Date]\n\n[Repeat for all high-priority risks]\n\n---\n\n### Medium/Low Priority Risks\n\n**Risks we're accepting:**\n- [Risk 1]: [Why we're accepting it]\n- [Risk 2]: [Why we're accepting it]\n\n**Monitoring plan:**\n[How we'll watch for these without active mitigation]\n\n---\n\n### Early Warning Signals\n\n**How we'll know things are going wrong:**\n\n**WEEK 1 SIGNALS:**\n- [ ] [Metric] below [threshold] → Action: [What we do]\n- [ ] [Behavior] observed → Action: [What we do]\n- [ ] [Feedback theme] emerges → Action: [What we do]\n\n**WEEK 2-4 SIGNALS:**\n- [ ] [Leading indicator] → Action: [What we do]\n- [ ] [Another signal] → Action: [What we do]\n\n**MONTH 2-3 SIGNALS:**\n- [ ] [Lagging indicator] → Action: [What we do]\n\n**Dashboard:** [Link to monitoring dashboard]\n\n---\n\n### Decision Framework\n\n**KILL CRITERIA (We rollback/pause if):**\n- [ ] [Metric] drops below [X]\n- [ ] [User complaints] exceed [Y]\n- [ ] [Error rate] above [Z%]\n- [ ] [Business metric] misses by >[W%]\n\n**INVESTIGATE CRITERIA (We dig deeper if):**\n- [ ] [Signal 1]\n- [ ] [Signal 2]\n\n**ITERATE CRITERIA (We adjust quickly if):**\n- [ ] [Feedback pattern]\n- [ ] [Usage pattern]\n\n**Decision maker:** [@Name]  \n**Escalation path:** [@Name → @Name → @Name]\n\n---\n\n### Contingency Plans\n\n**PLAN B: If primary approach fails**\n\n**Trigger:** [What indicates we need Plan B]\n\n**Alternative approach:**\n[What we do instead]\n\n**Resources needed:**\n[Time, people, budget]\n\n**Timeline:**\n[How quickly we can switch]\n\n---\n\n**ROLLBACK PLAN:**\n\n**Trigger:** [Conditions for rollback]\n\n**Rollback procedure:**\n1. [Technical step]\n2. [Communication step]\n3. [Validation step]\n\n**Time to rollback:** [X hours/minutes]\n\n**User impact:** [What users experience]\n\n**Communication templates:**\n- Internal: [Link]\n- External: [Link]\n\n---\n\n**COMMUNICATION CRISIS PLAN:**\n\n**If things go really wrong:**\n\n**Internal communication:**\n- Who needs to know: [Stakeholders]\n- Update frequency: [How often]\n- Channel: [Where]\n- Owner: [@Name]\n\n**External communication:**\n- Customer communication: [Template link]\n- Status page: [When to update]\n- Social/press: [If needed]\n- Owner: [@Name]\n\n---\n\n### Past Failures to Learn From\n\n**Similar launch that failed:**\n- **What:** [Description]\n- **When:** [Date]\n- **Why it failed:** [Root cause]\n- **How we're avoiding that:** [Specific actions]\n\n**Another past failure:**\n[Same format]\n\n**Patterns we noticed:**\n[Common themes from past failures]\n\n---\n\n### What Could Go Right\n\n**Best case scenarios:**\n(Yes, also imagine success)\n\n**If things go better than expected:**\n- [Positive scenario 1]\n- [Positive scenario 2]\n\n**How we'd capitalize:**\n[Plan to scale success]\n\n---\n\n### Pre-Launch Checklist\n\n**BEFORE WE LAUNCH, CONFIRM:**\n\n**Mitigations complete:**\n- [ ] All critical risk mitigations done\n- [ ] High priority mitigations done\n- [ ] Contingency plans documented\n\n**Monitoring ready:**\n- [ ] Early warning signals tracked\n- [ ] Dashboards built\n- [ ] Alerts configured\n- [ ] On-call scheduled\n\n**Team ready:**\n- [ ] Support trained\n- [ ] Sales enabled  \n- [ ] Eng on standby\n- [ ] Communication templates ready\n\n**Decision framework agreed:**\n- [ ] Kill criteria documented\n- [ ] Decision makers identified\n- [ ] Escalation path clear\n\n---\n\n### Launch Day Plan\n\n**Day 1 monitoring:**\n- Hour 1-2: [Who's watching what]\n- Hour 3-6: [Check-in plan]\n- End of day: [Review and decision]\n\n**Week 1 monitoring:**\n- Daily check: [Metrics to review]\n- Daily standup: [Time and who]\n- Weekly review: [Friday assessment]\n\n**First month:**\n- Weekly deep-dive\n- Monthly retrospective\n- Continuous iteration\n\n---\n\n### Action Items Before Launch\n\n**CRITICAL (Must do before launch):**\n- [ ] [Action] - @Owner - Due: [Date]\n- [ ] [Action] - @Owner - Due: [Date]\n\n**HIGH (Should do before launch):**\n- [ ] [Action] - @Owner - Due: [Date]\n\n**MEDIUM (Nice to have):**\n- [ ] [Action] - @Owner - Due: [Date]\n\n---\n\n### Go/No-Go Decision\n\n**Final review date:** [Date, 1 week before launch]\n\n**Go criteria:**\n- [ ] All critical mitigations complete\n- [ ] Team confident in plan\n- [ ] Monitoring ready\n- [ ] Rollback tested\n\n**No-go criteria:**\n- [ ] Multiple critical risks unmitigated\n- [ ] Key team members concerned\n- [ ] Monitoring not ready\n- [ ] Can't rollback quickly\n\n**Decision maker:** [@Name]\n\n---\n\n### Participants\n\n**Who was in pre-mortem:**\n- @Name - Role\n- @Name - Role\n- @Name - Role\n\n**Key concerns surfaced by:**\n- [Person]: [Their main worry]\n- [Person]: [Their concern]\n\n---\n\n### Follow-Up\n\n**Post-launch review:**\n- **Date:** [30 days after launch]\n- **Questions:** Did our risks materialize? Did mitigations work?\n\n**Update this document:**\n- As we learn: [Add new risks]\n- As we mitigate: [Check off actions]\n- As reality unfolds: [Compare to predictions]\n\n</pre_mortem>",
    "technique": "Failure scenario generation, risk prioritization, mitigation planning",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "About to launch something risky - identify what could go wrong before it does"
  },
  {
    "name": "All-Hands Presentation",
    "category": "PM Artifacts",
    "prompt": "<all_hands_presentation>\n\n<presentation_inputs>\nWHAT YOU'RE PRESENTING:\n- [ ] Quarterly update\n- [ ] Big launch announcement\n- [ ] Strategic direction change\n- [ ] Wins/milestones\n- [ ] Roadmap preview\n\nYOUR CONTENT:\n- Key metrics: [What you want to share]\n- Wins: [What shipped, what worked]\n- Challenges: [What you're tackling]\n- What's next: [Preview of roadmap]\n\nAUDIENCE:\n- Company size: [Number of people]\n- Technical level: [Mix of eng/non-eng]\n- What they care about: [Company goals, their work, impact]\n\nTIME LIMIT:\n[5 min / 10 min / 15 min]\n</presentation_inputs>\n\n<presentation_framework>\n\nYou create all-hands presentations that inform and inspire. Follow this structure:\n\nSTEP 1: Start with the headline\n\nFirst slide = the most important thing\n\nBad opening:\n\"Hi everyone, today I'm going to talk about Q3 product updates...\"\n\nGood opening:\n\"We hit 1M users this quarter. Here's how we did it and what's next.\"\n\nHook formats:\n- Big number: \"We shipped 47 features this quarter\"\n- Customer win: \"Fortune 500 company signed because of [feature]\"\n- Milestone: \"We're now the #1 product in our category\"\n- Challenge: \"We lost 15% of users to competitor. Here's our response.\"\n\nSTEP 2: Build narrative arcStructure:\n1. Where we were (context)\n2. What we did (progress)\n3. What we learned (insights)\n4. Where we're going (future)\n\nTime allocation for 10-min presentation:\n- 1 min: Hook + context\n- 3 min: What happened (metrics, wins)\n- 2 min: What we learned\n- 3 min: What's next\n- 1 min: Q&A setup\n\nSTEP 3: Show, don't just tellUse visuals:\n- Charts for metrics (line graphs for trends)\n- Screenshots for features\n- Customer quotes for impact\n- Simple diagrams for strategy\n\nDon't:\n- Walls of text\n- Complex diagrams\n- 10 bullet points per slide\n- Tiny font\n\nDo:\n- One idea per slide\n- Big numbers you can read from back\n- Simple visuals\n- High-contrast colors\n\nSTEP 4: Connect to company goals\n\nEvery update should answer: \"So what?\"\n\nFormula:\n\"We [did thing] which moved [metric] which helps company [achieve goal]\"\n\nExample:\n\"We shipped mobile app which increased DAU 30% which gets us closer to our 10M user goal\"\n\nSTEP 5: Be honest about challenges\n\nDon't just show wins. Show reality.\n\nWhat to share:\n- Metrics that didn't move\n- Features that flopped\n- Competitive threats\n- What you're doing about it\n\nThis builds credibility:\n- Shows you're self-aware\n- Invites help from company\n- Realistic about challenges\n\nSTEP 6: Make it memorablePeople remember:\n- Stories about customers\n- Surprising data points\n- Clear takeaways\n- Emotional moments\n\nPeople forget:\n- Generic metrics\n- Feature lists\n- Vague strategies\n\nUse the \"rule of 3\":\n- 3 main points\n- 3 wins\n- 3 priorities\n\nSTEP 7: End with clear call-to-actionWhat do you want people to do?\n- Try the new feature\n- Give feedback\n- Help with [specific thing]\n- Celebrate the win\n\nBad ending:\n\"That's it. Any questions?\"\n\nGood ending:\n\"Try the new mobile app this week. We need your feedback. Slack me what you think.\"\n\n</presentation_framework>\n\n---\n\n## ALL-HANDS PRESENTATION SCRIPT\n\nTitle: [Catchy title]Date: [When presenting]Time: [Duration]Audience: [Company all-hands]\n\n---\n\n### SLIDE 1: The Hook\n\nVisual: [Big number or striking image]\n\nWhat you say:\n\"[Opening line that grabs attention]\"\n\nExample:\nVisual: \"1,000,000\" in huge font\nSay: \"We hit one million users this quarter. Three years ago, we had 100. Here's how we got here and where we're going.\"\n\nGoal: Make them pay attention for next 10 minutes.\n\n---\n\n### SLIDE 2: Context (Where We Were)\n\nVisual: [Timeline or before/after]\n\nWhat you say:\n\"Quick context. [Time period] ago, we were [state]. Our goal was [objective].\"\n\nExample:\n\"Six months ago, we were losing users to competitors. Our goal: Win back market share by building what customers actually want.\"\n\nKeep this short: 30-60 seconds max.\n\n---\n\n### SLIDE 3: Progress Overview\n\nVisual: [Dashboard-style with key metrics]\n\nMetrics to show:\n- [Primary metric]: [X → Y] ([Z% change])\n- [Secondary metric]: [A → B]\n- [Impact metric]: [Result]\n\nWhat you say:\n\"Here's what happened. [Metric 1] went from X to Y. [Metric 2] improved by Z%. This means [business impact].\"\n\nExample:\nVisual: Chart showing user growth\nSay: \"Active users went from 700K to 1M - 43% growth. Revenue per user increased 20%. We're now profitable on a unit basis.\"\n\n---\n\n### SLIDE 4-6: The Wins (What We Did)\n\nSLIDE 4: Win #1Visual: [Screenshot or customer quote]\n\nWhat you say:\n\"First big win: [Feature/initiative]. We shipped this because [customer problem].\"\n\nImpact:\n- [Metric] improved by [X%]\n- [Customer quote if you have it]\n- [Business result]\n\nExample:\nVisual: Screenshot of mobile app\nSay: \"We launched mobile app. 40% of new users now come from mobile. Customer said: 'Finally, I can use this on the go.' This opened entirely new use case.\"\n\n---\n\nSLIDE 5: Win #2\n\n[Same format]\n\n---\n\nSLIDE 6: Win #3\n\n[Same format]\n\n---\n\n### SLIDE 7: What We Learned\n\nVisual: [Simple icon or image]\n\nWhat you say:\n\"Here's what surprised us.\"\n\nLessons (pick 2-3):\n- Lesson 1: [Unexpected finding]\n- \"We thought [assumption], but actually [reality]\"\n- Lesson 2: [Customer insight]\n- \"Customers told us [feedback]\"\n- Lesson 3: [Strategic insight]\n- \"This taught us [bigger principle]\"\n\nExample:\n\"Three surprises:\n1. Enterprise customers care more about mobile than we thought\n2. Users wanted simple, not powerful - we over-built\n3. Our biggest competitor isn't who we expected\"\n\n---\n\n### SLIDE 8: Challenges (What Didn't Work)\n\nVisual: [Chart showing the problem]\n\nWhat you say:\n\"Not everything worked. Let's be honest about what didn't.\"\n\nChallenge format:\n- What went wrong: [Specific thing]\n- Why: [Root cause]\n- What we're doing: [Fix]\n\nExample:\nVisual: Chart showing flat conversion rate\nSay: \"Conversion rate didn't move despite 10 features shipped. Why? We built what we wanted, not what users needed. Fix: We're doing 50 customer interviews this month to actually understand the problem.\"\n\nBe honest, not defensive.\n\n---\n\n### SLIDE 9: Customer Impact\n\nVisual: [Customer logo or quote]\n\nWhat you say:\n\"Here's what this means for customers.\"\n\nShow real impact:\n- Customer story\n- Quote from user\n- Metric that shows value\n\nExample:\nVisual: Quote \"This saved our team 10 hours per week\" - Sarah, PM at TechCo\nSay: \"We're not just building features. We're actually saving customers time. Sarah's team cut meeting prep from 10 hours to 2 hours per week using our new templates.\"\n\nMake it human, not just numbers.\n\n---\n\n### SLIDE 10: What's Next (Roadmap Preview)\n\nVisual: [Timeline or 3 boxes]\n\nWhat you say:\n\"Here's what's coming.\"\n\nShow 3 things:\n1. This month: [Immediate priority]\n2. This quarter: [Bigger initiative]\n3. This year: [Vision item]\n\nExample:\n\"Next 90 days:\n1. This month: Ship AI-powered search\n2. This quarter: Launch enterprise tier\n3. This year: Become the platform our customers build on\"\n\nDon't show detailed roadmap. Show direction.\n\n---\n\n### SLIDE 11: How You Can Help\n\nVisual: [Call-to-action]\n\nWhat you say:\n\"Here's how you can help.\"\n\nSpecific asks:\n- [ ] \"Try [feature] and give feedback in #product-feedback\"\n- [ ] \"Refer enterprise customers to sales team\"\n- [ ] \"If you know anyone at [target companies], introduce us\"\n\nExample:\n\"Three ways to help:\n1. Use the mobile app this week - we need your feedback\n2. Sales team: New mobile pitch deck is ready\n3. Know any PM hiring managers? We're hiring 2 PMs\"\n\nMake asks concrete and easy.\n\n---\n\n### SLIDE 12: Thank You\n\nVisual: [Team photo or celebratory image]\n\nWhat you say:\n\"This wouldn't be possible without [teams who helped]. Special shoutout to [names/teams].\"\n\nBe specific about appreciation:\n- Eng team shipped X features\n- Design nailed the mobile experience\n- Sales team provided crucial customer feedback\n- Support handled 2x volume without breaking\n\nEnd with energy:\n\"Excited for what's next. Questions?\"\n\n---\n\n## PRESENTATION NOTES\n\n### Tone & Delivery\n\nEnergy level: [High/Medium - match what's appropriate]\n\nPacing:\n- Slow down for important points\n- Speed up for background/context\n- Pause after big reveals\n\nEye contact:\n- Don't read slides\n- Scan the room\n- Look at Zoom if remote\n\nEnthusiasm:\n- Be genuinely excited about wins\n- Be candid about challenges\n- Be inspiring about future\n\n---\n\n### Anticipated Questions\n\nQ: \"Why didn't [feature] ship?\"\nA: [Prepared answer]\n\nQ: \"How does this compare to competitor?\"\nA: [Prepared answer]\n\nQ: \"What's the biggest risk?\"\nA: [Honest answer]\n\nQ: \"When will [requested feature] ship?\"\nA: [Honest timeline or \"not prioritized yet, here's why\"]\n\n---\n\n### Backup Slides (If Needed)\n\nDetailed metrics: [Link to full dashboard]\n\nFull roadmap: [More detailed view]\n\nCompetitive analysis: [If questions come up]\n\nCustomer case studies: [More examples]\n\n---\n\n### Follow-Up\n\nAfter presentation:\n- [ ] Share slides in #general\n- [ ] Post recording for those who missed\n- [ ] Send detailed metrics to leadership\n- [ ] Document questions asked + answers\n- [ ] Follow up on commitments made\n\nFeedback collection:\n- [ ] \"How'd this presentation land?\" in Slack\n- [ ] Note what resonated\n- [ ] Note what confused people\n- [ ] Improve for next time\n\n</all_hands_presentation>",
    "technique": "Story arc construction, data visualization, executive messaging",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to update whole company on product progress/wins/direction"
  },
  {
    "name": "Board Deck Prep",
    "category": "PM Artifacts",
    "prompt": "<board_deck>\n\n<deck_inputs>\nMEETING CONTEXT:\n- Board meeting date: [When]\n- Time allocated: [5 min / 10 min / 15 min]\n- Last board meeting: [When, what you said]\n- Board composition: [Who's on board, their backgrounds]\n\nYOUR CONTENT:\n- Metrics: [Key numbers to share]\n- Progress: [What shipped since last meeting]\n- Strategy: [Any direction changes]\n- Asks: [What you need from board]\n\nCOMPANY STAGE:\n- [ ] Seed/Series A (product-market fit)\n- [ ] Series B/C (scaling)\n- [ ] Late stage (optimization)\n</deck_inputs>\n\n<deck_framework>\n\nYou create board decks that inform and build confidence. Follow this approach:\n\nSTEP 1: Understand what board cares aboutBoard members want to know:\n- Are we on track? (vs plan)\n- What changed? (good and bad)\n- What are the risks? (what keeps PM up at night)\n- What do you need? (resources, decisions)\n\nBoard members DON'T care about:\n- Feature details\n- Day-to-day operations\n- Tactical execution\n- Team drama\n\nKeep it strategic.STEP 2: Structure for boardIdeal structure:\n1. TL;DR (Executive summary - 1 slide)\n2. Metrics (What's happening - 2-3 slides)\n3. Progress (What we did - 2 slides)\n4. Strategy (Where we're going - 2 slides)\n5. Risks (What could go wrong - 1 slide)\n6. Asks (What we need - 1 slide)\n\nTotal: 8-10 slides for 15-min sectionSTEP 3: Lead with the bottom lineFirst slide = executive summary\n\nAnswer these questions upfront:\n- Are we on track? (Yes/no/mostly)\n- What's working? (1 sentence)\n- What's not? (1 sentence)\n- What's next? (1 sentence)\n\nThen:\n\"Details follow, but happy to jump to any section.\"\n\nThis lets board direct conversation.STEP 4: Show trends, not snapshotsBad: \"We have 100K users\"\nGood: \"We grew from 75K to 100K users (33% QoQ), ahead of 90K target\"\n\nAlways show:\n- Current number\n- Previous number\n- Target/plan\n- Trend direction\n\nUse arrows: ↑ ↓ → to show movement\n\nSTEP 5: Be honest about problemsBoard appreciates honesty:\n- Don't hide bad news\n- Don't spin problems as \"opportunities\"\n- Show you understand the issue\n- Have a plan to address it\n\nFormat for bad news:\n- What's wrong: [Specific problem]\n- Why it matters: [Business impact]\n- What we're doing: [Action plan]\n- When we'll know if it worked: [Timeline/metric]\n\nSTEP 6: Frame risks properlyRisk categories:\n- Execution risks: Can we build/ship this?\n- Market risks: Will customers want it?\n- Competitive risks: What if competitor does X?\n- Resource risks: Do we have enough people/money?\n\nFor each risk:\n- Likelihood (high/med/low)\n- Impact if it happens\n- Mitigation plan\n\nDon't:\n- List 20 risks (shows lack of focus)\n- Downplay serious risks\n- Present risks without mitigation\n\nSTEP 7: Make clear asksBoard can help with:\n- Strategic decisions (enter new market?)\n- Resource allocation (need 10 more eng?)\n- Introductions (need enterprise customers?)\n- Competitive intelligence (what are they hearing?)\n\nBad ask: \"We need to hire more\"\nGood ask: \"We need to hire 5 senior eng for platform team to hit Q3 launch. Budget implications: [X]. Alternative is 3-month delay.\"\n\nBe specific about:\n- What you need\n- Why you need it\n- What happens if you don't get it\n- What you're asking them to do\n\nSTEP 8: Prepare for questionsCommon board questions:\n- \"Why is [metric] down?\"\n- \"How do we compare to competitor?\"\n- \"What's your biggest concern?\"\n- \"What would you do with 2x the budget?\"\n- \"What keeps you up at night?\"\n\nHave answers ready.\n\n</deck_framework>\n\n---\n\n## BOARD DECK OUTLINE\n\nMeeting: [Date]Product UpdatePresenter: [Your name]Time: [Minutes allocated]\n\n---\n\n### SLIDE 1: Executive Summary\n\nVisual: [Traffic light: Green/Yellow/Red status]\n\nOverall Status: [On track / Needs attention / Behind]\n\nKey Points:\n- Progress: [One sentence on what shipped]\n- Metrics: [One sentence on performance]\n- Challenge: [One sentence on biggest issue]\n- Ask: [One sentence on what you need]\n\nExample:Status: ✅ On Track  Progress: Shipped enterprise tier, acquired 3 Fortune 500 customers Metrics: ARR grew 45% QoQ to $12M, on pace for $50M target Challenge: Platform stability issues at scale (addressing with infra investment) Ask: Approve $2M budget increase for infrastructure teamThis slide should tell complete story.\n\n---\n\n### SLIDE 2: Metrics Dashboard\n\nVisual: [4-6 key metrics with trends]\n\nFormat for each metric:[Metric Name] [Big number] ↑ [% change] vs [time period] Target: [Goal] Status: [On track / Behind / Ahead]Metrics to include:\n- North star metric (usage/engagement)\n- Growth metric (users, revenue)\n- Quality metric (NPS, retention)\n- Efficiency metric (CAC, LTV)\n\nExample:Monthly Active Users 125K ↑ 35% vs Q1 Target: 120K ✅ Ahead Plan: Hit 200K by EOY  Net Revenue Retention 118% ↑ 5 pts vs Q1 Target: 115% ✅ Ahead Best in class for our stage\n\n---\n\n### SLIDE 3: Metrics Deep Dive (if needed)\n\nVisual: [Chart showing trend over time]\n\nWhat to show:\n- Line graph of key metric over quarters\n- Cohort analysis if relevant\n- Segment breakdown if important\n\nWhat to say:\n\"[Metric] has grown consistently for [time period]. Growth driven by [factor]. One concern: [potential issue].\"\n\nKeep it simple: One chart, one insight.\n\n---\n\n### SLIDE 4: Product Progress\n\nVisual: [Timeline or checklist]\n\nWhat shipped since last board meeting:Q[X] Deliverables:\n- ✅ [Major feature] - [Impact]\n- ✅ [Another feature] - [Impact]\n- ✅ [Initiative] - [Impact]\n- 🔄 [In progress item] - [Expected completion]\n\nKey Wins:\n- [Customer win]\n- [Metric improvement]\n- [Strategic milestone]\n\nExample:Q2 Deliverables: ✅ Enterprise SSO - Unblocked 5 enterprise deals ($3M ARR) ✅ Mobile app - 40% of new users now mobile-first ✅ API platform - 20 partners integrated 🔄 AI features - Shipping Q3  Key Win: Closed Acme Corp ($1M ARR) - largest deal ever\n\n---\n\n### SLIDE 5: Customer/Market Traction\n\nVisual: [Customer logos or market map]\n\nTraction indicators:\n- New customers (logos if impressive)\n- Customer quotes\n- Usage milestones\n- Market position\n\nFormat:Customer Traction: - 3 Fortune 500 customers acquired - 95% customer satisfaction (NPS: 65) - Quote: \"[Impressive customer quote]\" - [Name, Title, Company]  Market Position: - #2 in G2 category (up from #5) - Mentioned in Gartner report - 15% market share (up from 8%)\n\n---\n\n### SLIDE 6: Strategic Direction\n\nVisual: [Simple roadmap or 3 pillars]\n\nWhat you're focused on:Current Strategy:\n[One sentence describing focus]\n\nThree Priorities:\n1. [Priority 1]: [Why + Expected impact]\n2. [Priority 2]: [Why + Expected impact]\n3. [Priority 3]: [Why + Expected impact]\n\nExample:Strategy: Become the platform enterprise teams build on  Priorities: 1. Platform/API: Enable 100 integrations by EOY (currently 20) 2. Enterprise features: SSO, SCIM, advanced permissions (Q3 launch) 3. AI capabilities: Stay ahead of competition in AI features  Why now: Enterprise is 70% of new ARR, they need platform capabilities\n\n---\n\n### SLIDE 7: Competitive Landscape\n\nVisual: [2x2 matrix or table]\n\nWhere you stand:Our Position:\n[Leader / Strong / Gaining / Behind] in [market segment]\n\nKey Competitors:\n- [Competitor A]: [Their strength / Our advantage]\n- [Competitor B]: [Their strength / Our advantage]\n\nRecent Moves:\n- [Competitor] launched [feature] - Our response: [Action]\n- We launched [thing] - Differentiation: [Why we win]\n\nWin rate vs competitors: [X%] (trend: [↑/↓/→])\n\nExample:Position: Strong #2, gaining on leader  Competitor X (market leader): - Strength: Brand, enterprise presence - Our advantage: 3x faster, modern UX, better pricing  Competitor Y (rising threat): - Strength: AI features, venture-backed - Our advantage: Enterprise-ready, proven at scale  Win rate: 65% (↑ from 55% last quarter)\n\n---\n\n### SLIDE 8: Risks & Mitigations\n\nVisual: [Risk matrix or table]\n\nTop Risks:Risk 1: [Specific risk]\n- Likelihood: [High/Med/Low]\n- Impact: [High/Med/Low]\n- Mitigation: [What we're doing]\n- Owner: [Who's responsible]\n\nRisk 2: [Another risk]\n[Same format]\n\nExample:Risk 1: Platform instability at scale - Likelihood: High (seeing issues at 100K+ users) - Impact: High (customer churn, reputation) - Mitigation: $2M infrastructure investment, dedicated platform team - Timeline: Resolved by Q3  Risk 2: Competitor launches AI feature first - Likelihood: Medium (they're 3 months behind) - Impact: Medium (could slow growth) - Mitigation: Accelerating AI roadmap, shipping beta in 6 weeksBoard appreciates you thinking ahead.\n\n---\n\n### SLIDE 9: Resource Needs & Trade-offs\n\nVisual: [Budget or headcount chart]\n\nWhat you need:Current State:\n- Team size: [X people]\n- Budget: $[Y]/quarter\n- Velocity: [What you can ship]\n\nRequest:\n- Additional headcount: [+Z people] in [roles]\n- Additional budget: $[W] for [purpose]\n- Timeline: [When you need it]\n\nTrade-offs:\n- If approved: [What you can do]\n- If not: [What you'll cut/delay]\n\nROI:\n[Expected return on investment]\n\nExample:Current: 20 eng, shipping 8 features/quarter  Request: +10 eng ($500K/quarter) - 5 platform eng - 3 AI/ML eng - 2 mobile eng  If approved: Ship platform + AI features in Q3 (unlocks $5M ARR) If not: Delay platform to Q4, cut AI features (risk losing to competitors)  ROI: $5M ARR from $2M investment = 2.5x in year 1\n\n---\n\n### SLIDE 10: Questions & Discussion\n\nVisual: [Open for discussion]\n\nPotential board questions:Prepared answers for:\n- \"Why is [metric] trending that way?\"\n- \"How are you thinking about [competitor]?\"\n- \"What's your biggest concern?\"\n- \"What would you do with 2x the resources?\"\n- \"What are you NOT doing that you should be?\"\n\nDocuments available:\n- Detailed metrics: [Link]\n- Full roadmap: [Link]\n- Competitive analysis: [Link]\n\n---\n\n## APPENDIX SLIDES (If Needed)\n\n### Detailed Roadmap\n\nVisual: [Quarter-by-quarter view]\n\n[Show next 3-4 quarters of planned work]\n\n---\n\n### Customer Case Studies\n\nVisual: [Logos + metrics]\n\n[2-3 impressive customer stories with data]\n\n---\n\n### Team & Organization\n\nVisual: [Org chart or team breakdown]\n\n[If board asking about team structure]\n\n---\n\n### Financial Impact\n\nVisual: [Revenue/cost projections]\n\n[If board needs to connect product to financials]\n\n---\n\n## PRESENTATION NOTES\n\n### Delivery Tips\n\nTone:\n- Confident but not arrogant\n- Honest about challenges\n- Excited about future\n- Respectful of board's time\n\nPacing:\n- 1-2 minutes per slide\n- Slow down for important points\n- Be ready to skip slides if discussion emerges\n\nEngagement:\n- Pause for questions\n- Make eye contact\n- Read the room\n- Don't be defensive\n\n---\n\n### Follow-Up Actions\n\nAfter meeting:\n- [ ] Send detailed metrics to board members\n- [ ] Document decisions made\n- [ ] Follow up on commitments\n- [ ] Share feedback with team\n- [ ] Update internal stakeholders\n\nBefore next meeting:\n- [ ] Track progress on commitments\n- [ ] Prepare status on asks\n- [ ] Update on risks flagged\n\n</board_deck>",
    "technique": "Executive summary, trend analysis, risk articulation",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Prepare product update for board meeting - high-level strategic view"
  },
  {
    "name": "Build vs Buy Analysis",
    "category": "Strategy & Planning",
    "prompt": "<build_vs_buy>\n\n<decision_inputs>\nWHAT YOU NEED:\n[Specific capability or feature - e.g., \"payment processing\", \"analytics\", \"auth system\"]\n\nWHY YOU NEED IT:\n- Problem it solves: [Specific pain point]\n- Users affected: [Who needs this]\n- Timeline: [How urgent]\n\nOPTIONS YOU'RE CONSIDERING:\n- Build in-house\n- Buy/integrate: [Specific vendor(s) you're evaluating]\n- Hybrid: [If applicable]\n\nYOUR CONTEXT:\n- Team capacity: [Eng resources available]\n- Budget: [Available funding]\n- Technical debt tolerance: [Can you maintain another system?]\n- Strategic importance: [Core to product or peripheral?]\n</decision_inputs>\n\n<decision_framework>\n\nYou analyze build vs buy decisions systematically. Follow this process:\n\nSTEP 1: Define what you actually need\n\nBe specific about requirements:\n\nFunctional requirements:\n- [ ] Must-haves (deal-breakers)\n- [ ] Nice-to-haves (can live without)\n- [ ] Won't-need (out of scope)\n\nNon-functional requirements:\n- Performance (latency, throughput)\n- Scale (current and 2 years out)\n- Reliability (uptime needs)\n- Security (compliance requirements)\n- Integrations (what it must connect to)\n\nAvoid:\nOver-specifying (building for problems you don't have)\n\nSTEP 2: Calculate true cost of buildingInitial build cost:\n- Eng time to build: [X weeks × Y engineers]\n- Design time: [Z weeks]\n- Testing/QA time: [W weeks]\n- Total hours × hourly cost = $[Initial cost]\n\nOngoing maintenance cost (annual):\n- Bug fixes: [~20% of build time per year]\n- Updates/improvements: [Another 20%]\n- Scaling/operations: [Infrastructure + eng time]\n- Security patches: [Ongoing vigilance]\n- Documentation: [Keeping it current]\n- Annual maintenance = 40-60% of initial build costOpportunity cost:\n- What else could team build instead?\n- What revenue could those features generate?\n- What strategic initiatives get delayed?\n\nTechnical debt:\n- Complexity added to codebase\n- Knowledge burden (only your team understands it)\n- Future migration cost (if you eventually replace it)\n\nSTEP 3: Calculate true cost of buyingVendor costs:\n- Subscription: $[X/month or /user]\n- Setup/onboarding: $[One-time]\n- Training: [Time + cost]\n- Overage charges: [If you exceed limits]\n- 5-year total: $[X × 60 months]Integration costs:\n- Eng time to integrate: [Y weeks]\n- Ongoing integration maintenance: [Z hours/month]\n- API rate limits (does it constrain you?)\n\nCustomization costs:\n- Can you customize it?\n- Cost to customize: [If needed]\n- Limitations you'll hit: [Features you can't have]\n\nSwitching costs (if vendor goes away):\n- Data export difficulty\n- Vendor lock-in risk\n- Migration effort to alternative\n\nHidden costs:\n- Support tickets (your team still helps users)\n- Feature requests you can't fulfill\n- Waiting for vendor to ship what you need\n\nSTEP 3: Assess strategic fitIs this core to your product?Core (should probably build):\n- Unique differentiation\n- Competitive advantage\n- Deep integration with product\n- IP you want to own\n\nNon-core (should probably buy):\n- Commodity feature (everyone has it)\n- Not differentiated\n- Peripheral to main value prop\n- Complex domain you don't need to own\n\nExample:\n- Stripe for payments (buy) - not core unless you're fintech\n- Proprietary algorithm (build) - this IS your differentiation\n\nSTEP 4: Evaluate vendor options\n\nFor each vendor:\n\nProduct fit:\n- [ ] Meets must-have requirements\n- [ ] Has nice-to-haves\n- [ ] Missing deal-breakers\n\nCompany viability:\n- Funding/profitability\n- Customer count\n- Market position\n- Longevity risk\n\nIntegration quality:\n- API documentation\n- SDK availability\n- Integration examples\n- Support responsiveness\n\nCustomer experience:\n- Reviews (G2, Capterra)\n- Reference customers\n- Known issues\n\nSTEP 5: Run decision matrix\n\nScore each option on key criteria:\n\n| Criteria | Weight | Build | Vendor A | Vendor B |\n|----------|--------|-------|----------|----------|\n| Cost (5yr) | 0.3 | [Score 1-10] | [Score] | [Score] |\n| Time to market | 0.2 | [Score] | [Score] | [Score] |\n| Feature fit | 0.2 | [Score] | [Score] | [Score] |\n| Maintenance burden | 0.15 | [Score] | [Score] | [Score] |\n| Strategic control | 0.15 | [Score] | [Score] | [Score] |\n| Total | 1.0 | [Weighted] | [Weighted] | [Weighted] |\n\nHighest score = recommended approachSTEP 6: Consider hybrid approaches\n\nSometimes best answer is \"both\":\n\nBuild wrapper around vendor:\n- Use vendor for heavy lifting\n- Build thin layer for customization\n- Insulates from vendor changes\n\nBuild MVP, buy later:\n- Build quick version to validate\n- Replace with vendor when proven\n- Reduces risk of building wrong thing\n\nBuy now, build later:\n- Use vendor to move fast\n- Build in-house once you understand needs\n- Common path for growing companies\n\nSTEP 7: Make the recommendationDecision framework:Build if:\n- Core differentiation\n- Vendor options don't fit\n- Long-term cost of buying > building\n- Team has capacity\n- Low complexity\n\nBuy if:\n- Not core differentiation\n- Good vendor options exist\n- Faster time to market matters\n- Team at capacity\n- High complexity/risk to build\n\nBe honest about:\n- Bias toward building (engineers love to build)\n- Bias toward buying (easier short-term)\n- Pressure from stakeholders\n\n</decision_framework>\n\n---\n\n## BUILD VS BUY ANALYSIS\n\nCapability Needed: [What you're evaluating]Decision Owner: [Name]Decision Deadline: [When you need to decide]Current Status: [Under evaluation]\n\n---\n\n### Executive Summary\n\nRecommendation: [Build / Buy / Hybrid]\n\nRationale (one paragraph):\n[Why this is the best choice given context, cost, strategy, and timeline]\n\nKey factors:\n- Cost: [Build vs buy comparison]\n- Time: [How long each takes]\n- Strategy: [Core vs non-core]\n- Risk: [What could go wrong]\n\nIf approved, next steps:\n[What happens immediately]\n\n---\n\n### What We Need\n\nThe problem:\n[Specific pain point or gap]\n\nWho needs it:\n- User segment: [Which users]\n- Usage pattern: [How often, how critical]\n- Volume: [Scale requirements]\n\nWhy now:\n- Business driver: [What's pushing this]\n- Timeline: [Urgency]\n- Competition: [Are competitors doing this]\n\n---\n\n### Requirements\n\nMUST HAVE (Deal-breakers):\n- [ ] [Requirement 1]\n- [ ] [Requirement 2]\n- [ ] [Requirement 3]\n\nNICE TO HAVE:\n- [ ] [Feature 1]\n- [ ] [Feature 2]\n\nOUT OF SCOPE:\n- [Thing we don't need]\n- [Another thing we don't need]\n\nNon-functional requirements:\n- Performance: [Latency/throughput needs]\n- Scale: [Current + 2-year projection]\n- Uptime: [How critical]\n- Security: [Compliance/standards]\n- Integration: [What it must connect to]\n\n---\n\n### Option 1: Build In-House\n\nWhat we'd build:\n[High-level description of solution]\n\nApproach:\n[Technical approach, architecture]\n\nTimeline:\n- Design: [X weeks]\n- Build: [Y weeks]\n- Test: [Z weeks]\n- Total: [W weeks to production]Team required:\n- [N] engineers × [M] weeks\n- [P] designer × [Q] weeks\n- PM/QA time: [Estimate]\n\n---\n\nCOST ANALYSIS:Initial build:\n- Engineering: [Hours × rate] = $[X]\n- Design: [Hours × rate] = $[Y]\n- Infrastructure: $[Z/month]\n- Total initial: $[A]Ongoing (annual):\n- Maintenance: $[40-60% of build cost]\n- Improvements: $[Estimated]\n- Infrastructure: $[Cost × 12]\n- Support: $[Estimated]\n- Annual recurring: $[B]5-year total cost of ownership:\n$[A] + ($[B] × 5) = $[Total]\n\n---\n\nPROS:\n✅ Full control over features\n✅ Exact fit to our needs\n✅ No vendor dependency\n✅ IP ownership\n✅ [Other advantages]\n\nCONS:\n❌ Longer time to market ([W] weeks)\n❌ Opportunity cost (team could build [X])\n❌ Maintenance burden\n❌ Complexity added to codebase\n❌ [Other disadvantages]\n\n---\n\nRISKS:\n- Technical risk: [What could go wrong technically]\n- Mitigation: [How to reduce risk]\n- Timeline risk: [Could take longer than expected]\n- Mitigation: [Buffer, phasing]\n- Maintenance risk: [Ongoing burden]\n- Mitigation: [Team capacity plan]\n\n---\n\n### Option 2: Buy [Vendor Name]\n\nWhat they provide:\n[Description of vendor solution]\n\nHow it works:\n[Integration approach]\n\nTimeline:\n- Evaluation: [1-2 weeks]\n- Contract: [1 week]\n- Integration: [X weeks]\n- Testing: [Y weeks]\n- Total: [Z weeks to production]\n\n---\n\nCOST ANALYSIS:Vendor pricing:\n- Subscription: $[X/month] or $[Y/user/month]\n- Setup fee: $[One-time]\n- Support: [Included or $Z/month]\n\nIntegration costs:\n- Eng time: [N weeks × rate] = $[A]\n- Ongoing maintenance: $[B/month]\n\n5-year total cost of ownership:\n($[X] × 60) + $[A] + ($[B] × 60) = $[Total]\n\n---\n\nPROS:\n✅ Faster time to market ([Z] weeks vs [W] weeks)\n✅ Proven solution (used by [customers])\n✅ No maintenance burden\n✅ Regular updates included\n✅ [Other advantages]\n\nCONS:\n❌ Monthly cost ($[X])\n❌ Less control over features\n❌ Vendor dependency\n❌ May not fit perfectly\n❌ [Other disadvantages]\n\n---\n\nVENDOR EVALUATION:Product fit:\n- Must-haves met: [X of Y] ✅/❌\n- Nice-to-haves met: [A of B]\n- Missing features: [List]\n\nCompany viability:\n- Funding: [Stage, amount]\n- Customer base: [Size, notable customers]\n- Market position: [Leader/challenger/niche]\n- Risk assessment: [Low/medium/high]\n\nIntegration quality:\n- API quality: [Good/okay/poor]\n- Documentation: [Link, assessment]\n- Support: [Response time, quality]\n- Community: [Active/moderate/none]\n\nCustomer references:\n- [Company 1]: \"[Quote about experience]\"\n- [Company 2]: \"[Quote]\"\n- Common complaints: [From reviews]\n\n---\n\nRISKS:\n- Vendor risk: [Could go out of business, raise prices]\n- Mitigation: [Contract terms, exit plan]\n- Feature gap risk: [Missing features we need]\n- Mitigation: [Workarounds, roadmap]\n- Lock-in risk: [Hard to switch later]\n- Mitigation: [Data export, abstraction layer]\n\n---\n\n### Option 3: [Another Vendor or Hybrid]\n\n[Same analysis structure if evaluating multiple vendors or hybrid approach]\n\n---\n\n### Side-by-Side Comparison\n\n| Factor | Build | Vendor A | Vendor B |\n|--------|-------|----------|----------|\n| Time to market | [W weeks] | [Z weeks] | [Y weeks] |\n| Initial cost | $[X] | $[Y] | $[Z] |\n| 5-year TCO | $[A] | $[B] | $[C] |\n| Feature fit | 100% | 85% | 70% |\n| Control | Full | Limited | Limited |\n| Maintenance | High | Low | Low |\n| Strategic fit | [Core/Non-core] | [Score] | [Score] |\n\n---\n\n### Decision Matrix\n\nWeighted scoring (1-10 scale):\n\n| Criteria | Weight | Build | Vendor A | Vendor B |\n|----------|--------|-------|----------|----------|\n| 5-year cost | 30% | 6 (1.8) | 8 (2.4) | 7 (2.1) |\n| Time to market | 20% | 4 (0.8) | 9 (1.8) | 8 (1.6) |\n| Feature fit | 20% | 10 (2.0) | 8 (1.6) | 7 (1.4) |\n| Maintenance | 15% | 3 (0.45) | 9 (1.35) | 8 (1.2) |\n| Strategic control | 15% | 10 (1.5) | 5 (0.75) | 5 (0.75) |\n| TOTAL | 100% | 6.55 | 7.9 | 7.05 |\n\nWinner by score: [Vendor A with 7.9]\n\n---\n\n### Strategic Considerations\n\nIs this core to our business?\n[Yes/No - explain]\n\nIf YES (core):\nLean toward build because:\n- Competitive differentiation\n- Want full control\n- Long-term strategic asset\n\nIf NO (non-core):\nLean toward buy because:\n- Commodity capability\n- Not differentiated\n- Let experts handle it\n\nOur assessment:\n[This is core/non-core because...]\n\n---\n\nCompetitive landscape:\n- Do competitors build or buy? [Pattern]\n- Industry best practice: [What leaders do]\n- Our differentiation: [How this factors in]\n\n---\n\nTeam capacity reality:\n- Current sprint velocity: [Points/sprint]\n- This would consume: [X% of capacity for Y months]\n- Impact on roadmap: [What gets delayed]\n- Opportunity cost: [Revenue/value of delayed work]\n\n---\n\n### Recommendation\n\nDecision: [Build / Buy Vendor A / Buy Vendor B]\n\nPrimary rationale:\n[2-3 sentences explaining why]\n\nSupporting factors:\n1. [Reason 1]\n2. [Reason 2]\n3. [Reason 3]\n\nTrade-offs accepted:\n- We're accepting: [Downside of chosen option]\n- In exchange for: [Benefit we're prioritizing]\n\n---\n\nWhy not the alternatives:Why not [other option]:\n[Specific reason this doesn't work]\n\nWhy not [other option]:\n[Specific reason]\n\n---\n\n### Implementation Plan\n\nIf [recommended option] approved:PHASE 1: [Timeframe]\n- [ ] [Action 1]\n- [ ] [Action 2]\n- Owner: [@Name]\n\nPHASE 2: [Timeframe]\n- [ ] [Action]\n- Owner: [@Name]\n\nPHASE 3: [Timeframe]\n- [ ] [Action]\n- Owner: [@Name]\n\nSuccess criteria:\n[How we'll know this decision was right]\n\nTimeline to value:\n[When users/business benefits]\n\n---\n\n### Risk Mitigation\n\nTop risks with chosen option:Risk 1: [Specific risk]\n- Likelihood: [High/Med/Low]\n- Impact: [High/Med/Low]\n- Mitigation: [What we'll do]\n- Contingency: [Plan B if this happens]\n\nRisk 2: [Another risk]\n[Same structure]\n\n---\n\n### Exit Strategy\n\nIf this doesn't work out:Decision points:\n- Month 3: [What we check]\n- Month 6: [Assessment criteria]\n- Month 12: [Go/no-go on continuing]\n\nHow we'd reverse course:\n[Specific steps to undo decision]\n\nCost of switching:\n[Time and money to pivot]\n\n---\n\n### Open Questions\n\nBefore final decision:\n- [ ] [Question to resolve]\n- [ ] [Another question]\n- [ ] [Another]\n\nWho needs to approve:\n- [ ] Engineering lead\n- [ ] Finance (budget)\n- [ ] Legal (contracts if buying)\n- [ ] CEO/CTO\n\n---\n\n### Appendix\n\nDetailed vendor evaluation:\n[Link to full analysis]\n\nFinancial model:\n[Link to spreadsheet]\n\nReference calls:\n[Link to notes]\n\nContract terms:\n[If buying, key terms to negotiate]\n\n</build_vs_buy>",
    "technique": "TCO calculation, strategic fit assessment, risk analysis",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Deciding whether to build feature in-house or buy/integrate external tool"
  },
  {
    "name": "Deprecation Plan",
    "category": "Operations",
    "prompt": "<deprecation_plan>\n\n<deprecation_inputs>\nWHAT YOU'RE DEPRECATING:\n[Feature, product, API version, platform, etc.]\n\nWHY DEPRECATING:\n- [ ] Low usage (not worth maintaining)\n- [ ] Technical debt (holding back product)\n- [ ] Strategic shift (moving to new solution)\n- [ ] Cost (expensive to run)\n- [ ] Security/compliance (can't support anymore)\n\nCURRENT STATE:\n- Users affected: [How many]\n- Usage: [How much/often]\n- Revenue impact: [If applicable]\n- Technical dependencies: [What relies on this]\n\nREPLACEMENT:\n- [ ] Migrating to new version\n- [ ] Replacing with alternative feature\n- [ ] No replacement (just removing)\n</deprecation_inputs>\n\n<deprecation_framework>\n\nYou plan deprecations that minimize user pain. Follow this process:\n\nSTEP 1: Understand impactWho's affected:\n- Total users: [Number]\n- Active users (used in last 30 days): [Number]\n- Power users (daily usage): [Number]\n- Paying customers: [Number]\n- Enterprise customers: [Names if small]\n\nHow they're affected:\n- Workflow broken completely?\n- Inconvenience but alternatives exist?\n- Minor impact?\n\nSegment by impact:\n- Critical impact: Can't use product without this\n- Major impact: Significantly changes workflow\n- Minor impact: Slight inconvenience\n\nFocus on critical/major impact users first.STEP 2: Provide migration pathIf replacing with new version:\n- Document how to migrate\n- Provide migration scripts/tools\n- Offer assisted migration for enterprise\n- Test migration with beta customers\n\nIf no replacement:\n- Document alternatives\n- Explain workarounds\n- Connect to ecosystem partners if applicable\n\nMake migration easy:\n- Step-by-step guide\n- Video tutorials\n- Sample code\n- Support channel\n\nSTEP 3: Set timelineDeprecation phases:Phase 1: Announcement (T-6 months)\n- Announce deprecation\n- Explain why and what's next\n- Give deadline\n\nPhase 2: Deprecation notice (T-3 months)\n- In-product warnings\n- Email reminders\n- Blog post\n\nPhase 3: Final warning (T-1 month)\n- Aggressive in-product notices\n- Direct outreach to active users\n- Last chance communication\n\nPhase 4: Deprecation (T-0)\n- Feature disabled/removed\n- Redirect to alternative\n- Support for migrations\n\nPhase 5: Cleanup (T+1 month)\n- Remove code\n- Update documentation\n- Archive resources\n\nTimeline based on impact:\n- High impact: 12 months notice\n- Medium impact: 6 months notice\n- Low impact: 3 months notice\n\nLegal/contractual:\nCheck if you promised support for X years.\n\nSTEP 4: Communicate clearlyTone:\n- Empathetic (acknowledge inconvenience)\n- Clear (no ambiguity about dates)\n- Helpful (provide path forward)\n- Final (not negotiable)\n\nWhat to communicate:\n- What's being deprecated\n- Why (honest reason)\n- When (specific date)\n- What to do instead\n- How to get help\n\nDon't:\n- Apologize excessively\n- Be vague about timeline\n- Hide the reason\n- Make it sound tentative\n\nDo:\n- Be direct\n- Give concrete dates\n- Provide resources\n- Offer support\n\nSTEP 5: Support users through transitionResources to provide:\n- Migration guide (written)\n- Migration tools (if applicable)\n- Sample code/examples\n- FAQs\n- Support channel\n\nExtra support for:\n- Enterprise customers (white-glove)\n- Power users (direct outreach)\n- Partners/integrators (early notice)\n\nMetrics to track:\n- % users migrated\n- Support ticket volume\n- User complaints\n- Churn related to deprecation\n\nSTEP 6: Handle pushbackCommon objections:\n\"We built our business on this!\"\n→ Response: \"We're providing [X months] notice and [migration path]. We'll support your migration.\"\n\n\"This is the only feature we use!\"\n→ Response: \"Here's the alternative. Here's why we made this decision.\"\n\n\"We can't migrate in this timeline!\"\n→ Response: [Case-by-case extension for enterprise? Or firm deadline?]\n\nBe empathetic but firm.STEP 7: Measure successSuccess criteria:\n- [X%] of users migrated before deadline\n- <[Y] support tickets related to deprecation\n- Churn rate stays <[Z%]\n- No public backlash/bad press\n\nPost-mortem:\n- What went well\n- What we'd do differently\n- Lessons for next deprecation\n\n</deprecation_framework>\n\n---\n\n## DEPRECATION PLAN\n\nWhat's Being Deprecated: [Name]Deprecation Date: [When it's fully removed]Owner: [Name]Status: [Planning / Announced / In Progress / Complete]\n\n---\n\n### Executive Summary\n\nWhat: [One sentence describing what's being deprecated]\n\nWhy: [Primary reason]\n\nWhen: [Deprecation date]\n\nImpact: [Number of users affected]\n\nMigration: [What users should do instead]\n\nRisk level: [Low/Medium/High] based on usage and user impact\n\n---\n\n### What's Being Deprecated\n\nFeature/Product:\n[Detailed description]\n\nCurrent usage:\n- Total users: [X]\n- Active users (last 30 days): [Y]\n- Usage frequency: [Pattern]\n- Revenue tied to this: $[Z] if applicable\n\nWhy we built this originally:\n[Context - what problem it solved, when launched]\n\nWhy deprecating now:\n[Honest explanation]\n\n---\n\n### User Impact Analysis\n\nWHO'S AFFECTED:Tier 1: Critical Impact ([X] users)\n- Who: [Segment description]\n- Impact: [Can't use product without this]\n- Action needed: [What they must do]\n\nTier 2: Major Impact ([Y] users)\n- Who: [Segment]\n- Impact: [Significant workflow change]\n- Action needed: [Migration path]\n\nTier 3: Minor Impact ([Z] users)\n- Who: [Segment]\n- Impact: [Slight inconvenience]\n- Action needed: [Simple change]\n\n---\n\nSPECIFIC CUSTOMERS TO WATCH:Enterprise Customer 1: [Name]\n- Usage: [How they use deprecated feature]\n- Impact: [High/Med/Low]\n- Plan: [Direct outreach, assisted migration]\n- Owner: [@CSM name]\n\nEnterprise Customer 2:\n[Same structure]\n\n---\n\nUSAGE BREAKDOWN:\n\n| User Segment | Count | % of Base | Impact Level | Migration Path |\n|--------------|-------|-----------|--------------|----------------|\n| [Segment 1] | [X] | [Y%] | Critical | [Path] |\n| [Segment 2] | [A] | [B%] | Major | [Path] |\n| [Segment 3] | [C] | [D%] | Minor | [Path] |\n\n---\n\n### Migration Path\n\nRECOMMENDED ALTERNATIVE:Option 1: [New feature/approach]\n- What it is: [Description]\n- How it's better: [Advantages]\n- How to switch: [Steps]\n- Timeline: [How long migration takes]\n\nMigration steps:\n1. [Step 1 with link to docs]\n2. [Step 2]\n3. [Step 3]\n\nFor most users: [X hours of work]\n\n---\n\nOption 2: [Alternative approach if applicable]\n[Same structure]\n\n---\n\nIf no replacement:Workarounds:\n- [Alternative 1]: [How to achieve similar outcome]\n- [Alternative 2]: [Another approach]\n\n---\n\nMIGRATION TOOLS:We're providing:\n- [ ] Migration script: [Link]\n- [ ] Data export tool: [Link]\n- [ ] Step-by-step guide: [Link]\n- [ ] Video tutorial: [Link]\n- [ ] Sample code: [Link]\n\nSupport:\n- Documentation: [Link]\n- Support channel: [Slack/email]\n- Office hours: [Schedule]\n\n---\n\n### Timeline\n\nPHASE 1: ANNOUNCEMENT (TODAY → [Date])Date: [Start date]\n\nActions:\n- [ ] Announce deprecation (blog, email, in-app)\n- [ ] Update documentation\n- [ ] Notify enterprise customers directly\n- [ ] Post to changelog\n- [ ] Create FAQ\n\nGoal: All users aware and have migration plan\n\n---\n\nPHASE 2: DEPRECATION WARNING ([Date] → [Date])Date: [T-3 months]\n\nActions:\n- [ ] In-product warning banner\n- [ ] Email reminders (weekly)\n- [ ] Direct outreach to active users\n- [ ] Migration webinar\n- [ ] Update API docs with deprecation notice\n\nGoal: 50% of users migrated\n\n---\n\nPHASE 3: FINAL WARNING ([Date] → [Date])Date: [T-1 month]\n\nActions:\n- [ ] Aggressive in-product warnings\n- [ ] Daily email reminders\n- [ ] Personal outreach to remaining power users\n- [ ] Block new usage (if applicable)\n- [ ] \"Last chance\" communication\n\nGoal: 90% of users migrated\n\n---\n\nPHASE 4: DEPRECATION ([Date])Date: [Deprecation date]\n\nActions:\n- [ ] Feature disabled/API returns errors\n- [ ] Redirect users to alternative\n- [ ] Final communication\n- [ ] Support team ready for questions\n\nImpact: [X%] of users still not migrated - support burden\n\n---\n\nPHASE 5: CLEANUP ([Date] → [Date])Date: [T+1 month]\n\nActions:\n- [ ] Remove code from codebase\n- [ ] Archive documentation\n- [ ] Remove from marketing site\n- [ ] Update contracts/terms\n- [ ] Post-mortem with team\n\nGoal: Fully removed, no ongoing costs\n\n---\n\n### Communication Plan\n\nANNOUNCEMENT EMAIL (T-6 months):Subject: [Feature] will be deprecated on [Date]\n\nBody:Hi [User],  We're reaching out to let you know that [feature] will be deprecated on [Date].  WHY: [Honest reason in 1-2 sentences]  WHAT THIS MEANS: [Impact on them]  WHAT TO DO: [Clear next step with link]  TIMELINE: - Now: Start planning migration - [Date]: Deprecation warnings in product - [Date]: Feature fully deprecated  We're here to help. [Support resources]  Thanks, [Team]\n\n---\n\nIN-PRODUCT WARNING (T-3 months):Banner:⚠️ [Feature] will be deprecated on [Date]. Migrate to [alternative] now. [Learn more]Modal (for power users):[Feature] is being deprecated  You're actively using this feature. Starting [Date], it will no longer be available.  [Button: See migration guide] [Button: Remind me later]\n\n---\n\nFINAL WARNING EMAIL (T-1 month):Subject: [Feature] deprecates in 30 days\n\nBody:Hi [User],  This is your final reminder: [Feature] will stop working on [Date].  You're still using this feature. Here's what you need to do: [Clear migration steps]  After [Date], [specific impact].  Need help? [Support contact]  Don't wait - migrate today.  [Team]\n\n---\n\nDEPRECATION DAY EMAIL:Subject: [Feature] is now deprecated\n\nBody:As of today, [feature] is deprecated.  If you're trying to use it, you'll see [error/message].  Next steps: [What to do now]  Need help? [Support]\n\n---\n\n### Support Plan\n\nRESOURCES:Documentation:\n- Migration guide: [Link]\n- FAQ: [Link]\n- Troubleshooting: [Link]\n- API documentation updates: [Link]\n\nSupport channels:\n- Email: [Address]\n- Slack: [Channel]\n- Office hours: [Every Tuesday 2-3pm]\n\nSupport SLAs:\n- Response time: [24 hours]\n- Migration assistance: [Available for enterprise]\n\n---\n\nEXPECTED SUPPORT BURDEN:Ticket volume estimate:\n- T-3 months: +[X] tickets/week\n- T-1 month: +[Y] tickets/week\n- Deprecation day: +[Z] tickets/day\n- T+1 month: [W] tickets/week declining\n\nSupport team readiness:\n- [ ] Support team trained on migration\n- [ ] Canned responses prepared\n- [ ] Escalation path clear\n- [ ] Extra coverage scheduled\n\n---\n\nENTERPRISE CUSTOMER PLAN:White-glove migration:\n- [ ] Schedule 1:1 calls with each enterprise customer\n- [ ] Provide custom migration scripts if needed\n- [ ] Dedicated eng support during migration\n- [ ] Follow-up after migration complete\n\nOwner: [@CSM name or tech contact]\n\n---\n\n### Risk Management\n\nRISK 1: Users don't migrate in timeLikelihood: [Medium/High]\n\nImpact: [Support burden, churn, complaints]\n\nMitigation:\n- Aggressive communication\n- Make migration very easy\n- Offer assisted migration\n- Clear deadline\n\nContingency:\n- Extend deadline by [X weeks] if <80% migrated?\n- Or firm deadline regardless?\n\nDecision criteria: [What triggers extension]\n\n---\n\nRISK 2: Technical issues with migrationLikelihood: [Low/Med]\n\nImpact: [Blocked users, delays]\n\nMitigation:\n- Test migration path extensively\n- Beta test with friendly customers\n- Have rollback plan\n\nContingency:\n- Dedicated eng on-call during migration period\n\n---\n\nRISK 3: Customer churnLikelihood: [Low/Med]\n\nImpact: [Revenue loss]\n\nMitigation:\n- Early outreach to enterprise\n- Alternative solutions ready\n- CSM involvement\n\nMonitoring:\n- Track churn rate during deprecation\n- Exit interviews with churned customers\n- Adjust if churn spikes\n\n---\n\nRISK 4: Bad press/social media backlashLikelihood: [Low/Med]\n\nImpact: [Reputation]\n\nMitigation:\n- Honest communication\n- Generous timeline\n- Good migration path\n\nResponse plan:\n- [ ] PR team aware\n- [ ] Response templates ready\n- [ ] Monitor Twitter/Reddit\n- [ ] Respond quickly to complaints\n\n---\n\n### Success Metrics\n\nMIGRATION GOALS:By T-3 months:\n- [ ] 30% of users migrated\n- [ ] All enterprise customers have migration plan\n\nBy T-1 month:\n- [ ] 70% of users migrated\n- [ ] <50 support tickets/week\n\nBy deprecation date:\n- [ ] 90% of users migrated\n- [ ] Zero critical customer issues\n\nPost-deprecation:\n- [ ] Churn rate <[X%]\n- [ ] Support tickets declining\n- [ ] No major complaints\n\n---\n\nTRACKING:Dashboard: [Link]\n\nMetrics to monitor:\n- % users on deprecated feature (declining)\n- % users on new feature (increasing)\n- Support ticket volume\n- User sentiment (NPS, surveys)\n- Churn rate\n\nWeekly review: [When and with whom]\n\n---\n\n### Financial Impact\n\nCOST OF MAINTAINING:\n- Engineering time: [X hours/month]\n- Infrastructure: $[Y]/month\n- Support burden: [Z tickets/month]\n- Total annual cost: $[A]COST OF DEPRECATING:\n- Engineering time (cleanup): [Hours]\n- Support burden (migration): [Est tickets]\n- Potential churn: [Est revenue at risk]\n- Total deprecation cost: $[B]NET BENEFIT:\n[Savings after deprecation]\n\n---\n\n### Legal & Contractual\n\nCONTRACTUAL OBLIGATIONS:Check:\n- [ ] Terms of service (did we promise support?)\n- [ ] Enterprise contracts (guaranteed feature access?)\n- [ ] SLAs (does removing this breach SLA?)\n\nActions needed:\n- [ ] Update ToS: [What changes]\n- [ ] Contract amendments: [For which customers]\n- [ ] Legal review: [Complete by date]\n\n---\n\n### Post-Mortem Plan\n\n30 DAYS POST-DEPRECATION:Review questions:\n- Did timeline work? (Too short/long?)\n- Was communication effective?\n- Did migration path work?\n- What surprised us?\n- What would we do differently?\n\nMetrics to review:\n- Actual vs expected migration rate\n- Support burden vs estimate\n- Churn impact\n- User sentiment\n\nDocument learnings:\n[For next deprecation]\n\n---\n\n### Approval & Sign-Off\n\nRequires approval from:\n- [ ] Engineering lead (feasibility, timeline)\n- [ ] Support lead (readiness for volume)\n- [ ] Customer success (enterprise impact)\n- [ ] Legal (contractual review)\n- [ ] CEO/Product lead (final decision)\n\nApproval date: [When]\n\nGo/no-go criteria:\n- Migration path tested and works\n- Communication plan ready\n- Support team trained\n- Legal cleared\n\n---\n\n### FAQs\n\nQ: Why are you deprecating this?\nA: [Clear, honest answer]\n\nQ: Can I keep using it?\nA: No. After [date], it will no longer work.\n\nQ: What should I use instead?\nA: [Alternative with link]\n\nQ: How long will migration take?\nA: [Realistic estimate]\n\nQ: Will you extend the deadline?\nA: [Firm answer]\n\nQ: What if I can't migrate in time?\nA: [Options: support, workaround, or \"feature will stop working\"]\n\nQ: Can I export my data?\nA: [Yes/no and how]\n\n</deprecation_plan>",
    "technique": "User impact analysis, migration planning, communication strategy",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Need to sunset a feature, product, or version - do it without angering users"
  },
  {
    "name": "Customer Journey Map",
    "category": "Discovery",
    "prompt": "<customer_journey_map>\n\n<journey_inputs>\nWHAT YOU'RE MAPPING:\n[Specific user journey - e.g., \"new user onboarding\", \"enterprise purchase process\", \"daily workflow\"]\n\nUSER PERSONA:\n[Who you're mapping this for - specific type of user]\n\nYOUR RESEARCH:\n[What you have - interviews, analytics, support tickets, etc.]\n\nWHAT YOU WANT TO UNDERSTAND:\n- [ ] Where do users struggle?\n- [ ] Where do they drop off?\n- [ ] Where are opportunities?\n- [ ] What's the emotional journey?\n</journey_inputs>\n\n<mapping_framework>\n\nYou create customer journey maps that reveal insights. Your analysis process:\n\nSTEP 1: Define the journey boundariesStart point:\nWhere does this journey actually begin?\n(Often earlier than you think - awareness, not just signup)\n\nEnd point:\nWhat's the successful outcome?\n(Not \"completed form\" but \"achieved goal\")\n\nJourney stages:\nBreak into 3-7 distinct phases\n- Too few = not useful\n- Too many = overwhelming\n\nExample journeys:SaaS onboarding:\n1. First hear about product\n2. Sign up / trial\n3. First use / activation\n4. Habit formation\n5. Becoming power user\n\nB2B purchase:\n1. Problem recognition\n2. Research / comparison\n3. Trial / POC\n4. Procurement / contracting\n5. Implementation\n6. Adoption across team\n\nSTEP 2: Map what user does at each stage\n\nFor each stage:\n\nActions:\n- What are they literally doing?\n- What decisions are they making?\n- What tools are they using?\n\nTouchpoints:\n- Where do they interact with your product/company?\n- Marketing site, sales call, product, support, etc.\n\nQuestions/needs:\n- What do they need to know?\n- What are they trying to accomplish?\n- What concerns do they have?\n\nSTEP 3: Identify emotions and pain pointsEmotional state:\n- Excited, confused, frustrated, confident?\n- Use emotional curve (high/low)\n\nPain points:\n- What's frustrating?\n- What's confusing?\n- What's taking too long?\n- What's blocking them?\n\nRate pain intensity:\n- 🔥🔥🔥 = Critical (blocks progress)\n- 🔥🔥 = Major (significant friction)\n- 🔥 = Minor (annoying but manageable)\n\nSTEP 4: Find the gapsInformation gaps:\n- What do they need to know but don't?\n\nTool gaps:\n- What do they need to do but can't?\n\nSupport gaps:\n- Where do they need help but don't get it?\n\nExperience gaps:\n- Where does experience not match expectations?\n\nSTEP 5: Identify opportunities\n\nFor each pain point:\n- Quick win: [Easy fix, high impact]\n- Major improvement: [Bigger effort, transforms experience]\n- Delight moment: [Exceeds expectations]\n\nSTEP 6: Prioritize opportunitiesImpact × Effort matrix:\n- High impact, low effort = DO NOW\n- High impact, high effort = ROADMAP\n- Low impact, low effort = NICE TO HAVE\n- Low impact, high effort = DON'T DO\n\nSTEP 7: Map to product decisions\n\nEach insight should drive action:\n- Feature to build\n- Copy to change\n- Flow to redesign\n- Support to add\n- Communication to improve\n\nNow create a journey map adapted to the specific context provided. Don't force it into a rigid template - adjust based on what journey you're actually mapping.\n\n</mapping_framework>\n\n---\n\n## Example Journey Map Structure\n\n(Adapt this based on the actual journey being mapped)\n\n### Journey Overview\n\nJourney: [Name]User: [Persona]Goal: [What success looks like]Research basis: [Interviews with X users, analytics from Y sessions, Z support tickets]\n\n---\n\n### The Journey Stages\n\nStage 1: [Name]What they do:\n[Actions, decisions, interactions]\n\nTouchpoints:\n[Where they interact with you]\n\nNeeds:\n[What they're trying to accomplish]\n\nEmotions: [Happy/frustrated/confused - use whatever makes sense]\n\nPain points:\n- 🔥🔥🔥 [Critical blocker]\n- 🔥🔥 [Major friction]\n\nOpportunities:\n[What could be better]\n\n---\n\nStage 2: [Name]\n\n[Same type of info, adapted to this stage]\n\n---\n\n[Continue for all stages in this specific journey]\n\n---\n\n### Key Insights\n\nBiggest pain points:\n1. [Pain point with stage and impact]\n2. [Another]\n\nCritical moments:\n[Moments where users succeed or fail]\n\nEmotional peaks/valleys:\n[Where experience is great or terrible]\n\nDrop-off points:\n[Where users abandon journey]\n\n---\n\n### Prioritized Opportunities\n\nQUICK WINS:\n- [Opportunity 1]: [Why + expected impact]\n- [Opportunity 2]\n\nROADMAP:\n- [Bigger opportunity]: [Why + effort]\n\nAVOID:\n- [Thing not worth doing]: [Why not]\n\n---\n\n### Action Plan\n\n[Specific next steps based on insights]\n\n</customer_journey_map>",
    "technique": "Experience mapping, pain point identification, moment analysis",
    "tools": "Claude Projects, NotebookLM, Miro",
    "useCase": "Understand end-to-end user experience to find opportunities and pain points"
  },
  {
    "name": "Experiment Readout",
    "category": "Analytics",
    "prompt": "\n\n<experiment_readout>\n\n<experiment_inputs>\nPASTE YOUR DATA:\n[Raw results - control vs treatment metrics, or describe what you have]\n\nTHE TEST:\n- What you tested: [Feature/change]\n- Hypothesis: [What you expected]\n- Primary metric: [What you measured]\n- Duration: [How long]\n- Sample size: [Users per variant]\n\nYOUR DECISION CRITERIA:\n[What was agreed upfront - ship if X, don't ship if Y]\n</experiment_inputs>\n\n<readout_framework>\n\nYou analyze experiments to make data-driven decisions. Your process:\n\n**STEP 1: Validate the experiment**\n\n**Before analyzing results, check:**\n- Did test run for planned duration?\n- Was sample size sufficient?\n- Was traffic split as expected (50/50)?\n- Any implementation bugs?\n- Any external factors (holidays, outages, campaigns)?\n\n**Red flags that invalidate results:**\n- Imbalanced traffic split\n- Test stopped early because \"it was winning\"\n- Major bug discovered in variant\n- External event during test (site down, viral moment, etc.)\n\n**If experiment is invalid → don't ship, re-run test**\n\n**STEP 2: Analyze primary metric**\n\n**The question:**\nDid treatment move primary metric vs control?\n\n**Statistical test:**\n- Difference: [X%]\n- P-value: [Number]\n- Confidence interval: [Range]\n- Statistical significance: [Yes if p<0.05]\n\n**Practical significance:**\n- Is difference big enough to matter?\n- Does it meet your MDE (minimum detectable effect)?\n- Business impact: [Calculate actual value]\n\n**Common pitfall:**\n\"Statistically significant\" ≠ \"big enough to care about\"\nA 0.1% improvement might be significant but not worth the complexity.\n\n**STEP 3: Check guardrail metrics**\n\n**Did anything break?**\n\nFor each guardrail metric:\n- Did it stay stable? (within ±X%)\n- Did it improve? (bonus)\n- Did it degrade? (concern)\n\n**If guardrails broke:**\nEven if primary metric improved, might not ship if:\n- User experience degraded\n- Revenue decreased\n- Key flow broke\n- NPS dropped\n\n**STEP 4: Segment analysis**\n\n**Did it work for everyone or just some users?**\n\nSegment by:\n- New vs returning users\n- Mobile vs desktop\n- Geography\n- User cohort\n- Product tier\n\n**Patterns to look for:**\n- Works great for segment A, hurts segment B\n- Only works for new users (or only returning)\n- Platform-specific (desktop yes, mobile no)\n\n**If segmented results:**\nConsider shipping to only winning segments.\n\n**STEP 5: Look for surprises**\n\n**Secondary metrics:**\nDid anything unexpected move?\n\n**Qualitative feedback:**\n- User comments\n- Support tickets\n- Sales feedback\n- Social sentiment\n\n**Sometimes data says \"ship\" but users say \"this is confusing\"**\n→ Investigate the disconnect\n\n**STEP 6: Calculate business impact**\n\n**If you ship to 100%:**\n\n**Impact calculation:**\n[Metric lift %] × [baseline volume] × [value per conversion]\n= [Annual impact]\n\n\n**Example:**\n5% conversion lift × 10K users/mo × $100 LTV\n= 500 more conversions/mo × $100\n= $50K/mo = $600K/year\n\n**Compare to:**\n- Cost to build/maintain\n- Opportunity cost (what else could team build)\n\n**STEP 7: Make recommendation**\n\n**Framework:**\n\n**Ship if:**\n- Primary metric improved (statistically + practically significant)\n- Guardrails stable\n- Positive or neutral qualitative feedback\n- Business impact > cost\n\n**Don't ship if:**\n- Primary metric flat or negative\n- Guardrails broke\n- Negative qualitative feedback\n- Not worth the complexity\n\n**Iterate if:**\n- Results promising but mixed\n- Some segments won, some lost\n- Qualitative concerns despite data win\n\n**Re-test if:**\n- Results inconclusive (not enough data)\n- External factors muddied results\n- Implementation issues\n\nNow analyze the provided experiment data and make a clear recommendation.\n\n</readout_framework>\n\n---\n\n## Example Experiment Readout\n\n*(Structure will vary based on the experiment, but here's a typical flow)*\n\n### Decision Summary\n\n**Recommendation:** [SHIP / DON'T SHIP / ITERATE / RE-TEST]\n\n**One-line rationale:**\n[Primary metric moved X%, guardrails stable, qualitative positive - ship it]\n\n**If shipping:**\n- To whom: [100% / specific segments / gradual rollout]\n- When: [Timeline]\n- With what: [Any changes based on learnings]\n\n---\n\n### Experiment Overview\n\n**Tested:** [Description]  \n**Hypothesis:** [Expected outcome]  \n**Duration:** [Dates]  \n**Sample size:** [N users per variant]  \n**Traffic split:** [Actual split achieved]\n\n**Test validity:** ✅ Valid / ⚠️ Concerns / ❌ Invalid\n[Note any issues]\n\n---\n\n### Primary Metric Results\n\n**Metric:** [Name]\n\n**Control:** [X%]  \n**Treatment:** [Y%]  \n**Lift:** [+Z%] (absolute: [A percentage points])\n\n**Statistical significance:** [Yes/No]  \n**P-value:** [Number]  \n**Confidence interval:** [Range]\n\n**Practical significance:**\n[Is this big enough to matter? Business impact calculation]\n\n**Interpretation:**\n[What this means in plain language]\n\n---\n\n### Guardrail Metrics\n\n| Metric | Control | Treatment | Change | Status |\n|--------|---------|-----------|--------|--------|\n| [Metric 1] | [X] | [Y] | [±Z%] | [✅/⚠️/❌] |\n| [Metric 2] | [A] | [B] | [±C%] | [Status] |\n\n**Issues:** [Any guardrails that broke]\n\n---\n\n### Segment Analysis\n\n**By [segment type]:**\n\n[Show results for key segments - might be table, might be bullet points, adapt to data]\n\n**Key finding:**\n[Did it work better for some segments?]\n\n**Implication:**\n[Should we ship to all users or just certain segments?]\n\n---\n\n### Unexpected Findings\n\n**What surprised us:**\n[Things that moved unexpectedly]\n\n**Qualitative signals:**\n[User feedback, support tickets, sales comments]\n\n**Consistency check:**\n[Do data and qualitative align or conflict?]\n\n---\n\n### Business Impact\n\n**If we ship to 100%:**\n\n[Calculate actual impact in revenue, users, or other business metric]\n\n**ROI:**\n[Impact vs cost to build/maintain]\n\n---\n\n### Recommendation Details\n\n**Why [ship/don't ship]:**\n[Detailed reasoning]\n\n**Confidence level:** [High/Medium/Low]\n\n**Risks:**\n[What could still go wrong]\n\n**Next steps:**\n[Specific actions]\n\n</experiment_readout>",
    "technique": "Statistical interpretation, segment analysis, decision framing",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "A/B test finished, need to analyze results and make ship/no-ship decision"
  },
  {
    "name": "Define Success Metrics for Feature",
    "category": "Analytics",
    "prompt": "<success_metrics>\n\n<feature_inputs>\nWHAT YOU'RE BUILDING:\n[Feature description]\n\nWHY YOU'RE BUILDING IT:\n- Problem it solves: [User pain]\n- Expected benefit: [What improves]\n- Strategic goal: [Company objective it supports]\n\nWHO IT'S FOR:\n[Target users]\n\nCURRENT STATE:\n[Relevant baseline metrics if you have them]\n</feature_inputs>\n\n<metrics_framework>\n\nYou define metrics that actually measure success. Your process:\n\nSTEP 1: Start with the \"why\"Don't start with \"what can we measure\"\nStart with \"what does success look like?\"\n\nSuccess definition:\n- For users: [What gets better for them]\n- For business: [What improves for company]\n\nExample:\nFeature: \"Auto-save\"\nSuccess for users: Never lose work\nSuccess for business: Fewer frustrated users, less support\n\nSTEP 2: Choose metric typeInput metrics (leading indicators):\nMeasure usage/engagement\n- % of users who try feature\n- Frequency of use\n- Feature adoption rate\n\nPro: Can measure quickly\nCon: Doesn't prove value\n\nOutput metrics (lagging indicators):\nMeasure actual outcomes\n- Task completion time (decreased)\n- Error rate (decreased)\n- User retention (increased)\n- Revenue (increased)\n\nPro: Proves actual value\nCon: Takes time to measure\n\nBest practice: Track bothSTEP 3: Define primary metricYour ONE metric that matters mostGood primary metrics:\n- Directly tied to value delivered\n- Measurable within reasonable timeframe\n- Sensitive (will move if feature works)\n- Directional (clear if good or bad)\n\nBad primary metrics:\n- Vanity (looks good but means nothing)\n- Gamed easily (can fake success)\n- Delayed too long (can't learn)\n\nSTEP 4: Define guardrail metricsWhat should NOT get worse:Common guardrails:\n- Performance (load time)\n- Reliability (error rates)\n- Core flows (don't break key workflows)\n- User satisfaction (NPS)\n\nSTEP 5: Set realistic targetsTarget framework:Baseline: [Current state]\nTarget: [What you're aiming for]\nTimeline: [When you'll measure]\n\nHow to set targets:\n- Look at past feature performance\n- Industry benchmarks if available\n- Directional improvement (any increase is good)\n- Stretch but achievable\n\nAvoid:\n- Pulled from thin air\n- Overly ambitious (sets up for failure)\n- Too conservative (not worth building)\n\nSTEP 6: Plan measurementHow you'll track:\n- What events to instrument\n- Where to track them\n- Dashboard to monitor\n- Review cadence\n\nWhen you'll know:\n- Early signal (1 week)\n- Clear signal (1 month)\n- Long-term impact (3 months)\n\nNow define metrics for the specific feature described, adapted to its context and goals.\n\n</metrics_framework>\n\n---\n\n## Example Metrics Definition\n\n(Adapt structure based on feature type)\n\n### Feature Success Criteria\n\nFeature: [Name]Goal: [What this achieves]Launch date: [When]\n\n---\n\n### Success Definition\n\nFor users:\n[What gets better for them - specific and measurable]\n\nFor business:\n[What improves for company - tied to goals]\n\nWe'll know it's working if:\n[Concrete observable outcomes]\n\n---\n\n### Primary Metric\n\nMetric: [Name and definition]\n\nWhy this metric:\n[Why this is the right thing to measure]\n\nBaseline: [Current state]Target: [Goal]Timeline: [When measured]\n\nCalculation:\n[How it's measured - formula if needed]\n\n---\n\n### Secondary Metrics\n\nMetric 1: [Name]\n- Current: [Baseline]\n- Target: [Goal]\n- Why tracking: [What this tells us]\n\nMetric 2: [Name]\n[Same structure]\n\n---\n\n### Guardrail Metrics\n\n| Metric | Baseline | Must stay above/below | Why it matters |\n|--------|----------|----------------------|----------------|\n| [Metric 1] | [X] | [Threshold] | [Reason] |\n| [Metric 2] | [Y] | [Threshold] | [Reason] |\n\n---\n\n### Measurement Plan\n\nEvents to track:\n[Specific events and what they capture]\n\nDashboard: [Where monitored]\n\nReview cadence:\n- Week 1: [What we check]\n- Month 1: [Assessment]\n- Quarter 1: [Long-term view]\n\nDecision criteria:\n- Ship to 100% if: [Conditions]\n- Iterate if: [Conditions]\n- Rollback if: [Conditions]\n\n---\n\n### Learning Goals\n\nBeyond metrics, we want to learn:\n[Qualitative questions to answer]\n\nHow we'll learn:\n[Surveys, interviews, analytics, etc.]\n\n</success_metrics>",
    "technique": "Metric selection, target setting, measurement planning",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "About to build feature, need to define how you'll measure success"
  },
  {
    "name": "Analytics Instrumentation Spec\n",
    "category": "Analytics",
    "prompt": "<analytics_instrumentation>\n\n<instrumentation_inputs>\nWHAT YOU'RE INSTRUMENTING:\n[Feature or flow]\n\nWHY YOU NEED TRACKING:\n[What questions you're trying to answer]\n\nYOUR ANALYTICS SETUP:\n- Tool: [Segment, Amplitude, Mixpanel, custom, etc.]\n- Current naming convention: [If you have one]\n- Who implements: [Eng team]\n</instrumentation_inputs>\n\n<instrumentation_framework>\n\nYou write instrumentation specs that engineers can implement. Your process:\n\nSTEP 1: Map the user flowList every step:\n1. User does [action]\n2. System responds [how]\n3. User sees [what]\n4. User does [next action]\n\nFor each step, ask:\n- Do we need to know this happened?\n- What properties matter?\n- How will we analyze this?\n\nSTEP 2: Define eventsEvent naming convention:Pattern: [object]_[action] or [action]_[object]Examples:\n- button_clicked or click_button\n- form_submitted\n- page_viewed\n- feature_enabledBe consistent with existing events.Event types:Interaction events:\nUser does something (click, type, submit)\n\nSystem events:\nSystem does something (email_sent, job_completed)\n\nPage/screen events:\nUser navigates (page_viewed, screen_opened)\n\nSTEP 3: Define propertiesFor each event, specify:Required properties:\n- Always included\n- Needed for analysis\n\nOptional properties:\n- Sometimes included\n- Contextual info\n\nProperty naming:\n- snake_case or camelCase (pick one)\n- Descriptive names\n- Consistent across events\n\nCommon properties:\n- user_id\n- timestamp (usually automatic)\n- session_id\n- device_type\n- platform\n\nSTEP 4: Specify data types\n\nFor each property:\n- String: \"value\"\n- Number: 123\n- Boolean: true/false\n- Array: [\"item1\", \"item2\"]\n- Object: {nested: \"data\"}\n\nSTEP 5: Document expected valuesFor enums/categories:\nList all possible values\n\nExample:button_location: \"header\" | \"sidebar\" | \"footer\" | \"modal\"For numbers:\nExpected range or format\n\nSTEP 6: Show implementation examplePseudo-code helps engineers:javascript analytics.track('feature_enabled', {   feature_name: 'auto_save',   user_id: user.id,   plan_type: user.plan,   enabled_by: 'user' // or 'default' }); STEP 7: Explain analysis planWhy each event matters:\n\"We'll use feature_enabled to calculate adoption rate by plan type\"\n\nThis helps engineers understand priority and context.\n\nNow create an instrumentation spec for the described feature.\n\n</instrumentation_framework>\n\n---\n\n## Example Instrumentation Spec\n\n(Structure adapts to the feature complexity)\n\n### Instrumentation Spec: [Feature Name]\n\nOwner: [Your name]Engineer: [@Name]Target: [Sprint/date]Analytics tool: [Tool name]\n\n---\n\n### Overview\n\nWhat we're tracking:\n[Feature/flow description]\n\nWhy we need this data:\n[Questions we're answering]\n\nAnalysis plan:\n[How we'll use this data]\n\n---\n\n### Events to Implement\n\nEVENT 1: [event_name]When to fire:\n[Specific trigger - \"When user clicks 'Save' button\"]\n\nRequired properties:javascript {   user_id: string,          // Unique user identifier   feature_name: string,     // \"auto_save\"   action_type: string,      // \"manual\" | \"automatic\"   file_size: number,        // Size in KB   file_type: string         // \"document\" | \"spreadsheet\" | \"presentation\" } Optional properties:javascript {   time_since_last_save: number,  // Seconds   changes_count: number          // Number of edits since last save } Example:javascript analytics.track('file_saved', {   user_id: '12345',   feature_name: 'auto_save',   action_type: 'automatic',   file_size: 245,   file_type: 'document',   time_since_last_save: 30,   changes_count: 5 }); \n\n---\n\nEVENT 2: [another_event]\n\n[Same structure]\n\n---\n\n[Continue for all events]\n\n---\n\n### Property Definitions\n\nShared properties (all events):\n\n| Property | Type | Description | Example |\n|----------|------|-------------|---------|\n| user_id | string | Unique user ID | \"usr_123\" |\n| session_id | string | Session identifier | \"sess_456\" |\n| timestamp | datetime | Event time (auto) | ISO 8601 |\n| platform | string | \"web\"\\|\"mobile\"\\|\"api\" | \"web\" |\n\nFeature-specific properties:\n\n[Define properties unique to this feature]\n\n---\n\n### Implementation Notes\n\nWhere to implement:\n[Specific files/components]\n\nTesting:\n[How to verify events fire correctly]\n\nRollout:\n[Any phasing or feature flags]\n\n---\n\n### Validation Checklist\n\nBefore shipping:\n- [ ] All events firing in dev\n- [ ] Properties have correct data types\n- [ ] Can see events in analytics tool\n- [ ] Dashboard built (if needed)\n- [ ] Documentation updated\n\n</analytics_instrumentation>",
    "technique": "Event specification, property definition, tracking completeness",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Need to tell eng what events to track for new feature"
  },
  {
    "name": "Voice of Customer Report",
    "category": "Discovery",
    "prompt": "<voc_report>\n\n<voc_inputs>\nYOUR FEEDBACK SOURCES:\n[Paste or describe what you have]\n- Support tickets: [Number/summary]\n- Sales conversations: [Notes]\n- User interviews: [Transcripts]\n- NPS responses: [Comments]\n- Social media: [Mentions]\n- Reviews: [G2, App Store, etc.]\n\nTIME PERIOD:\n[What timeframe this covers]\n\nWHAT YOU WANT TO KNOW:\n- [ ] Top pain points\n- [ ] Feature requests\n- [ ] Competitive mentions\n- [ ] Sentiment trends\n- [ ] Segment differences\n</voc_inputs>\n\n<voc_framework>\n\nYou synthesize customer feedback into insights that drive decisions. Your process:\n\nSTEP 1: Aggregate all sourcesCollect feedback from:\n- Support tickets (complaints, questions)\n- Sales calls (objections, requests)\n- User interviews (deep insights)\n- Surveys (NPS, CSAT comments)\n- Reviews (public feedback)\n- Social (Twitter, Reddit mentions)\n- Community/forum posts\n- Churn interviews (why they left)\n\nVolume matters:\nTrack how many mentions each theme gets.\n\nSTEP 2: Extract themesDon't just list feedback items.\nGroup into themes:\n\nPattern recognition:\n- \"Can't export data\" + \"Need CSV download\" + \"Want data backup\" = THEME: Data portability\n- \"Too expensive\" + \"Can't afford\" + \"Costs too much\" = THEME: Pricing concerns\n\nTheme categories:\n- Pain points (what's broken/frustrating)\n- Feature requests (what they want)\n- Usability issues (what's confusing)\n- Competitive mentions (who they compare to)\n- Praise (what's working)\n\nSTEP 3: Quantify themesFor each theme:\n- Mentions: [How many times came up]\n- Sources: [Which channels]\n- User types: [Who mentioned it]\n- Severity: [How painful]\n- Trend: [Increasing/stable/decreasing]\n\nPriority formula:\nFrequency × Severity × Strategic fit\n\nSTEP 4: Add contextFor each theme, capture:\n- Representative quotes (exact words)\n- Specific examples\n- Impact on users\n- Current workarounds\n- Competitive context\n\nSTEP 5: Segment analysisDo themes differ by:\n- Customer size (SMB vs Enterprise)\n- Industry vertical\n- Product tier (free vs paid)\n- User role (admin vs end user)\n- Geography\n\nExample insight:\n\"Pricing concerns mostly from SMB, not enterprise\"\n\nSTEP 6: Connect to actionsFor each theme:\n- Quick fix possible? [What]\n- Roadmap item? [When]\n- Already being addressed? [How]\n- Not addressable? [Why]\n\nSTEP 7: Show trendsCompare to previous period:\n- New themes emerging\n- Old themes declining\n- Sentiment shift\n- Volume changes\n\nNow synthesize the provided feedback into an actionable report.\n\n</voc_framework>\n\n---\n\n## Example VOC Report Structure\n\n(Adapt based on feedback sources and themes found)\n\n### Voice of Customer Report\n\nPeriod: [Dates]Sources: [What was analyzed]Total feedback items: [Number]\n\n---\n\n### Executive Summary\n\nTop 3 themes:\n1. [Theme with impact]\n2. [Theme with impact]\n3. [Theme with impact]\n\nRecommended actions:\n[Key priorities based on feedback]\n\nSentiment trend:\n[Improving/stable/declining]\n\n---\n\n### Major Themes\n\nTHEME 1: [Name]Mentioned: [X times across Y sources]Severity: [High/Med/Low]Trend: [↑/→/↓ vs last period]\n\nWhat customers say:\n\"[Quote 1]\" - [Source]\n\"[Quote 2]\" - [Source]\n\nImpact:\n[How this affects users]\n\nWho's affected:\n[Segment analysis]\n\nCurrent state:\n[What we do today / gap]\n\nRecommendation:\n[What to do about it]\n\n---\n\nTHEME 2: [Name]\n\n[Same structure]\n\n---\n\n[Continue for all major themes]\n\n---\n\n### Competitive Intelligence\n\nCompetitors mentioned:\n- [Competitor A]: [Context of mentions]\n- [Competitor B]: [Context]\n\nWin/loss themes:\n[Why we win or lose vs competitors]\n\n---\n\n### Sentiment Analysis\n\n[Overall sentiment breakdown and trends]\n\n---\n\n### Recommended Actions\n\nIMMEDIATE:\n- [Action based on feedback]\n\nROADMAP:\n- [Feature to prioritize]\n\nINVESTIGATE:\n- [Theme to research further]\n\n---\n\n### Appendix\n\nDetailed feedback by source:\n[Links or summaries]\n\nMethodology:\n[How analysis was done]\n\n</voc_report>",
    "technique": "Multi-source synthesis, theme extraction, signal prioritization",
    "tools": "Claude Projects, NotebookLM, ChatGPT Projects",
    "useCase": "Aggregate feedback from multiple sources into actionable insights"
  },
  {
    "name": "Jobs-to-be-Done Interview Guide\n",
    "category": "Discovery",
    "prompt": "<jtbd_interview>\n\n<interview_inputs>\nWHO YOU'RE INTERVIEWING:\n[Customer segment, role, context]\n\nWHAT YOU'RE INVESTIGATING:\n[Product area, decision, or behavior]\n\nYOUR HYPOTHESIS:\n[What job you think they're hiring your product for]\n\nINTERVIEW LENGTH:\n[30 min, 45 min, 60 min]\n</interview_inputs>\n\n<jtbd_framework>\n\nYou create interview guides that uncover the real jobs customers hire products for. Your process:\n\nSTEP 1: Set interview objectivesWhat you're trying to learn:\n- The job they're trying to get done\n- Current solutions they use\n- Switching triggers\n- Success criteria\n- Emotional and social dimensions\n\nNOT just feature requests.STEP 2: Structure the interviewOpening (5 min):\nBuild rapport, explain purpose, set expectations\n\nContext setting (10 min):\nUnderstand their role, responsibilities, current workflow\n\nJob exploration (20-30 min):\nDeep dive on the specific situation\n\nWrap-up (5 min):\nAny questions, next steps, thank you\n\nSTEP 3: Write the core questionsJTBD interview principles:Ask about PAST behavior, not future intent:\n❌ \"Would you use a feature that...\"\n✅ \"Tell me about the last time you...\"\n\nFocus on SITUATION, not opinion:\n❌ \"What do you think about...\"\n✅ \"Walk me through what happened when...\"\n\nDig for CAUSATION:\n❌ \"Do you like this?\"\n✅ \"What made you decide to...\"\n\nUncover the STRUGGLE:\n❌ \"What features do you want?\"\n✅ \"What's frustrating about how you do this today?\"\n\nSTEP 4: Build question sequencesFor each topic, create layers:Layer 1: Situation\n\"Tell me about the last time you [did this job]\"\n\nLayer 2: Context\n\"What led up to that moment?\"\n\"What were you trying to accomplish?\"\n\nLayer 3: Existing solutions\n\"How did you handle it?\"\n\"What did you try before that?\"\n\nLayer 4: Struggles\n\"What was difficult about that approach?\"\n\"What compromises did you make?\"\n\nLayer 5: Desired outcome\n\"What would success have looked like?\"\n\"When you think about it now, what would have been ideal?\"\n\nSTEP 5: Prepare follow-upsFor every main question, have 3-5 follow-ups:If they give surface answer:\n- \"Can you give me a specific example?\"\n- \"Help me understand what you mean by [term]\"\n- \"What happened next?\"\n\nIf they jump to solutions:\n- \"Before we talk about solutions, help me understand the problem better\"\n- \"What makes that solution appealing?\"\n\nIf they generalize:\n- \"You said 'always' - tell me about one specific time\"\n\nSTEP 6: Include probes for dimensionsFunctional dimension:\nWhat practical outcome are they trying to achieve?\n\nEmotional dimension:\nHow do they want to feel? What do they want to avoid feeling?\n\nSocial dimension:\nHow do they want to be perceived? What's their role/identity?\n\nTimeline probes:\n- First thought: \"When did you first realize you needed to do this?\"\n- Evaluation: \"How did you decide between options?\"\n- Acquisition: \"What made you finally pull the trigger?\"\n- Usage: \"How has it worked in practice?\"\n\nSTEP 7: Add interview mechanicsConsent and recording:\n[Script for permission]\n\nTransitions between sections:\n[How to move topics smoothly]\n\nIf interview goes off track:\n[How to redirect politely]\n\nIf you're running out of time:\n[Which questions to prioritize]\n\nNow create a JTBD interview guide for the described scenario.\n\n</jtbd_framework>\n\n---\n\n## Example Interview Guide Structure\n\n### Jobs-to-be-Done Interview Guide: [Job/Product Area]\n\nInterviewer: [Your name]Interview length: [Duration]Recording: [Y/N + tool]\n\n---\n\n### Pre-Interview Setup\n\nParticipant screener:\n✅ Has done [job] in last [timeframe]\n✅ [Other criteria]\n\nContext to gather beforehand:\n- [Info from CRM/usage data]\n\nWhat success looks like:\n[Specific insights you hope to gain]\n\n---\n\n### Interview Script\n\nOPENING (5 minutes)\n\n\"Thanks for taking the time. I'm [name] and I work on [product]. We're trying to understand how people [do this job], and I'd love to hear about your experience.\n\nThis is pure research - I'm not trying to sell you anything. There are no right or wrong answers. I'm just trying to understand your world better.\n\nI'd like to record this so I can focus on our conversation instead of taking notes. Is that okay?\"\n\n[Start recording]\n\n\"Before we dive in, tell me a bit about your role and what a typical day looks like for you.\"\n\n---\n\nCONTEXT SETTING (10 minutes)Role and responsibilities:\n1. \"Walk me through what you're responsible for.\"\n2. \"Who do you work with day-to-day?\"\n3. \"How does [this area] fit into your job?\"\n\nCurrent state:\n1. \"How do you [do this job] today?\"\n2. \"What tools or processes do you use?\"\n\n---\n\nJOB EXPLORATION (25 minutes)Topic 1: Recent situation\n\n\"Tell me about the last time you [needed to do this job].\"\n\nFollow-ups:\n- \"What was happening that made you need to do this?\"\n- \"Walk me through what you did, step by step.\"\n- \"How long did this take?\"\n- \"Who else was involved?\"\n\nTopic 2: Existing solutions\n\n\"You mentioned using [tool/approach] - how did you end up with that solution?\"\n\nFollow-ups:\n- \"What did you try before that?\"\n- \"What made you switch?\"\n- \"What works well about it?\"\n- \"What's frustrating about it?\"\n\nTopic 3: Struggles and workarounds\n\n\"What's the hardest part about [doing this job]?\"\n\nFollow-ups:\n- \"Can you give me a specific example of when that was a problem?\"\n- \"What do you do when that happens?\"\n- \"Have you found any workarounds?\"\n- \"What would you do if you had unlimited time/resources?\"\n\nTopic 4: Success criteria\n\n\"When you finish [this job], what does 'done well' look like?\"\n\nFollow-ups:\n- \"How do you know you've succeeded?\"\n- \"What would make you feel confident in the outcome?\"\n- \"Who else cares about this being done well?\"\n\nTopic 5: Emotional/social dimensions\n\n\"How do you feel when you have to [do this job]?\"\n\nFollow-ups:\n- \"What's at stake if this doesn't go well?\"\n- \"How would it reflect on you?\"\n- \"What would success mean for your role/team/company?\"\n\n---\n\nWRAP-UP (5 minutes)\n\n\"This has been incredibly helpful. A few final questions:\n\n1. \"If you could wave a magic wand and change one thing about [this process], what would it be?\"\n\n2. \"Is there anything I should have asked about but didn't?\"\n\n3. \"Would you be open to a follow-up conversation if questions come up?\"\n\nThank you so much for your time. This really helps us understand [the problem space] better.\"\n\n---\n\n### Post-Interview Analysis\n\nWithin 24 hours:\n- [ ] Review recording\n- [ ] Pull key quotes\n- [ ] Identify job dimensions (functional, emotional, social)\n- [ ] Note surprises or contradictions\n- [ ] Update interview guide if needed\n\nSynthesis across interviews:\n- [ ] Pattern recognition\n- [ ] Job statement creation\n- [ ] Forces diagram (push/pull/anxiety/habit)\n\n</jtbd_interview>",
    "technique": "Interview scripting, follow-up generation, pattern recognition",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Conduct JTBD research interviews to understand customer motivations"
  },
  {
    "name": "Opportunity Sizing",
    "category": "Discovery",
    "prompt": "\n\n<opportunity_sizing>\n\n<sizing_inputs>\nWHAT YOU'RE SIZING:\n[Feature, product, market]\n\nWHAT YOU KNOW:\n- Market data: [Any research you have]\n- User data: [Current metrics]\n- Competitive data: [Benchmark info]\n\nBUSINESS CONTEXT:\n- Current ARR/revenue: [If relevant]\n- Target segments: [Who you're going after]\n- Pricing model: [How you'll monetize]\n\nCONFIDENCE LEVEL NEEDED:\n- [ ] Rough estimate (back of napkin)\n- [ ] Business case (defendable numbers)\n- [ ] Board deck (high confidence)\n</sizing_inputs>\n\n<sizing_framework>\n\nYou calculate market opportunities in ways that are both ambitious and defensible. Your process:\n\n**STEP 1: Choose your methodology**\n\n**Top-down (market-based):**\nStart with total market, filter down\nBest when: Entering existing category\n\n**Bottom-up (customer-based):**\nStart with customer units, multiply up  \nBest when: Clear target customer\n\n**Value-based:**\nStart with value created, take percentage\nBest when: Replacing existing spend\n\n**Use multiple methods to triangulate.**\n\n**STEP 2: Calculate TAM (Total Addressable Market)**\n\n**How big is the full opportunity?**\n\n**Top-down approach:**\n- Market research (Gartner, Forrester, CB Insights)\n- Industry reports\n- Competitor valuations × market share assumptions\n\n**Bottom-up approach:**\nTAM = [# of potential customers] × [revenue per customer per year]\n\n\n**Value-based approach:**\nTAM = [total spend on problem] × [% you can capture]\n\n\n**Document assumptions:**\nEvery number needs a source or logic.\n\n**STEP 3: Calculate SAM (Serviceable Addressable Market)**\n\n**Who can you realistically serve?**\n\n**Filter TAM by:**\n- Geographic reach\n- Company size focus\n- Industry verticals you serve\n- Technical requirements\n- Regulatory constraints\nSAM = TAM × [% that matches your ICP]\n\n\n**Example:**\n- TAM: $50B (all companies needing analytics)\n- SAM: $5B (B2B SaaS companies 100-1000 employees in US)\n\n**STEP 4: Calculate SOM (Serviceable Obtainable Market)**\n\n**What can you actually capture?**\n\n**Consider:**\n- Your GTM reach\n- Sales capacity\n- Product maturity\n- Competitive position\n- Timeline (Year 1, Year 3, Year 5)\n\n**Common approaches:**\n\n**Bottoms-up:**\nYear 1 SOM = [sales capacity] × [close rate] × [deal size]\n\n\n**Market share:**\nSOM = SAM × [realistic market share %]\n\n\n**Realistic market share for new entrant:**\n- Year 1: 0.1-1% of SAM\n- Year 3: 1-5% of SAM  \n- Year 5: 5-15% of SAM (if successful)\n\n**STEP 5: Build scenarios**\n\n**Don't give one number. Show range:**\n\n**Conservative scenario:**\n- Lower conversion rates\n- Slower adoption\n- More competition\n\n**Base case:**\n- Realistic assumptions\n- Documented logic\n\n**Optimistic scenario:**\n- Better execution\n- Network effects\n- Market growth\n\n**STEP 6: Revenue model**\n\n**How does market size convert to revenue?**\n\n**For each scenario:**\nRevenue = [customers] × [ARPU] × [retention]\n\n\n**Break down by:**\n- Customer segment\n- Pricing tier\n- Time to full adoption\n\n**Year 1:** [X customers] × [$Y ARPU] = $Z revenue\n**Year 3:** [Growing to...]\n**Year 5:** [At scale...]\n\n**STEP 7: Validate assumptions**\n\n**Stress test your math:**\n\n**Reality checks:**\n- \"This means we need to close X deals per month\"\n- \"This assumes Y% of our target market buys\"\n- \"Our sales team needs Z quota to hit this\"\n\n**Comparable checks:**\n- How did similar companies grow?\n- What's typical market penetration?\n- Where are we being aggressive vs conservative?\n\n**STEP 8: Build the business case**\n\n**Connect to strategy:**\n- How this fits company goals\n- Resource investment needed\n- Expected ROI\n- Risk factors\n\n**Create decision frameworks:**\n- Break-even analysis\n- Payback period\n- Comparison to other opportunities\n\nNow size the described opportunity with documented assumptions.\n\n</sizing_framework>\n\n---\n\n## Example Opportunity Sizing\n\n### Opportunity Sizing: [Feature/Product/Market]\n\n**Date:** [When]  \n**Owner:** [PM name]  \n**Confidence:** [Low/Medium/High]\n\n---\n\n### Executive Summary\n\n**Market opportunity:** $[X]M - $[Y]M SAM  \n**3-year target:** $[Z]M revenue ([X]% of SAM)  \n**Investment needed:** $[A]M over [timeframe]\n\n**Key assumption:** [The 1-2 biggest assumptions this depends on]\n\n---\n\n### Market Sizing (TAM → SAM → SOM)\n\n**TAM: Total Addressable Market**\n\n**Methodology:** [Top-down / Bottom-up / Value-based]\n\n**Calculation:**\n[# units] × [$ per unit] = $[TAM]\nExample:\n50M businesses globally × $200 avg spend = $10B TAM\n\n\n**Data sources:**\n- [Source 1 + link]\n- [Source 2 + link]\n\n---\n\n**SAM: Serviceable Addressable Market**\n\n**Our focus:** [Specific segment]\n\n**Filters applied:**\n- Geography: [e.g., US only] → [%]\n- Company size: [e.g., 100-1000 employees] → [%]\n- Industry: [e.g., B2B SaaS] → [%]\n- Technical fit: [e.g., uses Salesforce] → [%]\n\n**Calculation:**\n$[TAM] × [%] × [%] × [%] = $[SAM]\nExample:\n$10B × 25% (US) × 15% (size) × 40% (vertical) = $150M SAM\n\n\n---\n\n**SOM: Serviceable Obtainable Market**\n\n**What we can realistically capture:**\n\n| Timeframe | Market Share | Revenue | Customers |\n|-----------|-------------|---------|-----------|\n| Year 1 | 0.5% of SAM | $750K | ~75 |\n| Year 2 | 2% of SAM | $3M | ~250 |\n| Year 3 | 5% of SAM | $7.5M | ~600 |\n\n**Assumptions:**\n- Sales capacity: [X reps]\n- Close rate: [Y%]\n- Ramp time: [Z months]\n- Churn: [A%]\n\n---\n\n### Three Scenarios\n\n**CONSERVATIVE**\n\n**Assumptions:**\n- Slower adoption: [Why]\n- More competition: [Who]\n- Lower pricing: [How much]\n\n**3-year outcome:** $[X]M revenue\n\n---\n\n**BASE CASE**\n\n**Assumptions:**\n- [Key assumption 1]\n- [Key assumption 2]\n- [Key assumption 3]\n\n**3-year outcome:** $[Y]M revenue\n\n---\n\n**OPTIMISTIC**\n\n**Assumptions:**\n- Network effects kick in\n- Faster enterprise adoption\n- Expansion into adjacent markets\n\n**3-year outcome:** $[Z]M revenue\n\n---\n\n### Revenue Model\n\n**Unit economics:**\nARPU: $[X]/month\nLTV: $[Y]\nCAC: $[Z]\nLTV:CAC = [ratio]\n\n\n**Growth drivers:**\n- [Driver 1]: [Expected impact]\n- [Driver 2]: [Expected impact]\n\n**Revenue breakdown:**\n\n| Segment | % of Revenue | ARPU | Count |\n|---------|-------------|------|-------|\n| SMB | 30% | $100/mo | 500 |\n| Mid-market | 50% | $500/mo | 200 |\n| Enterprise | 20% | $2000/mo | 20 |\n\n---\n\n### Validation & Comparable\n\n**Similar company benchmarks:**\n- [Company A]: Reached $XM in Y years serving similar market\n- [Company B]: Captured Z% market share\n\n**Reality checks:**\n- ✅ Requires [X] deals/month (achievable with [Y] reps)\n- ⚠️ Assumes [Z]% of market will switch (high but possible)\n- ✅ CAC payback of [A] months (healthy)\n\n---\n\n### Investment & Returns\n\n**Investment needed:**\n- Engineering: [FTE × months]\n- Sales/marketing: $[X]\n- Ops/support: [Resources]\n**Total:** $[Y]M\n\n**Expected returns:**\n- Break-even: Month [X]\n- ROI: [Y]% over 3 years\n- NPV: $[Z]M\n\n---\n\n### Risks & Assumptions\n\n**Biggest risks:**\n1. [Risk 1]: [Mitigation]\n2. [Risk 2]: [Mitigation]\n\n**Key assumptions to validate:**\n- [ ] [Assumption 1] - [How to test]\n- [ ] [Assumption 2] - [How to test]\n\n---\n\n### Appendix\n\n**Detailed calculations:** [Spreadsheet link]  \n**Data sources:** [Links to research]  \n**Interview notes:** [Customer validation]\n\n</opportunity_sizing>",
    "technique": "TAM/SAM/SOM calculation, assumption documentation, scenario modeling",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Quantify market opportunity and build business case"
  },
  {
    "name": "Win/Loss Analysis",
    "category": "Discovery",
    "prompt": "<win_loss_analysis>\n\n<analysis_inputs>\nYOUR WIN/LOSS DATA:\n- Wins: [# and context]\n- Losses: [# and context]\n- Time period: [When]\n\nDATA SOURCES:\n- [ ] Sales notes\n- [ ] Customer interviews\n- [ ] CRM data\n- [ ] Call recordings\n- [ ] Competitive intel\n\nWHAT YOU WANT TO KNOW:\n- [ ] Why we win vs [competitor]\n- [ ] Common objections\n- [ ] Deal-breaker features\n- [ ] Pricing sensitivity\n- [ ] Buying process insights\n</analysis_inputs>\n\n<winloss_framework>\n\nYou analyze wins and losses to find actionable patterns that improve close rates. Your process:\n\nSTEP 1: Categorize outcomesBasic buckets:\n- Won: Chose us\n- Lost to competitor: Chose [specific competitor]\n- Lost to status quo: Chose to do nothing\n- Lost to budget: No money\n- Lost to other: [Build product themselves, different approach]\n\nFor losses, always identify WHERE they went.STEP 2: Extract key data pointsFor each deal, capture:Deal characteristics:\n- Company size\n- Industry\n- Use case\n- Deal size\n- Sales cycle length\n\nCompetition:\n- Who else they evaluated\n- How far along each got\n\nDecision factors:\n- What mattered most\n- What was \"nice to have\"\n- Deal breakers (if any)\n\nSTEP 3: Find patterns in WINSWhy did we win?Look for:\n- Feature differentiation\n- Pricing/value perception\n- Sales process effectiveness\n- Timing factors\n- Relationship/trust elements\n\nGroup into themes:Product reasons:\n\"We won because we have [feature] that [competitor] lacks\"\n\nGTM reasons:\n\"We won because our sales process was [faster/more consultative/better]\"\n\nMarket position:\n\"We won with [segment] because [positioning]\"\n\nSTEP 4: Find patterns in LOSSESWhy did we lose?Loss categories:Product gaps:\n- Missing features\n- Integration limitations\n- Performance issues\n- UX problems\n\nPricing issues:\n- Too expensive\n- Wrong packaging\n- ROI not clear\n\nGTM problems:\n- Sales process too slow\n- Wrong messaging\n- Poor demo\n- Lack of references\n\nMarket fit:\n- Not built for their segment\n- Competitor better aligned\n\nFor each loss, ask:\n- Was this winnable?\n- What would have changed the outcome?\n- Is this a pattern?\n\nSTEP 5: Competitive analysisBy competitor:[Competitor A]\n- When we win vs them: [Reasons]\n- When we lose vs them: [Reasons]\n- Their strength: [What they do well]\n- Our advantage: [Where we beat them]\n- Positioning: [How to position against them]\n\nSTEP 6: Segment analysisDo patterns differ by:Company size:\n- SMB wins/losses\n- Mid-market wins/losses\n- Enterprise wins/losses\n\nIndustry:\n- [Vertical A] patterns\n- [Vertical B] patterns\n\nUse case:\n- [Use case X] patterns\n- [Use case Y] patterns\n\nExample insight:\n\"We lose to Competitor A in enterprise but win in mid-market because [reason]\"\n\nSTEP 7: Quantify impactPrioritize themes by:\n- Frequency: How often does this come up?\n- Magnitude: How much revenue at stake?\n- Addressability: Can we fix it?\n\nImpact formula:",
    "technique": "Pattern extraction, competitive intelligence, objection categorization",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Understand why deals close or don't close"
  },
  {
    "name": "Say No Gracefully",
    "category": "Productivity",
    "prompt": "<say_no_gracefully>\n\n<no_inputs>\nWHO'S ASKING:\n[Executive, customer, teammate, etc.]\n\nWHAT THEY WANT:\n[The request]\n\nWHY YOU'RE SAYING NO:\n- [ ] Not aligned with strategy\n- [ ] Resource constraints\n- [ ] Technical limitations\n- [ ] Wrong priority/timing\n- [ ] Other: [Explain]\n\nRELATIONSHIP CONTEXT:\n[How important is this stakeholder? History?]\n\nYOUR CONSTRAINT:\n- [ ] Can be transparent about real reason\n- [ ] Need to be diplomatic (politics)\n- [ ] Need to preserve future relationship\n</no_inputs>\n\n<graceful_no_framework>\n\nYou decline requests in ways that preserve relationships and redirect energy productively. Your process:\n\nSTEP 1: Understand the \"no\" typeHard no:\n\"We're never building this\"\n\nNot now:\n\"Not on the roadmap currently\"\n\nNot us:\n\"Wrong team to own this\"\n\nNot that:\n\"We'd solve it differently\"\n\nChoose language that matches reality.STEP 2: Structure your responseThe graceful no formula:1. Acknowledge + Appreciate\nShow you heard them, value input\n\n2. Align on problem\n\"I understand this is important because...\"\n\n3. Explain decision (careful!)\nThe delicate part - see Step 3\n\n4. Offer alternative\nWhat CAN you do?\n\n5. Keep door open (if appropriate)\nHow they can influence future\n\nSTEP 3: Frame the \"why\"Good frames (focus on strategy):\n- \"We're focused on [priority] this quarter\"\n- \"This doesn't align with our [goal]\"\n- \"We're optimizing for [outcome]\"\n\nRisky frames (avoid if possible):\n- \"We don't have resources\" (sounds like excuse)\n- \"It's too hard technically\" (sounds like inability)\n- \"Legal won't allow it\" (passing blame)\n\nFor customers:\nNever say \"just you\" - even if true.\nFrame as \"not our current focus\" not \"your idea is bad\"\n\nSTEP 4: Offer alternativesNever leave them empty-handed:What you CAN offer:\n- Workaround with existing features\n- Different solution to same problem\n- Partial solution on shorter timeline\n- Community/partner solution\n- Documentation/guidance\n\nThe redirect:\n\"While we won't build [X], have you considered [Y]?\"\n\nSTEP 5: Adjust tone to audienceTo executives:\nStrategic, confident, solution-oriented\nFocus on: Business impact, priorities, alternatives\n\nTo customers:\nEmpathetic, specific, helpful\nFocus on: Understanding their need, workarounds, timeline\n\nTo teammates:\nCollaborative, transparent, realistic\nFocus on: Trade-offs, priorities, what you CAN do\n\nTo junior team members:\nEducational, encouraging, growth-focused\nFocus on: The \"why\", how decisions are made\n\nSTEP 6: Choose your mediumAsync (email/Slack):\n✅ For: Clear-cut decisions, documented responses\n❌ Avoid: Emotional topics, big disappointments\n\nSync (meeting/call):\n✅ For: Complex explanations, relationships matter\n❌ Avoid: Simple requests, clear documentation needs\n\nFace-to-face:\n✅ For: High-stakes, sensitive, relationship-critical\n❌ Avoid: Routine decisions\n\nSTEP 7: Language templatesInstead of: \"No, we can't do that\"\nSay: \"That's not something we're planning to pursue because [reason]\"\n\nInstead of: \"That's not a priority\"\nSay: \"We're prioritizing [X] over [Y] because [strategic reason]\"\n\nInstead of: \"We don't have bandwidth\"\nSay: \"To do this well would require [resources] we've committed to [priority]\"\n\nInstead of: \"That's a bad idea\"\nSay: \"Here's what concerns me about that approach: [specific issues]\"\n\nSTEP 8: Handle pushbackIf they escalate:\n\"I understand this is important to you. Let me connect you with [decision maker] to discuss the strategic priorities.\"\n\nIf they argue:\n\"I hear you. The trade-off is [this] vs [that]. Given [context], we chose [this].\"\n\nIf they're upset:\n\"I appreciate your passion for this. Can we talk about what you're trying to accomplish? Maybe there's another way.\"\n\nNow craft a graceful \"no\" for the described situation.\n\n</graceful_no_framework>\n\n---\n\n## Example \"No\" Messages\n\n(Adapt tone and detail based on relationship and context)\n\n---\n\n### Example 1: To Customer\n\nSubject: Re: Feature Request - [Feature Name]\n\nHi [Name],\n\nThanks for the thoughtful feature request. I can see why [specific capability] would be valuable for [their use case].\n\nAfter discussing with the team, we've decided not to pursue this on our near-term roadmap. We're currently focused on [strategic priority] because [customer segment] has told us [pain point] is their biggest challenge, and we want to deliver a great solution there first.\n\nThat said, here are a couple of ways you could [accomplish their goal] today:\n1. [Workaround A]: [How it works]\n2. [Workaround B]: [How it works]\n\nI've also added this to our feature request tracker. If we see more demand or our priorities shift, we'll definitely revisit it.\n\nWould either of those workarounds help in the meantime? Happy to jump on a quick call to walk through them.\n\nBest,\n[Your name]\n\n---\n\n### Example 2: To Executive Stakeholder\n\nContext: Executive wants to add feature mid-sprint\n\nHi [Name],\n\nI understand why [feature] feels urgent given [business context]. You're right that it would help with [their goal].\n\nHere's my concern: We're 2 weeks into our sprint building [current priority], which supports our Q4 objective of [goal]. If we pivot now, we'd:\n- Miss our [milestone] deadline by ~2 weeks\n- Context-switch the engineering team (historically costs us 20-30% velocity)\n- Push [other stakeholder's] project to next quarter\n\nAlternative approach:\n\nWhat if we:\n1. Finish current sprint (2 weeks)\n2. Do discovery on [their feature] while we're wrapping up (5 days)\n3. Kick off [their feature] start of next sprint with full team\n\nThis gets you [feature] by [date], just 2 weeks later than your ask, but doesn't derail our Q4 commitments.\n\nWould that work? Or if this truly can't wait, help me understand what we should de-prioritize to make room.\n\n[Your name]\n\n---\n\n### Example 3: To Teammate\n\nContext: Engineer wants to refactor code instead of shipping feature\n\nHey [Name],\n\nI totally get the appeal of refactoring this before we ship. You're right that the code is messy and technical debt is piling up.\n\nHere's my thinking: We've got [customer] waiting on this feature for their [big event in 2 weeks]. They've built their [campaign/project] around having this ready.\n\nTrade-off:\n- Ship now: Feature works, code is imperfect, we can iterate\n- Refactor first: Code is clean, but we miss their deadline and risk the relationship\n\nFor this one, I think we ship and schedule the refactor for next sprint. What if we:\n1. Ship the MVP this week\n2. Block off [X days] next sprint for you to refactor properly\n3. Document the technical debt so we don't forget\n\nThen you get the clean code AND we hit the customer deadline.\n\nDoes that work? If you feel strongly this is a bad idea from a technical perspective, let's discuss - I don't want to ship something that'll break.\n\n[Your name]\n\n---\n\n### Example 4: To Junior PM\n\nContext: Junior PM wants to add their pet feature to roadmap\n\nHi [Name],\n\nI really appreciate you thinking strategically about [feature area]. It shows great product instinct that you identified [opportunity].\n\nHere's why I don't think we should prioritize this right now:\n\n1. Strategy alignment: Our North Star is [metric], and this feature would impact [other metric]. Given we need to move [North Star] for our [goal], we have to stay focused there.\n\n2. Opportunity cost: Building this would take [X weeks]. That's [X weeks] not building [higher priority thing] that impacts [more customers/revenue/strategic goal].\n\n3. Uncertain demand: We have [Y] feature requests for this vs [Z] for [competing priority]. The data suggests the other need is more acute.\n\nWhat I'd love you to do:\n\nAdd this to our \"Backlog - Future Ideas\" and create a one-pager with:\n- The specific customer problem\n- Evidence of demand (how many requests, from what segments)\n- How this ties to our strategy\n- Expected impact (what metric moves, by how much)\n\nIf you can make a compelling case with data, I'm totally open to re-evaluating next quarter. And this is great practice for building business cases.\n\nSound good?\n\n[Your name]\n\n---\n\n### Example 5: Hard No (This is Never Happening)\n\nContext: Legal concern, technical impossibility, or strategic mismatch\n\nHi [Name],\n\nThanks for bringing this idea to the table. I want to be direct with you: this isn't something we're going to pursue, now or in the future.\n\nHere's why: [Clear, honest reason - legal constraint / technical impossibility / fundamentally misaligned with company strategy].\n\nI know that's not the answer you wanted. What I can offer is [alternative that actually helps]:\n- [Option A]\n- [Option B]\n\nWould either of those address what you're trying to accomplish? If not, let's get on a call and I can help you think through other approaches.\n\n[Your name]\n\n</say_no_gracefully>",
    "technique": "Stakeholder framing, alternative offering, relationship preservation",
    "tools": "Claude Projects, ChatGPT Projects",
    "useCase": "Decline requests while maintaining relationships"
  },
  {
    "name": "Influence Without Authority\n",
    "category": "Operations",
    "prompt": "<influence_without_authority>\n\n<influence_inputs>\nWHAT YOU NEED:\n[Specific outcome you're trying to drive]\n\nWHO NEEDS TO ACT:\n[Person/team you need help from]\n\nWHAT THEY CONTROL:\n[Resources, decisions, effort they own]\n\nYOUR RELATIONSHIP:\n- Reporting structure: [Peer, different org, etc.]\n- History: [Have you worked together before?]\n- Current dynamic: [Good/neutral/strained]\n\nTHEIR PRIORITIES:\n[What they care about, if you know]\n\nYOUR LEVERAGE:\n[What you can offer or bring to the table]\n</influence_inputs>\n\n<influence_framework>\n\nYou get people to act when you have no formal authority over them. Your process:\n\nSTEP 1: Map the influence landscapeWho's the real decision maker?Don't assume title = power.Look for:\n- Who actually has resources/headcount?\n- Who does the senior leader ask for advice?\n- Who controls the process/timeline?\n- Who has veto power?\n\nMap stakeholders:\n- Primary: Who must say yes\n- Secondary: Who influences primary\n- Blockers: Who could derail it\n- Supporters: Who's already on your side\n\nSTEP 2: Understand their motivationsWhat do they care about?Common motivations:\n- Career advancement (visibility, promotions)\n- Team success (OKRs, metrics, headcount)\n- Ease/efficiency (less work, fewer meetings)\n- Innovation/impact (building cool things)\n- Relationships (looking good to their boss)\n- Risk avoidance (not getting blamed)\n\nFor each stakeholder, identify:\n- What's their win?\n- What's their fear?\n- What's their constraint?\n\nSTEP 3: Frame in their termsStop saying: \"I need you to...\"\nStart saying: \"Here's how this helps you...\"\n\nTranslation examples:Your goal: Get eng to build feature faster\nTheir goal: Hit sprint commitments, manage tech debt\nYour frame: \"This unblocks [eng priority] and helps us validate [tech approach]\"\n\nYour goal: Get marketing to prioritize your launch\nTheir goal: Hit lead gen targets\nYour frame: \"This campaign should drive [X] MQLs based on similar launches\"\n\nYour goal: Get design resources\nTheir goal: Avoid rework, showcase portfolio pieces\nYour frame: \"This lets us nail the UX from the start. Could be a great portfolio piece.\"\n\nSTEP 4: Build reciprocityInfluence is a bank account.Deposits:\n- Make them look good to their boss\n- Make their job easier\n- Give them credit publicly\n- Help them hit their OKRs\n- Share information they need\n- Be low-maintenance\n\nWithdrawals:\n- Ask for urgent help\n- Change requirements mid-flight\n- Create extra work\n- Make them miss their goals\n\nBefore you ask for something big, make deposits.STEP 5: Choose your influence tacticDifferent situations need different approaches:TACTIC 1: Logic & Data\nBest for: Analytical stakeholders, technical decisions\n\n\"Here's the data that shows [your case]\"\n\nTACTIC 2: Coalition Building\nBest for: Big organizational shifts, contentious decisions\n\n\"[Name], [Name], and [Name] are all on board\"\n\nTACTIC 3: Pilot/Experiment\nBest for: Risk-averse stakeholders, uncertain outcomes\n\n\"Let's try it with one team and measure results\"\n\nTACTIC 4: Escalation (nuclear option)\nBest for: Deadlocked situations, last resort\n\n\"I'm going to [mutual boss] to decide\"\n\nTACTIC 5: Make It Easy\nBest for: Busy stakeholders, low-priority requests\n\n\"I've done 90% of the work, just need [small thing]\"\n\nTACTIC 6: Social Proof\nBest for: Competitive teams, trendy initiatives\n\n\"[Competitor/Industry leader] is doing this\"\n\nSTEP 6: Structure your askThe influence pitch:1. Context (30 seconds)\n\"Here's what's happening and why it matters\"\n\n2. The ask (15 seconds)\n\"Specifically, I need [concrete request]\"\n\n3. What's in it for them (30 seconds)\n\"Here's how this helps you achieve [their goal]\"\n\n4. Make it easy (15 seconds)\n\"I've already [done prep work], you just need to [small step]\"\n\n5. Timeline (15 seconds)\n\"Can we decide by [date]? If yes, here's what happens next.\"\n\nSTEP 7: Handle objectionsCommon objections and responses:\"We don't have capacity\"\n→ \"What if I [reduce scope / push timeline / find resources]?\"\n\n\"That's not a priority for us\"\n→ \"I understand. What would make it a priority?\" (Then tie to that)\n\n\"We tried that before and it didn't work\"\n→ \"Tell me what happened. Here's what's different now: [X]\"\n\n\"I need approval from [person]\"\n→ \"Want me to draft the proposal for them? Or should we meet with them together?\"\n\n\"It's too risky\"\n→ \"Agreed. What if we [de-risk strategy]?\"\n\nSTEP 8: Build ongoing relationshipsInfluence is long-term.Relationship maintenance:\n- Regular 1:1s (even informal coffee chats)\n- Share credit loudly\n- Help them win, even when you don't benefit\n- Keep them informed (no surprises)\n- Ask for their input early\n- Celebrate their successes\n\nBe the PM other teams WANT to work with.\n\nNow create an influence strategy for the described situation.\n\n</influence_framework>\n\n---\n\n## Example Influence Strategies\n\n(Adapt based on stakeholder and situation)\n\n---\n\n### Strategy 1: Getting Engineering Resources\n\nSituation: Need eng team to prioritize your feature over their roadmap\n\nStakeholder: Engineering Manager (peer)\n\nTheir motivation:\n- Hit sprint commitments\n- Keep team happy (not overloaded)\n- Deliver on tech debt promises\n\nYour approach:1. Make it strategically aligned:\n\"This feature directly impacts [company OKR] which affects [eng leader's] goals\"\n\n2. Reduce scope to be practical:\n\"What if we build the MVP first - [X days] instead of [Y days]\"\n\n3. Offer to help:\n\"I'll write the specs, do the user testing, and handle the launch. You focus on building.\"\n\n4. Create urgency (if legit):\n\"[Customer] needs this by [date] for [their important thing]. If we miss it, we risk [business impact].\"\n\n5. Make them the hero:\n\"When this works, I'm going to highlight your team's execution in the exec meeting.\"\n\nThe ask (in meeting):\n\n\"Hey [Name], quick question. We have an opportunity to [business outcome] but it requires [specific eng work]. I know you're slammed, so I did some thinking:\n\nWhat if we:\n- Scope it to just [MVP version] → [X days instead of Y]\n- Schedule it for [sprint window that works for them]\n- I handle all the PM work around it\n\nThis helps us hit [your OKR] AND [their OKR]. Plus it could be a great [technical win they care about].\n\nWould that work? If not, what would make it feasible?\"\n\n---\n\n### Strategy 2: Influencing Up (Getting Executive Approval)\n\nSituation: Need executive to approve budget/headcount/strategy\n\nStakeholder: VP or C-level (your skip-level or higher)\n\nTheir motivation:\n- Company-level results\n- Board perception\n- Resource efficiency\n- Strategic wins\n\nYour approach:1. Lead with business impact:\nSubject: [Specific $ or metric outcome] Opportunity\n\n\"TL;DR: We can drive [$X revenue / Y% increase in metric] by [action]. Needs [resource] by [date].\"\n\n2. Show you've done the work:\n\"I've validated this with [data sources], pressure-tested with [stakeholders], and have [team] ready to execute.\"\n\n3. Present trade-offs clearly:\n\"We can do this OR [alternative]. Here's the comparison:\n- [This approach]: [pros/cons]\n- [Alternative]: [pros/cons]\nMy recommendation: [X] because [strategic reason]\"\n\n4. Make the ask easy:\n\"I need a yes/no by [date]. If yes, here's the 30/60/90 day plan.\"\n\nThe pitch (in meeting):\n\n\"I'd like 15 minutes to walk you through an opportunity I think we should pursue.\n\nContext: [What's happening in market/business]\n\nOpportunity: We can [outcome] if we [action]\n\nWhy this matters:\n- [Metric impact]\n- [Strategic alignment]\n- [Competitive advantage]\n\nWhat I need: [Specific ask]\n\nTrade-off: We're choosing this over [alternative] because [reason]\n\nWhat happens next: [Clear plan]\n\nMy ask: Decision by [date]\n\nQuestions?\"\n\n---\n\n### Strategy 3: Cross-Functional Alignment\n\nSituation: Need Sales, Marketing, and CS aligned on launch\n\nStakeholders: Multiple team leaders (peers)\n\nTheir motivations:\n- Sales: Hit quota, avoid customer complaints\n- Marketing: Lead gen, campaign success\n- CS: Not overwhelmed with support, happy customers\n\nYour approach:1. Find the shared goal:\n\"We all want [successful launch that hits revenue goals without overwhelming CS]\"\n\n2. Show you've thought about their concerns:\n\nTo Sales: \"I've built enablement so reps can demo in [X] minutes\"\nTo Marketing: \"Here's the campaign brief with assets you need\"\nTo CS: \"We're launching to [limited segment] first to manage support load\"\n\n3. Make it collaborative:\n\"I need your input on [specific decisions]. What am I missing?\"\n\n4. Build coalition:\nGet sales leader on board first → use that to convince marketing → create momentum\n\nThe alignment meeting:\n\n\"Thanks for making time. I want to align on [launch] so we hit our goals without chaos.\n\nShared goal: [Revenue/adoption target]\n\nLaunch plan: [One-pager with timeline]\n\nWhat I need from each team:\n- Sales: [Specific asks]\n- Marketing: [Specific asks]\n- CS: [Specific asks]\n\nWhat I'm providing:\n- [Enablement materials]\n- [Support resources]\n- [Weekly sync meetings]\n\nConcerns I've heard:\n- [Concern A]: Here's how we're addressing: [Solution]\n- [Concern B]: Here's mitigation: [Plan]\n\nOpen questions:\n- [Question for sales]\n- [Question for marketing]\n- [Question for CS]\n\nLet's go through these and make sure we're set up to win.\"\n\n---\n\n### Strategy 4: Influencing a Blocker\n\nSituation: Someone is blocking your project\n\nStakeholder: Legal, Security, Another PM (has veto power)\n\nTheir motivation:\n- Risk avoidance\n- Protecting their domain\n- Not looking bad\n\nYour approach:1. Understand the real objection:\nSchedule 1:1: \"Help me understand your concerns\"\nListen for: Real risk vs. politics vs. NIH syndrome\n\n2. Collaborate on solution:\n\"Let's solve this together. What if we [address concern]?\"\n\n3. Bring them into the process:\n\"Can you review this approach? I want to make sure we're handling [their concern] properly.\"\n\n4. Give them credit:\n\"[Blocker's name] helped us think through [concern] so we could move forward safely.\"\n\n5. If still blocked, escalate thoughtfully:\n\"I'd like to get [mutual boss] to help us decide between [your approach] and [their concern].\"\n\nThe conversation:\n\n\"Hey [Name], I know you have concerns about [project]. I'd love to understand them better so we can address them.\n\nWalk me through what worries you about this?\"\n\n[Listen actively]\n\n\"That makes sense. Let me think out loud about how we could handle that:\n- [Option A]: [How it addresses concern]\n- [Option B]: [Alternative]\n\nWhat would make you comfortable moving forward?\"\n\n[Collaborate on solution]\n\n\"Awesome. Can I document this and share it back with you? And I'd love to credit you in the project plan for helping us think through [concern].\"\n\n---\n\n### Key Principles Across All Influence Situations\n\n1. Do your homework\nKnow their goals, constraints, and fears\n\n2. Frame in their terms\nWhat's their win?\n\n3. Make it easy\nReduce friction, do the work\n\n4. Build relationships before you need them\nDeposits before withdrawals\n\n5. Show, don't tell\nData, examples, proof\n\n6. Create coalitions\n\"[Other stakeholders] are already on board\"\n\n7. Offer options\nNever \"my way or nothing\"\n\n8. Be low-maintenance\nDon't create extra work\n\n9. Share credit\nMake them look good\n\n10. Play long game\nYou'll work together again\n\n</influence_without_authority>",
    "technique": "Persuasion framing, stakeholder mapping, alignment tactics",
    "tools": "Claude, ChatGPT Projects",
    "useCase": "Get things done when you don't have direct control"
  }
];
